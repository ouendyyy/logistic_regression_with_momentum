{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":59291,"databundleVersionId":6678907,"sourceType":"competition"}],"dockerImageVersionId":30627,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-09T12:00:30.835577Z","iopub.execute_input":"2024-01-09T12:00:30.836121Z","iopub.status.idle":"2024-01-09T12:00:30.843981Z","shell.execute_reply.started":"2024-01-09T12:00:30.836012Z","shell.execute_reply":"2024-01-09T12:00:30.842987Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"/kaggle/input/linking-writing-processes-to-writing-quality/sample_submission.csv\n/kaggle/input/linking-writing-processes-to-writing-quality/test_logs.csv\n/kaggle/input/linking-writing-processes-to-writing-quality/train_scores.csv\n/kaggle/input/linking-writing-processes-to-writing-quality/train_logs.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd \nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom tqdm import tqdm\nfrom collections import Counter\nimport re\nfrom sklearn import model_selection\nfrom sklearn import metrics\nimport lightgbm as lgb\n","metadata":{"execution":{"iopub.status.busy":"2024-01-09T12:00:30.861881Z","iopub.execute_input":"2024-01-09T12:00:30.862949Z","iopub.status.idle":"2024-01-09T12:00:30.868200Z","shell.execute_reply.started":"2024-01-09T12:00:30.862910Z","shell.execute_reply":"2024-01-09T12:00:30.867266Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"input_dir=\"/kaggle/input/linking-writing-processes-to-writing-quality\"\ntrain_logs=pd.read_csv(f'{input_dir}/train_logs.csv')\ntest_logs=pd.read_csv(f'{input_dir}/test_logs.csv')\ntrain_scores=pd.read_csv(f'{input_dir}/train_scores.csv')","metadata":{"execution":{"iopub.status.busy":"2024-01-09T12:00:30.882761Z","iopub.execute_input":"2024-01-09T12:00:30.883063Z","iopub.status.idle":"2024-01-09T12:00:42.516578Z","shell.execute_reply.started":"2024-01-09T12:00:30.883011Z","shell.execute_reply":"2024-01-09T12:00:42.515487Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"def getEssays(df):\n    textInputDf = df[['id', 'activity', 'cursor_position', 'text_change']].copy()\n    textInputDf = textInputDf[textInputDf.activity != 'Nonproduction']\n    valCountsArr = textInputDf['id'].value_counts(sort=False).values\n    lastIndex = 0\n    essaySeries = pd.Series()\n\n    for index, valCount in enumerate(valCountsArr):\n\n        # Indexes down_time at current Id\n        currTextInput = textInputDf[['activity', 'cursor_position', 'text_change']].iloc[lastIndex : lastIndex + valCount]\n\n        # Update the last index\n        lastIndex += valCount\n\n        # Where the essay content will be stored\n        essayText = \"\"\n\n        \n        # Produces the essay\n        for Input in currTextInput.values:\n            \n            # Input[0] = activity\n            # Input[1] = cursor_position\n            # Input[2] = text_change\n            \n            # If activity = Replace\n            if Input[0] == 'Replace':\n                # splits text_change at ' => '\n                replaceTxt = Input[2].split(' => ')\n                \n                # DONT TOUCH\n                essayText = essayText[:Input[1] - len(replaceTxt[1])] + replaceTxt[1] + essayText[Input[1] - len(replaceTxt[1]) + len(replaceTxt[0]):]\n                continue\n\n                \n            # If activity = Paste    \n            if Input[0] == 'Paste':\n                # DONT TOUCH\n                essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n                continue\n\n                \n            # If activity = Remove/Cut\n            if Input[0] == 'Remove/Cut':\n                # DONT TOUCH\n                essayText = essayText[:Input[1]] + essayText[Input[1] + len(Input[2]):]\n                continue\n\n                \n            # If activity = Move...\n            if \"M\" in Input[0]:\n                # Gets rid of the \"Move from to\" text\n                croppedTxt = Input[0][10:]\n                \n                # Splits cropped text by ' To '\n                splitTxt = croppedTxt.split(' To ')\n                \n                # Splits split text again by ', ' for each item\n                valueArr = [item.split(', ') for item in splitTxt]\n                \n                # Move from [2, 4] To [5, 7] = (2, 4, 5, 7)\n                moveData = (int(valueArr[0][0][1:]), int(valueArr[0][1][:-1]), int(valueArr[1][0][1:]), int(valueArr[1][1][:-1]))\n\n                # Skip if someone manages to activiate this by moving to same place\n                if moveData[0] != moveData[2]:\n                    # Check if they move text forward in essay (they are different)\n                    if moveData[0] < moveData[2]:\n                        # DONT TOUCH\n                        essayText = essayText[:moveData[0]] + essayText[moveData[1]:moveData[3]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[3]:]\n                    else:\n                        # DONT TOUCH\n                        essayText = essayText[:moveData[2]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[2]:moveData[0]] + essayText[moveData[1]:]\n                continue\n                \n                \n            # If just input\n            # DONT TOUCH\n            essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n\n            \n        # Sets essay at index  \n        essaySeries[index] = essayText\n     \n    \n    # Sets essay series index to the ids\n    essaySeries.index =  textInputDf['id'].unique()\n    \n    \n    # Returns the essay series\n    return essaySeries","metadata":{"execution":{"iopub.status.busy":"2024-01-09T12:00:42.518505Z","iopub.execute_input":"2024-01-09T12:00:42.518806Z","iopub.status.idle":"2024-01-09T12:00:42.536241Z","shell.execute_reply.started":"2024-01-09T12:00:42.518780Z","shell.execute_reply":"2024-01-09T12:00:42.535269Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"train_essays = getEssays(train_logs)\ntest_essays=getEssays(test_logs)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T12:00:42.537533Z","iopub.execute_input":"2024-01-09T12:00:42.537804Z","iopub.status.idle":"2024-01-09T12:11:08.604667Z","shell.execute_reply.started":"2024-01-09T12:00:42.537780Z","shell.execute_reply":"2024-01-09T12:11:08.603570Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"train_essaysdf = pd.DataFrame({'id': train_essays.index, 'essay': train_essays.values})\ntest_essaysdf = pd.DataFrame({'id': test_essays.index, 'essay': test_essays.values})\n\nmerged_data = train_essaysdf.merge(train_scores, on='id')\nprint(merged_data)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T12:11:08.607689Z","iopub.execute_input":"2024-01-09T12:11:08.608446Z","iopub.status.idle":"2024-01-09T12:11:08.624807Z","shell.execute_reply.started":"2024-01-09T12:11:08.608406Z","shell.execute_reply":"2024-01-09T12:11:08.623916Z"},"trusted":true},"execution_count":61,"outputs":[{"name":"stdout","text":"            id                                              essay  score\n0     001519c8  qqqqqqqqq qq qqqqq qq qqqq qqqq.  qqqqqq qqq q...    3.5\n1     0022f953  qqqq qq qqqqqqqqqqq ? qq qq qqq qqq qqq, qqqqq...    3.5\n2     0042269b  qqqqqqqqqqq qq qqqqq qqqqqqqqq qq qqqqqqqqqqq ...    6.0\n3     0059420b  qq qqqqqqq qqqqqq qqqqqqqqqqqqq qqqq q qqqq qq...    2.0\n4     0075873a  qqqqqqqqqqq qq qqq qqqqq qq qqqqqqqqqq, qqq qq...    4.0\n...        ...                                                ...    ...\n2466  ffb8c745       qq qqqqq'q qqqqqqq, qqq'q qqqqq q qqqq qq...    3.5\n2467  ffbef7e5  qqqq qqqqqq qqqqq qq qqqqq qqqqq, qq qq q qqqq...    4.0\n2468  ffccd6fd  qqqqqq qqqq q qqqqqqq qqqqqqqqq qq qqqqqq qqqq...    1.5\n2469  ffec5b38  qqqqqqqqqq qqqqqqq, qqqqqq qqqq qqqqq qqqq qqq...    5.0\n2470  fff05981  qq qqqq qqqqqqq qqqqqqqq qq qqqqqqqqqqq qq qq ...    4.0\n\n[2471 rows x 3 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"df_train = pd.DataFrame()\ndf_test = pd.DataFrame()","metadata":{"execution":{"iopub.status.busy":"2024-01-09T12:11:08.625731Z","iopub.execute_input":"2024-01-09T12:11:08.625980Z","iopub.status.idle":"2024-01-09T12:11:08.634289Z","shell.execute_reply.started":"2024-01-09T12:11:08.625956Z","shell.execute_reply":"2024-01-09T12:11:08.633175Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"count_vectorizer = CountVectorizer(ngram_range=(1, 2))\nX_tokenizer_train = count_vectorizer.fit_transform(merged_data['essay'])\nX_tokenizer_test = count_vectorizer.transform(test_essaysdf['essay'])\ncount_vectorizer.get_feature_names_out() #ADDED\ny = merged_data['score']","metadata":{"execution":{"iopub.status.busy":"2024-01-09T12:11:08.635414Z","iopub.execute_input":"2024-01-09T12:11:08.635742Z","iopub.status.idle":"2024-01-09T12:11:10.085903Z","shell.execute_reply.started":"2024-01-09T12:11:08.635716Z","shell.execute_reply":"2024-01-09T12:11:10.084891Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"dense_array_train = X_tokenizer_train.toarray()\ndense_array_test = X_tokenizer_test.toarray()\n\ndf_train_features = pd.DataFrame(dense_array_train, columns=[f'feature {i}' for i in range(dense_array_train.shape[1])])\ndf_test_features = pd.DataFrame(dense_array_test, columns=[f'feature {i}' for i in range(dense_array_test.shape[1])])\n\ndf_train = pd.concat([df_train, df_train_features], axis=1)\ndf_test = pd.concat([df_test, df_test_features], axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T12:11:10.087371Z","iopub.execute_input":"2024-01-09T12:11:10.087771Z","iopub.status.idle":"2024-01-09T12:11:10.098444Z","shell.execute_reply.started":"2024-01-09T12:11:10.087732Z","shell.execute_reply":"2024-01-09T12:11:10.097599Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"df_train_index = train_essaysdf['id']\ndf_test_index = test_essaysdf['id']\ndf_train.loc[:, 'id'] = df_train_index\ndf_test.loc[:, 'id'] = df_test_index\ndf_train","metadata":{"execution":{"iopub.status.busy":"2024-01-09T12:11:10.099621Z","iopub.execute_input":"2024-01-09T12:11:10.100224Z","iopub.status.idle":"2024-01-09T12:11:10.124750Z","shell.execute_reply.started":"2024-01-09T12:11:10.100194Z","shell.execute_reply":"2024-01-09T12:11:10.123805Z"},"trusted":true},"execution_count":65,"outputs":[{"execution_count":65,"output_type":"execute_result","data":{"text/plain":"      feature 0  feature 1  feature 2  feature 3  feature 4  feature 5  \\\n0            53          8         12          8          6          8   \n1            61          8         12         19         11          5   \n2            64          6          8          8          6          6   \n3            44          6          7          8          4          6   \n4            49          3         15         11          9          2   \n...         ...        ...        ...        ...        ...        ...   \n2466         49          5          4          9         13          7   \n2467         74         16          9         21         10          7   \n2468         42          5          9          8          7          5   \n2469         80          5         19         12         11          6   \n2470         47          5          5         10          4          4   \n\n      feature 6  feature 7  feature 8  feature 9  ...  feature 282  \\\n0             1          4          4          2  ...            0   \n1             1          1          0          1  ...            0   \n2             4         10          4          4  ...            0   \n3             6          1          1          2  ...            0   \n4             3          3          0          1  ...            0   \n...         ...        ...        ...        ...  ...          ...   \n2466          7          0          1          0  ...            0   \n2467          4          4          1          1  ...            0   \n2468          3          1          1          1  ...            0   \n2469         11          2          1          5  ...            0   \n2470          3          2          5          1  ...            0   \n\n      feature 283  feature 284  feature 285  feature 286  feature 287  \\\n0               0            0            0            0            0   \n1               0            0            0            0            0   \n2               0            0            0            0            0   \n3               0            0            0            0            0   \n4               0            0            0            0            0   \n...           ...          ...          ...          ...          ...   \n2466            0            0            0            0            0   \n2467            0            0            0            0            0   \n2468            0            0            0            0            0   \n2469            0            0            0            0            0   \n2470            0            0            0            0            0   \n\n      feature 288  feature 289  feature 290        id  \n0               0            0            0  001519c8  \n1               0            0            0  0022f953  \n2               0            0            0  0042269b  \n3               0            0            0  0059420b  \n4               0            0            0  0075873a  \n...           ...          ...          ...       ...  \n2466            0            0            0  ffb8c745  \n2467            0            0            0  ffbef7e5  \n2468            0            0            0  ffccd6fd  \n2469            0            0            0  ffec5b38  \n2470            0            0            0  fff05981  \n\n[2471 rows x 292 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature 0</th>\n      <th>feature 1</th>\n      <th>feature 2</th>\n      <th>feature 3</th>\n      <th>feature 4</th>\n      <th>feature 5</th>\n      <th>feature 6</th>\n      <th>feature 7</th>\n      <th>feature 8</th>\n      <th>feature 9</th>\n      <th>...</th>\n      <th>feature 282</th>\n      <th>feature 283</th>\n      <th>feature 284</th>\n      <th>feature 285</th>\n      <th>feature 286</th>\n      <th>feature 287</th>\n      <th>feature 288</th>\n      <th>feature 289</th>\n      <th>feature 290</th>\n      <th>id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>53</td>\n      <td>8</td>\n      <td>12</td>\n      <td>8</td>\n      <td>6</td>\n      <td>8</td>\n      <td>1</td>\n      <td>4</td>\n      <td>4</td>\n      <td>2</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>001519c8</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>61</td>\n      <td>8</td>\n      <td>12</td>\n      <td>19</td>\n      <td>11</td>\n      <td>5</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0022f953</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>64</td>\n      <td>6</td>\n      <td>8</td>\n      <td>8</td>\n      <td>6</td>\n      <td>6</td>\n      <td>4</td>\n      <td>10</td>\n      <td>4</td>\n      <td>4</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0042269b</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>44</td>\n      <td>6</td>\n      <td>7</td>\n      <td>8</td>\n      <td>4</td>\n      <td>6</td>\n      <td>6</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0059420b</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>49</td>\n      <td>3</td>\n      <td>15</td>\n      <td>11</td>\n      <td>9</td>\n      <td>2</td>\n      <td>3</td>\n      <td>3</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0075873a</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2466</th>\n      <td>49</td>\n      <td>5</td>\n      <td>4</td>\n      <td>9</td>\n      <td>13</td>\n      <td>7</td>\n      <td>7</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>ffb8c745</td>\n    </tr>\n    <tr>\n      <th>2467</th>\n      <td>74</td>\n      <td>16</td>\n      <td>9</td>\n      <td>21</td>\n      <td>10</td>\n      <td>7</td>\n      <td>4</td>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>ffbef7e5</td>\n    </tr>\n    <tr>\n      <th>2468</th>\n      <td>42</td>\n      <td>5</td>\n      <td>9</td>\n      <td>8</td>\n      <td>7</td>\n      <td>5</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>ffccd6fd</td>\n    </tr>\n    <tr>\n      <th>2469</th>\n      <td>80</td>\n      <td>5</td>\n      <td>19</td>\n      <td>12</td>\n      <td>11</td>\n      <td>6</td>\n      <td>11</td>\n      <td>2</td>\n      <td>1</td>\n      <td>5</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>ffec5b38</td>\n    </tr>\n    <tr>\n      <th>2470</th>\n      <td>47</td>\n      <td>5</td>\n      <td>5</td>\n      <td>10</td>\n      <td>4</td>\n      <td>4</td>\n      <td>3</td>\n      <td>2</td>\n      <td>5</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>fff05981</td>\n    </tr>\n  </tbody>\n</table>\n<p>2471 rows × 292 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train_agg_fe_df = train_logs.groupby(\"id\")[['down_time', 'up_time', 'action_time', 'cursor_position', 'word_count']].agg(['mean', 'std', 'min', 'max', 'last', 'first', 'sem', 'median', 'sum'])\ntrain_agg_fe_df.columns = ['_'.join(x) for x in train_agg_fe_df.columns]\ntrain_agg_fe_df = train_agg_fe_df.add_prefix(\"tmp_\")\ntrain_agg_fe_df.reset_index(inplace=True)\nprint(train_agg_fe_df)\ntest_agg_fe_df = test_logs.groupby(\"id\")[['down_time', 'up_time', 'action_time', 'cursor_position', 'word_count']].agg(['mean', 'std', 'min', 'max', 'last', 'first', 'sem', 'median', 'sum'])\ntest_agg_fe_df.columns = ['_'.join(x) for x in test_agg_fe_df.columns]\ntest_agg_fe_df = test_agg_fe_df.add_prefix(\"tmp_\")\ntest_agg_fe_df.reset_index(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T12:11:10.126234Z","iopub.execute_input":"2024-01-09T12:11:10.126630Z","iopub.status.idle":"2024-01-09T12:11:14.852095Z","shell.execute_reply.started":"2024-01-09T12:11:10.126594Z","shell.execute_reply":"2024-01-09T12:11:14.850882Z"},"trusted":true},"execution_count":66,"outputs":[{"name":"stdout","text":"            id  tmp_down_time_mean  tmp_down_time_std  tmp_down_time_min  \\\n0     001519c8        8.481808e+05      395112.665961               4526   \n1     0022f953        5.188553e+05      384959.404177              30623   \n2     0042269b        8.284918e+05      489500.796565               4441   \n3     0059420b        7.854830e+05      385205.014399              41395   \n4     0075873a        7.133542e+05      405576.409034              78470   \n...        ...                 ...                ...                ...   \n2466  ffb8c745        7.361019e+05      503882.020411              22467   \n2467  ffbef7e5        8.419662e+05      512744.745940              21732   \n2468  ffccd6fd        1.229015e+06      514320.848199              23482   \n2469  ffec5b38        5.765185e+05      334477.976640              19885   \n2470  fff05981        1.076844e+06      581107.759299              39727   \n\n      tmp_down_time_max  tmp_down_time_last  tmp_down_time_first  \\\n0               1801877             1801877                 4526   \n1               1788842             1788842                30623   \n2               1771219             1771219                 4441   \n3               1404394             1404394                41395   \n4               1662390             1662390                78470   \n...                 ...                 ...                  ...   \n2466            1791581             1791581                22467   \n2467            1799124             1799124                21732   \n2468            1959273             1959273                23482   \n2469            1508335             1508335                19885   \n2470            2070065             2070065                39727   \n\n      tmp_down_time_sem  tmp_down_time_median  tmp_down_time_sum  ...  \\\n0           7813.679400              891716.0         2168798234  ...   \n1           7771.013336              407673.0         1273271023  ...   \n2           7611.375322              759582.0         3426641982  ...   \n3           9765.334758              848240.5         1222211589  ...   \n4           8061.699636              686588.0         1805499474  ...   \n...                 ...                   ...                ...  ...   \n2466        7319.568976              735095.0         3488386746  ...   \n2467       10048.025509              748404.5         2192480040  ...   \n2468        9293.100430             1506525.0         3764472937  ...   \n2469        5874.366278              573912.0         1869073112  ...   \n2470        9659.672066             1137100.0         3897099261  ...   \n\n      tmp_cursor_position_sum  tmp_word_count_mean  tmp_word_count_std  \\\n0                     1818445           128.116152           76.498372   \n1                     1904809           182.714751           97.763090   \n2                     3025946           194.772727          108.935068   \n3                      844188           103.618895           61.882250   \n4                     1518729           125.082971           77.255054   \n...                       ...                  ...                 ...   \n2466                  3667989           256.353661          118.093794   \n2467                  2661493           223.013057          126.627934   \n2468                  4009729           157.589292           61.236111   \n2469                  3866542           205.917027          118.473905   \n2470                  2132220           105.105278           67.890428   \n\n      tmp_word_count_min  tmp_word_count_max  tmp_word_count_last  \\\n0                      0                 256                  255   \n1                      0                 323                  320   \n2                      0                 404                  404   \n3                      0                 206                  206   \n4                      0                 252                  252   \n...                  ...                 ...                  ...   \n2466                   0                 461                  273   \n2467                   0                 438                  438   \n2468                   0                 201                  201   \n2469                   0                 413                  413   \n2470                   0                 241                  240   \n\n      tmp_word_count_first  tmp_word_count_sem  tmp_word_count_median  \\\n0                        0            1.512819                  132.0   \n1                        0            1.973502                  186.0   \n2                        0            1.693860                  193.0   \n3                        0            1.568777                  108.5   \n4                        0            1.535610                  113.0   \n...                    ...                 ...                    ...   \n2466                     0            1.715472                  297.0   \n2467                     0            2.481470                  227.5   \n2468                     0            1.106456                  201.0   \n2469                     0            2.080732                  205.0   \n2470                     0            1.128533                   96.0   \n\n      tmp_word_count_sum  \n0                 327593  \n1                 448382  \n2                 805580  \n3                 161231  \n4                 316585  \n...                  ...  \n2466             1214860  \n2467              580726  \n2468              482696  \n2469              667583  \n2470              380376  \n\n[2471 rows x 46 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import BertTokenizer, BertModel","metadata":{"execution":{"iopub.status.busy":"2024-01-09T12:11:14.855761Z","iopub.execute_input":"2024-01-09T12:11:14.856090Z","iopub.status.idle":"2024-01-09T12:11:14.860506Z","shell.execute_reply.started":"2024-01-09T12:11:14.856061Z","shell.execute_reply":"2024-01-09T12:11:14.859392Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# 使用 Preprocessor 类进行文本嵌入\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained('bert-base-uncased').to(device)\n\ntrain=pd.DataFrame(columns=['bert_embedding'])\ntest=pd.DataFrame(columns=['bert_embedding'])\n\nfor row in train_essays.items():\n    text = row[1]\n    # 使用分词器对文本进行编码\n    input_ids = tokenizer.encode(text, return_tensors='pt',max_length=512, truncation=True).to(device)\n    \n    # 获取BERT模型的输出\n    with torch.no_grad():\n        outputs = model(input_ids)\n    \n    # 提取最后一层的隐藏状态\n    last_hidden_states = outputs.last_hidden_state\n    sentence_vector = last_hidden_states.mean(dim=1)\n    # 在这里可以使用last_hidden_states作为特征进行进一步的处理\n    # ...\n\n    # 例如，将嵌入表示添加到DataFrame中\n    train.loc[len(train)] = [sentence_vector.cpu().numpy()]\nprint(train)\n\nfor row in test_essays.items():\n    text = row[1]\n    # 使用分词器对文本进行编码\n    input_ids = tokenizer.encode(text, return_tensors='pt',max_length=512, truncation=True).to(device)\n    \n    # 获取BERT模型的输出\n    with torch.no_grad():\n        outputs = model(input_ids)\n    \n    # 提取最后一层的隐藏状态\n    last_hidden_states = outputs.last_hidden_state\n    sentence_vector = last_hidden_states.mean(dim=1)\n    # 在这里可以使用last_hidden_states作为特征进行进一步的处理\n    # ...\n\n    # 例如，将嵌入表示添加到DataFrame中\n    test.loc[len(test)] = [sentence_vector.cpu().numpy()]","metadata":{"execution":{"iopub.status.busy":"2024-01-09T12:11:14.862088Z","iopub.execute_input":"2024-01-09T12:11:14.862503Z","iopub.status.idle":"2024-01-09T12:13:17.864005Z","shell.execute_reply.started":"2024-01-09T12:11:14.862466Z","shell.execute_reply":"2024-01-09T12:13:17.863065Z"},"trusted":true},"execution_count":68,"outputs":[{"name":"stdout","text":"                                         bert_embedding\n0     [[0.10270646, 0.31191358, 0.21102765, -0.70029...\n1     [[0.08346323, 0.24788868, 0.2624147, -0.646316...\n2     [[0.11100255, 0.32315826, 0.19025856, -0.70320...\n3     [[0.11526609, 0.32681537, 0.20288798, -0.69565...\n4     [[0.015251498, 0.23293838, 0.4429636, -0.66533...\n...                                                 ...\n2466  [[0.023321021, 0.25657976, 0.36098096, -0.6651...\n2467  [[0.030726206, 0.24117437, 0.40864456, -0.6456...\n2468  [[0.13183317, 0.39548647, 0.030384887, -0.7385...\n2469  [[0.09183662, 0.31349424, 0.22715673, -0.69854...\n2470  [[0.10618518, 0.34759587, 0.14362203, -0.77023...\n\n[2471 rows x 1 columns]\n123\n123\n123\n","output_type":"stream"}]},{"cell_type":"code","source":"train_agg_fe_df = train_logs.groupby(\"id\")[['down_time', 'up_time', 'action_time', 'cursor_position', 'word_count']].agg(['mean', 'std', 'min', 'max', 'last', 'first', 'sem', 'median', 'sum'])\ntrain_agg_fe_df.columns = ['_'.join(x) for x in train_agg_fe_df.columns]\ntrain_agg_fe_df = train_agg_fe_df.add_prefix(\"tmp_\")\ntrain_agg_fe_df.reset_index(inplace=True)\nprint(train_agg_fe_df)\ntest_agg_fe_df = test_logs.groupby(\"id\")[['down_time', 'up_time', 'action_time', 'cursor_position', 'word_count']].agg(['mean', 'std', 'min', 'max', 'last', 'first', 'sem', 'median', 'sum'])\ntest_agg_fe_df.columns = ['_'.join(x) for x in test_agg_fe_df.columns]\ntest_agg_fe_df = test_agg_fe_df.add_prefix(\"tmp_\")\ntest_agg_fe_df.reset_index(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T12:13:17.865492Z","iopub.execute_input":"2024-01-09T12:13:17.865877Z","iopub.status.idle":"2024-01-09T12:13:22.586912Z","shell.execute_reply.started":"2024-01-09T12:13:17.865839Z","shell.execute_reply":"2024-01-09T12:13:22.585925Z"},"trusted":true},"execution_count":69,"outputs":[{"name":"stdout","text":"            id  tmp_down_time_mean  tmp_down_time_std  tmp_down_time_min  \\\n0     001519c8        8.481808e+05      395112.665961               4526   \n1     0022f953        5.188553e+05      384959.404177              30623   \n2     0042269b        8.284918e+05      489500.796565               4441   \n3     0059420b        7.854830e+05      385205.014399              41395   \n4     0075873a        7.133542e+05      405576.409034              78470   \n...        ...                 ...                ...                ...   \n2466  ffb8c745        7.361019e+05      503882.020411              22467   \n2467  ffbef7e5        8.419662e+05      512744.745940              21732   \n2468  ffccd6fd        1.229015e+06      514320.848199              23482   \n2469  ffec5b38        5.765185e+05      334477.976640              19885   \n2470  fff05981        1.076844e+06      581107.759299              39727   \n\n      tmp_down_time_max  tmp_down_time_last  tmp_down_time_first  \\\n0               1801877             1801877                 4526   \n1               1788842             1788842                30623   \n2               1771219             1771219                 4441   \n3               1404394             1404394                41395   \n4               1662390             1662390                78470   \n...                 ...                 ...                  ...   \n2466            1791581             1791581                22467   \n2467            1799124             1799124                21732   \n2468            1959273             1959273                23482   \n2469            1508335             1508335                19885   \n2470            2070065             2070065                39727   \n\n      tmp_down_time_sem  tmp_down_time_median  tmp_down_time_sum  ...  \\\n0           7813.679400              891716.0         2168798234  ...   \n1           7771.013336              407673.0         1273271023  ...   \n2           7611.375322              759582.0         3426641982  ...   \n3           9765.334758              848240.5         1222211589  ...   \n4           8061.699636              686588.0         1805499474  ...   \n...                 ...                   ...                ...  ...   \n2466        7319.568976              735095.0         3488386746  ...   \n2467       10048.025509              748404.5         2192480040  ...   \n2468        9293.100430             1506525.0         3764472937  ...   \n2469        5874.366278              573912.0         1869073112  ...   \n2470        9659.672066             1137100.0         3897099261  ...   \n\n      tmp_cursor_position_sum  tmp_word_count_mean  tmp_word_count_std  \\\n0                     1818445           128.116152           76.498372   \n1                     1904809           182.714751           97.763090   \n2                     3025946           194.772727          108.935068   \n3                      844188           103.618895           61.882250   \n4                     1518729           125.082971           77.255054   \n...                       ...                  ...                 ...   \n2466                  3667989           256.353661          118.093794   \n2467                  2661493           223.013057          126.627934   \n2468                  4009729           157.589292           61.236111   \n2469                  3866542           205.917027          118.473905   \n2470                  2132220           105.105278           67.890428   \n\n      tmp_word_count_min  tmp_word_count_max  tmp_word_count_last  \\\n0                      0                 256                  255   \n1                      0                 323                  320   \n2                      0                 404                  404   \n3                      0                 206                  206   \n4                      0                 252                  252   \n...                  ...                 ...                  ...   \n2466                   0                 461                  273   \n2467                   0                 438                  438   \n2468                   0                 201                  201   \n2469                   0                 413                  413   \n2470                   0                 241                  240   \n\n      tmp_word_count_first  tmp_word_count_sem  tmp_word_count_median  \\\n0                        0            1.512819                  132.0   \n1                        0            1.973502                  186.0   \n2                        0            1.693860                  193.0   \n3                        0            1.568777                  108.5   \n4                        0            1.535610                  113.0   \n...                    ...                 ...                    ...   \n2466                     0            1.715472                  297.0   \n2467                     0            2.481470                  227.5   \n2468                     0            1.106456                  201.0   \n2469                     0            2.080732                  205.0   \n2470                     0            1.128533                   96.0   \n\n      tmp_word_count_sum  \n0                 327593  \n1                 448382  \n2                 805580  \n3                 161231  \n4                 316585  \n...                  ...  \n2466             1214860  \n2467              580726  \n2468              482696  \n2469              667583  \n2470              380376  \n\n[2471 rows x 46 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"from collections import defaultdict\n\nclass Preprocessor:\n    \n    def __init__(self, seed):\n        self.seed = seed\n        \n        self.activities = ['Input', 'Remove/Cut', 'Nonproduction', 'Replace', 'Paste']\n        self.events = ['q', 'Space', 'Backspace', 'Shift', 'ArrowRight', 'Leftclick', 'ArrowLeft', '.', ',', \n              'ArrowDown', 'ArrowUp', 'Enter', 'CapsLock', \"'\", 'Delete', 'Unidentified']\n        self.text_changes = ['q', ' ', 'NoChange', '.', ',', '\\n', \"'\", '\"', '-', '?', ';', '=', '/', '\\\\', ':']\n        self.punctuations = ['\"', '.', ',', \"'\", '-', ';', ':', '?', '!', '<', '>', '/',\n                        '@', '#', '$', '%', '^', '&', '*', '(', ')', '_', '+']\n        self.gaps = [1, 2, 3, 5, 10, 20, 50, 100]\n        \n        self.idf = defaultdict(float)\n#         self.gaps = [1, 2]\n    \n    def activity_counts(self, df):\n        tmp_df = df.groupby('id').agg({'activity': list}).reset_index()\n        ret = list()\n        for li in tqdm(tmp_df['activity'].values):\n            items = list(Counter(li).items())\n            di = dict()\n            for k in self.activities:\n                di[k] = 0\n            for item in items:\n                k, v = item[0], item[1]\n                if k in di:\n                    di[k] = v\n            ret.append(di)\n        ret = pd.DataFrame(ret)\n        cols = [f'activity_{i}_count' for i in range(len(ret.columns))]\n        ret.columns = cols\n\n        cnts = ret.sum(1)\n\n        for col in cols:\n            if col in self.idf.keys():\n                idf = self.idf[col]\n            else:\n                idf = df.shape[0] / (ret[col].sum() + 1)\n                idf = np.log(idf)\n                self.idf[col] = idf\n\n            ret[col] = 1 + np.log(ret[col] / cnts)\n            ret[col] *= idf\n\n        return ret\n\n\n    def event_counts(self, df, colname):\n        tmp_df = df.groupby('id').agg({colname: list}).reset_index()\n        ret = list()\n        for li in tqdm(tmp_df[colname].values):\n            items = list(Counter(li).items())\n            di = dict()\n            for k in self.events:\n                di[k] = 0\n            for item in items:\n                k, v = item[0], item[1]\n                if k in di:\n                    di[k] = v\n            ret.append(di)\n        ret = pd.DataFrame(ret)\n        cols = [f'{colname}_{i}_count' for i in range(len(ret.columns))]\n        ret.columns = cols\n\n        cnts = ret.sum(1)\n\n        for col in cols:\n            if col in self.idf.keys():\n                idf = self.idf[col]\n            else:\n                idf = df.shape[0] / (ret[col].sum() + 1)\n                idf = np.log(idf)\n                self.idf[col] = idf\n            \n            ret[col] = 1 + np.log(ret[col] / cnts)\n            ret[col] *= idf\n\n        return ret\n\n\n    def text_change_counts(self, df):\n        tmp_df = df.groupby('id').agg({'text_change': list}).reset_index()\n        ret = list()\n        for li in tqdm(tmp_df['text_change'].values):\n            items = list(Counter(li).items())\n            di = dict()\n            for k in self.text_changes:\n                di[k] = 0\n            for item in items:\n                k, v = item[0], item[1]\n                if k in di:\n                    di[k] = v\n            ret.append(di)\n        ret = pd.DataFrame(ret)\n        cols = [f'text_change_{i}_count' for i in range(len(ret.columns))]\n        ret.columns = cols\n\n        cnts = ret.sum(1)\n\n        for col in cols:\n            if col in self.idf.keys():\n                idf = self.idf[col]\n            else:\n                idf = df.shape[0] / (ret[col].sum() + 1)\n                idf = np.log(idf)\n                self.idf[col] = idf\n            \n            ret[col] = 1 + np.log(ret[col] / cnts)\n            ret[col] *= idf\n            \n        return ret\n\n    def match_punctuations(self, df):\n        tmp_df = df.groupby('id').agg({'down_event': list}).reset_index()\n        ret = list()\n        for li in tqdm(tmp_df['down_event'].values):\n            cnt = 0\n            items = list(Counter(li).items())\n            for item in items:\n                k, v = item[0], item[1]\n                if k in self.punctuations:\n                    cnt += v\n            ret.append(cnt)\n        ret = pd.DataFrame({'punct_cnt': ret})\n        return ret\n\n\n    def get_input_words(self, df):\n        tmp_df = df[(~df['text_change'].str.contains('=>'))&(df['text_change'] != 'NoChange')].reset_index(drop=True)\n        tmp_df = tmp_df.groupby('id').agg({'text_change': list}).reset_index()\n        tmp_df['text_change'] = tmp_df['text_change'].apply(lambda x: ''.join(x))\n        tmp_df['text_change'] = tmp_df['text_change'].apply(lambda x: re.findall(r'q+', x))\n        tmp_df['input_word_count'] = tmp_df['text_change'].apply(len)\n        tmp_df['input_word_length_mean'] = tmp_df['text_change'].apply(lambda x: np.mean([len(i) for i in x] if len(x) > 0 else 0))\n        tmp_df['input_word_length_max'] = tmp_df['text_change'].apply(lambda x: np.max([len(i) for i in x] if len(x) > 0 else 0))\n        tmp_df['input_word_length_std'] = tmp_df['text_change'].apply(lambda x: np.std([len(i) for i in x] if len(x) > 0 else 0))\n        tmp_df.drop(['text_change'], axis=1, inplace=True)\n        return tmp_df\n    \n    def make_feats(self, df):\n        \n        print(\"Starting to engineer features\")\n        \n        # initialize features dataframe\n        feats = pd.DataFrame({'id': df['id'].unique().tolist()})\n        \n        # get shifted features\n        # time shift\n        print(\"Engineering time data\")\n        for gap in self.gaps:\n            print(f\"> for gap {gap}\")\n            df[f'up_time_shift{gap}'] = df.groupby('id')['up_time'].shift(gap)\n            df[f'action_time_gap{gap}'] = df['down_time'] - df[f'up_time_shift{gap}']\n        df.drop(columns=[f'up_time_shift{gap}' for gap in self.gaps], inplace=True)\n\n        # cursor position shift\n        print(\"Engineering cursor position data\")\n        for gap in self.gaps:\n            print(f\"> for gap {gap}\")\n            df[f'cursor_position_shift{gap}'] = df.groupby('id')['cursor_position'].shift(gap)\n            df[f'cursor_position_change{gap}'] = df['cursor_position'] - df[f'cursor_position_shift{gap}']\n            df[f'cursor_position_abs_change{gap}'] = np.abs(df[f'cursor_position_change{gap}'])\n        df.drop(columns=[f'cursor_position_shift{gap}' for gap in self.gaps], inplace=True)\n\n        # word count shift\n        print(\"Engineering word count data\")\n        for gap in self.gaps:\n            print(f\"> for gap {gap}\")\n            df[f'word_count_shift{gap}'] = df.groupby('id')['word_count'].shift(gap)\n            df[f'word_count_change{gap}'] = df['word_count'] - df[f'word_count_shift{gap}']\n            df[f'word_count_abs_change{gap}'] = np.abs(df[f'word_count_change{gap}'])\n        df.drop(columns=[f'word_count_shift{gap}' for gap in self.gaps], inplace=True)\n        \n        # get aggregate statistical features\n        print(\"Engineering statistical summaries for features\")\n        # [(feature name, [ stat summaries to add ])]\n        feats_stat = [\n            ('event_id', ['max']),\n            ('up_time', ['max']),\n            ('action_time', ['max', 'min', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew']),\n            ('activity', ['nunique']),\n            ('down_event', ['nunique']),\n            ('up_event', ['nunique']),\n            ('text_change', ['nunique']),\n            ('cursor_position', ['nunique', 'max', 'quantile', 'sem', 'mean']),\n            ('word_count', ['nunique', 'max', 'quantile', 'sem', 'mean'])]\n        for gap in self.gaps:\n            feats_stat.extend([\n                (f'action_time_gap{gap}', ['max', 'min', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew']),\n                (f'cursor_position_change{gap}', ['max', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew']),\n                (f'word_count_change{gap}', ['max', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew'])\n            ])\n        \n        pbar = tqdm(feats_stat)\n        for item in pbar:\n            colname, methods = item[0], item[1]\n            for method in methods:\n                pbar.set_postfix()\n                if isinstance(method, str):\n                    method_name = method\n                else:\n                    method_name = method.__name__\n                    \n                pbar.set_postfix(column=colname, method=method_name)\n                tmp_df = df.groupby(['id']).agg({colname: method}).reset_index().rename(columns={colname: f'{colname}_{method_name}'})\n                feats = feats.merge(tmp_df, on='id', how='left')\n\n        # counts\n        print(\"Engineering activity counts data\")\n        tmp_df = self.activity_counts(df)\n        feats = pd.concat([feats, tmp_df], axis=1)\n        \n        print(\"Engineering event counts data\")\n        tmp_df = self.event_counts(df, 'down_event')\n        feats = pd.concat([feats, tmp_df], axis=1)\n        tmp_df = self.event_counts(df, 'up_event')\n        feats = pd.concat([feats, tmp_df], axis=1)\n        \n        print(\"Engineering text change counts data\")\n        tmp_df = self.text_change_counts(df)\n        feats = pd.concat([feats, tmp_df], axis=1)\n        \n        print(\"Engineering punctuation counts data\")\n        tmp_df = self.match_punctuations(df)\n        feats = pd.concat([feats, tmp_df], axis=1)\n\n        # input words\n        print(\"Engineering input words data\")\n        tmp_df = self.get_input_words(df)\n        feats = pd.merge(feats, tmp_df, on='id', how='left')\n\n        # compare feats\n        print(\"Engineering ratios data\")\n        feats['word_time_ratio'] = feats['word_count_max'] / feats['up_time_max']\n        feats['word_event_ratio'] = feats['word_count_max'] / feats['event_id_max']\n        feats['event_time_ratio'] = feats['event_id_max']  / feats['up_time_max']\n        feats['idle_time_ratio'] = feats['action_time_gap1_sum'] / feats['up_time_max']\n        \n        print(\"Done!\")\n        return feats","metadata":{"execution":{"iopub.status.busy":"2024-01-09T12:13:22.588674Z","iopub.execute_input":"2024-01-09T12:13:22.589286Z","iopub.status.idle":"2024-01-09T12:13:22.640933Z","shell.execute_reply.started":"2024-01-09T12:13:22.589243Z","shell.execute_reply":"2024-01-09T12:13:22.640195Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"preprocessor = Preprocessor(seed=42)\n\nprint(\"Engineering features for training data\")\n\nother_train_feats = preprocessor.make_feats(train_logs)\n\nprint()\nprint(\"-\"*25)\nprint(\"Engineering features for test data\")\nprint(\"-\"*25)\nother_test_feats = preprocessor.make_feats(test_logs)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T12:13:22.641932Z","iopub.execute_input":"2024-01-09T12:13:22.642225Z","iopub.status.idle":"2024-01-09T12:17:35.579508Z","shell.execute_reply.started":"2024-01-09T12:13:22.642199Z","shell.execute_reply":"2024-01-09T12:17:35.578571Z"},"trusted":true},"execution_count":71,"outputs":[{"name":"stdout","text":"Engineering features for training data\nStarting to engineer features\nEngineering time data\n> for gap 1\n> for gap 2\n> for gap 3\n> for gap 5\n> for gap 10\n> for gap 20\n> for gap 50\n> for gap 100\nEngineering cursor position data\n> for gap 1\n> for gap 2\n> for gap 3\n> for gap 5\n> for gap 10\n> for gap 20\n> for gap 50\n> for gap 100\nEngineering word count data\n> for gap 1\n> for gap 2\n> for gap 3\n> for gap 5\n> for gap 10\n> for gap 20\n> for gap 50\n> for gap 100\nEngineering statistical summaries for features\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 33/33 [03:10<00:00,  5.78s/it, column=word_count_change100, method=skew]         \n","output_type":"stream"},{"name":"stdout","text":"Engineering activity counts data\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2471/2471 [00:00<00:00, 5212.39it/s]\n/opt/conda/lib/python3.10/site-packages/pandas/core/arraylike.py:396: RuntimeWarning: divide by zero encountered in log\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Engineering event counts data\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2471/2471 [00:00<00:00, 5762.58it/s]\n/opt/conda/lib/python3.10/site-packages/pandas/core/arraylike.py:396: RuntimeWarning: divide by zero encountered in log\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n100%|██████████| 2471/2471 [00:00<00:00, 5518.83it/s]\n/opt/conda/lib/python3.10/site-packages/pandas/core/arraylike.py:396: RuntimeWarning: divide by zero encountered in log\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Engineering text change counts data\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2471/2471 [00:00<00:00, 5801.43it/s]\n/opt/conda/lib/python3.10/site-packages/pandas/core/arraylike.py:396: RuntimeWarning: divide by zero encountered in log\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Engineering punctuation counts data\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2471/2471 [00:00<00:00, 5605.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Engineering input words data\nEngineering ratios data\nDone!\n\n-------------------------\nEngineering features for test data\n-------------------------\nStarting to engineer features\nEngineering time data\n> for gap 1\n> for gap 2\n> for gap 3\n> for gap 5\n> for gap 10\n> for gap 20\n> for gap 50\n> for gap 100\nEngineering cursor position data\n> for gap 1\n> for gap 2\n> for gap 3\n> for gap 5\n> for gap 10\n> for gap 20\n> for gap 50\n> for gap 100\nEngineering word count data\n> for gap 1\n> for gap 2\n> for gap 3\n> for gap 5\n> for gap 10\n> for gap 20\n> for gap 50\n> for gap 100\nEngineering statistical summaries for features\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 33/33 [00:01<00:00, 23.68it/s, column=word_count_change100, method=skew]         \n","output_type":"stream"},{"name":"stdout","text":"Engineering activity counts data\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 3/3 [00:00<00:00, 24916.66it/s]\n/opt/conda/lib/python3.10/site-packages/pandas/core/arraylike.py:396: RuntimeWarning: divide by zero encountered in log\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Engineering event counts data\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 3/3 [00:00<00:00, 20164.92it/s]\n/opt/conda/lib/python3.10/site-packages/pandas/core/arraylike.py:396: RuntimeWarning: divide by zero encountered in log\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n100%|██████████| 3/3 [00:00<00:00, 34473.73it/s]\n/opt/conda/lib/python3.10/site-packages/pandas/core/arraylike.py:396: RuntimeWarning: divide by zero encountered in log\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Engineering text change counts data\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 3/3 [00:00<00:00, 24966.10it/s]\n/opt/conda/lib/python3.10/site-packages/pandas/core/arraylike.py:396: RuntimeWarning: divide by zero encountered in log\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Engineering punctuation counts data\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 3/3 [00:00<00:00, 27962.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Engineering input words data\nEngineering ratios data\nDone!\n","output_type":"stream"}]},{"cell_type":"code","source":"df_train_all = pd.DataFrame()\ndf_test_all = pd.DataFrame()\n\ndf_train_all = df_train.merge(train_agg_fe_df,on='id')\ndf_test_all = df_test.merge(test_agg_fe_df,on='id')","metadata":{"execution":{"iopub.status.busy":"2024-01-09T12:17:35.580768Z","iopub.execute_input":"2024-01-09T12:17:35.581102Z","iopub.status.idle":"2024-01-09T12:17:35.594773Z","shell.execute_reply.started":"2024-01-09T12:17:35.581076Z","shell.execute_reply":"2024-01-09T12:17:35.593950Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"train_expanded = train[\"bert_embedding\"].apply(lambda x: pd.Series(x.flatten()))\ntest_expanded = test[\"bert_embedding\"].apply(lambda x: pd.Series(x.flatten()))","metadata":{"execution":{"iopub.status.busy":"2024-01-09T12:17:35.596058Z","iopub.execute_input":"2024-01-09T12:17:35.596438Z","iopub.status.idle":"2024-01-09T12:17:35.892427Z","shell.execute_reply.started":"2024-01-09T12:17:35.596402Z","shell.execute_reply":"2024-01-09T12:17:35.891439Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"def q1(x):\n    return x.quantile(0.25)\ndef q3(x):\n    return x.quantile(0.75)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T12:17:35.893831Z","iopub.execute_input":"2024-01-09T12:17:35.894270Z","iopub.status.idle":"2024-01-09T12:17:35.899313Z","shell.execute_reply.started":"2024-01-09T12:17:35.894227Z","shell.execute_reply":"2024-01-09T12:17:35.898253Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"AGGREGATIONS = ['count', 'mean', 'std', 'min', 'max', 'first', 'last', 'sem', q1, 'median', q3, 'skew', 'sum']\n\ndef split_essays_into_sentences(df):\n    essay_df = df\n    essay_df['id'] = essay_df.index\n    essay_df['sent'] = essay_df['essay'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!',str(x)))\n    essay_df = essay_df.explode('sent')\n    essay_df['sent'] = essay_df['sent'].apply(lambda x: x.replace('\\n','').strip())\n    # Number of characters in sentences\n    essay_df['sent_len'] = essay_df['sent'].apply(lambda x: len(x))\n    # Number of words in sentences\n    essay_df['sent_word_count'] = essay_df['sent'].apply(lambda x: len(x.split(' ')))\n    essay_df = essay_df[essay_df.columns.tolist()].reset_index(drop=True)\n    return essay_df\n\ndef compute_sentence_aggregations(df):\n    sent_agg_df = pd.concat(\n        [df[['id','sent_len']].groupby(['id']).agg(AGGREGATIONS), df[['id','sent_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1\n    )\n    sent_agg_df.columns = ['_'.join(x) for x in sent_agg_df.columns]\n    sent_agg_df['id'] = sent_agg_df.index\n    sent_agg_df = sent_agg_df.reset_index(drop=True)\n    sent_agg_df.drop(columns=[\"sent_word_count_count\"], inplace=True)\n    sent_agg_df = sent_agg_df.rename(columns={\"sent_len_count\":\"sent_count\"})\n    return sent_agg_df\n\ndef split_essays_into_paragraphs(df):\n    essay_df = df\n    essay_df['id'] = essay_df.index\n    essay_df['paragraph'] = essay_df['essay'].apply(lambda x: str(x).split('\\n'))\n    essay_df = essay_df.explode('paragraph')\n    # Number of characters in paragraphs\n    essay_df['paragraph_len'] = essay_df['paragraph'].apply(lambda x: len(x)) \n    # Number of words in paragraphs\n    essay_df['paragraph_word_count'] = essay_df['paragraph'].apply(lambda x: len(x.split(' ')))\n    essay_df = essay_df[essay_df.paragraph_len!=0].reset_index(drop=True)\n    return essay_df\n\ndef compute_paragraph_aggregations(df):\n    paragraph_agg_df = pd.concat(\n        [df[['id','paragraph_len']].groupby(['id']).agg(AGGREGATIONS), df[['id','paragraph_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1\n    ) \n    paragraph_agg_df.columns = ['_'.join(x) for x in paragraph_agg_df.columns]\n    paragraph_agg_df['id'] = paragraph_agg_df.index\n    paragraph_agg_df = paragraph_agg_df.reset_index(drop=True)\n    paragraph_agg_df.drop(columns=[\"paragraph_word_count_count\"], inplace=True)\n    paragraph_agg_df = paragraph_agg_df.rename(columns={\"paragraph_len_count\":\"paragraph_count\"})\n    return paragraph_agg_df","metadata":{"execution":{"iopub.status.busy":"2024-01-09T12:17:35.900845Z","iopub.execute_input":"2024-01-09T12:17:35.901836Z","iopub.status.idle":"2024-01-09T12:17:35.919856Z","shell.execute_reply.started":"2024-01-09T12:17:35.901796Z","shell.execute_reply":"2024-01-09T12:17:35.918841Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"train_sent_df = split_essays_into_sentences(train_essaysdf)\ntrain_sent_agg_df = compute_sentence_aggregations(train_sent_df)\n\ntrain_paragraph_df = split_essays_into_paragraphs(train_essaysdf)\ntrain_paragraph_agg_df = compute_paragraph_aggregations(train_paragraph_df)\n\ntest_sent_agg_df = compute_sentence_aggregations(split_essays_into_sentences(test_essaysdf))\ntest_paragraph_agg_df = compute_paragraph_aggregations(split_essays_into_paragraphs(test_essaysdf))\n\ntrain_paragraph_agg_df.loc[:, 'id'] = df_train_index\ntrain_sent_agg_df.loc[:, 'id'] = df_train_index\n\ntest_paragraph_agg_df.loc[:, 'id'] = df_test_index\ntest_sent_agg_df.loc[:, 'id'] = df_test_index","metadata":{"execution":{"iopub.status.busy":"2024-01-09T12:17:35.921003Z","iopub.execute_input":"2024-01-09T12:17:35.921292Z","iopub.status.idle":"2024-01-09T12:17:47.923707Z","shell.execute_reply.started":"2024-01-09T12:17:35.921265Z","shell.execute_reply":"2024-01-09T12:17:47.922370Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"new_train_feats = pd.DataFrame()\nnew_test_feats = pd.DataFrame()\n\nnew_train_feats = train_paragraph_agg_df.merge(df_train_all,on='id')\nnew_train_feats = new_train_feats.merge(train_sent_agg_df,on='id')\n\nnew_test_feats = test_paragraph_agg_df.merge(df_test_all,on='id')\nnew_test_feats = new_test_feats.merge(test_sent_agg_df,on='id')\n\ntrain_feats = pd.DataFrame()\ntest_feats = pd.DataFrame()\n\ntrain_feats = new_train_feats.merge(other_train_feats,on='id')\ntest_feats = new_test_feats.merge(other_test_feats,on='id')","metadata":{"execution":{"iopub.status.busy":"2024-01-09T12:17:47.925361Z","iopub.execute_input":"2024-01-09T12:17:47.925767Z","iopub.status.idle":"2024-01-09T12:17:47.979795Z","shell.execute_reply.started":"2024-01-09T12:17:47.925727Z","shell.execute_reply":"2024-01-09T12:17:47.978434Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"data = []\n\nfor logs in [train_logs, test_logs]:\n    logs['up_time_lagged'] = logs.groupby('id')['up_time'].shift(1).fillna(logs['down_time'])\n    logs['time_diff'] = abs(logs['down_time'] - logs['up_time_lagged']) / 1000\n\n    group = logs.groupby('id')['time_diff']\n    largest_lantency = group.max()\n    smallest_lantency = group.min()\n    median_lantency = group.median()\n    initial_pause = logs.groupby('id')['down_time'].first() / 1000\n    pauses_half_sec = group.apply(lambda x: ((x > 0.5) & (x < 1)).sum())\n    pauses_1_sec = group.apply(lambda x: ((x > 1) & (x < 1.5)).sum())\n    pauses_1_half_sec = group.apply(lambda x: ((x > 1.5) & (x < 2)).sum())\n    pauses_2_sec = group.apply(lambda x: ((x > 2) & (x < 3)).sum())\n    pauses_3_sec = group.apply(lambda x: (x > 3).sum())\n\n    data.append(pd.DataFrame({\n        'id': logs['id'].unique(),\n        'largest_lantency': largest_lantency,\n        'smallest_lantency': smallest_lantency,\n        'median_lantency': median_lantency,\n        'initial_pause': initial_pause,\n        'pauses_half_sec': pauses_half_sec,\n        'pauses_1_sec': pauses_1_sec,\n        'pauses_1_half_sec': pauses_1_half_sec,\n        'pauses_2_sec': pauses_2_sec,\n        'pauses_3_sec': pauses_3_sec,\n    }).reset_index(drop=True))\n\ntrain_eD592674, test_eD592674 = data\n\ntrain_feats = train_feats.merge(train_eD592674, on='id', how='left')\ntest_feats = test_feats.merge(test_eD592674, on='id', how='left')\ntrain_feats = train_feats.merge(train_scores, on='id', how='left')","metadata":{"execution":{"iopub.status.busy":"2024-01-09T12:17:47.981498Z","iopub.execute_input":"2024-01-09T12:17:47.981802Z","iopub.status.idle":"2024-01-09T12:17:55.860779Z","shell.execute_reply.started":"2024-01-09T12:17:47.981775Z","shell.execute_reply":"2024-01-09T12:17:55.859951Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"train_feats = pd.concat([train_feats,train_expanded],axis=1)\ntest_feats = pd.concat([test_feats,test_expanded],axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T12:17:55.861984Z","iopub.execute_input":"2024-01-09T12:17:55.862306Z","iopub.status.idle":"2024-01-09T12:17:55.874226Z","shell.execute_reply.started":"2024-01-09T12:17:55.862277Z","shell.execute_reply":"2024-01-09T12:17:55.873378Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"train_feats.shape","metadata":{"execution":{"iopub.status.busy":"2024-01-09T12:17:55.875394Z","iopub.execute_input":"2024-01-09T12:17:55.875692Z","iopub.status.idle":"2024-01-09T12:17:55.882480Z","shell.execute_reply.started":"2024-01-09T12:17:55.875667Z","shell.execute_reply":"2024-01-09T12:17:55.881564Z"},"trusted":true},"execution_count":80,"outputs":[{"execution_count":80,"output_type":"execute_result","data":{"text/plain":"(2471, 1426)"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\n\ntrain_feats['score_class'] = le.fit_transform(train_feats['score'])","metadata":{"execution":{"iopub.status.busy":"2024-01-09T12:17:55.883698Z","iopub.execute_input":"2024-01-09T12:17:55.884112Z","iopub.status.idle":"2024-01-09T12:17:55.892486Z","shell.execute_reply.started":"2024-01-09T12:17:55.884083Z","shell.execute_reply":"2024-01-09T12:17:55.891661Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"target_col = ['score']\n\ndrop_cols = ['id', 'score_class']\ntrain_cols = list()\n\ntrain_cols = [col for col in train_feats.columns if col not in target_col + drop_cols]","metadata":{"execution":{"iopub.status.busy":"2024-01-09T12:17:55.893651Z","iopub.execute_input":"2024-01-09T12:17:55.893977Z","iopub.status.idle":"2024-01-09T12:17:55.901917Z","shell.execute_reply.started":"2024-01-09T12:17:55.893943Z","shell.execute_reply":"2024-01-09T12:17:55.901061Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"nan_cols = train_feats.columns[train_feats.isna().any()].tolist()\n\n\nfor col in nan_cols:\n    mode_value_train = train_feats[col].mode()[0] \n    train_feats[col].fillna(mode_value_train, inplace=True)\n    \nfor col in test_feats.columns[test_feats.isna().any()].tolist():\n\n    most_frequent_value_train = train_feats[col].mode()[0]\n    \n\n    test_feats[col].fillna(most_frequent_value_train, inplace=True)\n\ntrain_feats.shape, test_feats.shape  ","metadata":{"execution":{"iopub.status.busy":"2024-01-09T12:17:55.903064Z","iopub.execute_input":"2024-01-09T12:17:55.904035Z","iopub.status.idle":"2024-01-09T12:17:56.027965Z","shell.execute_reply.started":"2024-01-09T12:17:55.903987Z","shell.execute_reply":"2024-01-09T12:17:56.027062Z"},"trusted":true},"execution_count":83,"outputs":[{"execution_count":83,"output_type":"execute_result","data":{"text/plain":"((2471, 1427), (3, 1425))"},"metadata":{}}]},{"cell_type":"markdown","source":"# **Model**","metadata":{}},{"cell_type":"code","source":"import gc\nimport ctypes\nimport torch\ndef clean_memory():\n    gc.collect()\n    ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n    torch.cuda.empty_cache()\nclean_memory()","metadata":{"execution":{"iopub.status.busy":"2024-01-09T12:17:56.029126Z","iopub.execute_input":"2024-01-09T12:17:56.029434Z","iopub.status.idle":"2024-01-09T12:17:56.379120Z","shell.execute_reply.started":"2024-01-09T12:17:56.029406Z","shell.execute_reply":"2024-01-09T12:17:56.378326Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"models_dict = {}\nscores = []\n\ntest_predict_list = []\nbest_params = {'boosting_type': 'gbdt', \n               'metric': 'rmse',\n               'reg_alpha': 0.003188447814669599, \n               'reg_lambda': 0.0010228604507564066, \n               'colsample_bytree': 0.5420247656839267, \n               'subsample': 0.9778252382803456, \n               'feature_fraction': 0.8,\n               'bagging_freq': 1,\n               'bagging_fraction': 0.75,\n               'learning_rate': 0.01716485155812008, \n               'num_leaves': 19, \n               'min_child_samples': 46,\n               'verbosity': -1,\n               'random_state': 42,\n               'n_estimators': 500,\n               'device_type': 'cpu'}\n\nfor i in range(5): \n    kf = model_selection.KFold(n_splits=10, random_state=42 + i, shuffle=True)\n\n    oof_valid_preds = np.zeros(train_feats.shape[0], )\n\n    X_test = test_feats[train_cols]\n\n\n    for fold, (train_idx, valid_idx) in enumerate(kf.split(train_feats)):\n\n        print(\"==-\"* 50)\n        print(\"Fold : \", fold)\n\n        X_train, y_train = train_feats.iloc[train_idx][train_cols], train_feats.iloc[train_idx][target_col]\n        X_valid, y_valid = train_feats.iloc[valid_idx][train_cols], train_feats.iloc[valid_idx][target_col]\n\n        print(\"Trian :\", X_train.shape, y_train.shape)\n        print(\"Valid :\", X_valid.shape, y_valid.shape)\n\n        params = {\n            \"objective\": \"regression\",\n            \"metric\": \"rmse\",\n            'random_state': 42,\n            \"n_estimators\" : 12001,\n            \"verbosity\": -1,\n            \"device_type\": \"cpu\",\n            **best_params\n        }\n\n        model = lgb.LGBMRegressor(**params)\n\n        early_stopping_callback = lgb.early_stopping(200, first_metric_only=True, verbose=False)\n        verbose_callback = lgb.callback.record_evaluation({})\n\n        model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)],  \n                  callbacks=[early_stopping_callback, verbose_callback],\n        )\n\n        valid_predict = model.predict(X_valid)\n        oof_valid_preds[valid_idx] = valid_predict\n\n        test_predict = model.predict(X_test)\n        test_predict_list.append(test_predict)\n\n        score = metrics.mean_squared_error(y_valid, valid_predict, squared=False)\n        print(\"Fold RMSE Score : \", score)\n\n        models_dict[f'{fold}_{i}'] = model\n\n\n    oof_score = metrics.mean_squared_error(train_feats[target_col], oof_valid_preds, squared=False)\n    scores.append(oof_score)\n    print(\"OOF RMSE Score : \", oof_score)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T12:17:56.384681Z","iopub.execute_input":"2024-01-09T12:17:56.385268Z","iopub.status.idle":"2024-01-09T12:36:47.004447Z","shell.execute_reply.started":"2024-01-09T12:17:56.385239Z","shell.execute_reply":"2024-01-09T12:36:47.003490Z"},"trusted":true},"execution_count":85,"outputs":[{"name":"stdout","text":"==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  0\nTrian : (2223, 1424) (2223, 1)\nValid : (248, 1424) (248, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.5849775813804726\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  1\nTrian : (2224, 1424) (2224, 1)\nValid : (247, 1424) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.5240872598599342\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  2\nTrian : (2224, 1424) (2224, 1)\nValid : (247, 1424) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.6732925713076392\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  3\nTrian : (2224, 1424) (2224, 1)\nValid : (247, 1424) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.6215947315650063\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  4\nTrian : (2224, 1424) (2224, 1)\nValid : (247, 1424) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.5955553489329495\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  5\nTrian : (2224, 1424) (2224, 1)\nValid : (247, 1424) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.6046529330864301\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  6\nTrian : (2224, 1424) (2224, 1)\nValid : (247, 1424) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.6482338998156011\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  7\nTrian : (2224, 1424) (2224, 1)\nValid : (247, 1424) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.6453270220387332\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  8\nTrian : (2224, 1424) (2224, 1)\nValid : (247, 1424) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.6328763738054316\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  9\nTrian : (2224, 1424) (2224, 1)\nValid : (247, 1424) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.5700695541890783\nOOF RMSE Score :  0.6114652989543359\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  0\nTrian : (2223, 1424) (2223, 1)\nValid : (248, 1424) (248, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.5880398229332195\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  1\nTrian : (2224, 1424) (2224, 1)\nValid : (247, 1424) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.594686970954377\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  2\nTrian : (2224, 1424) (2224, 1)\nValid : (247, 1424) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.5671264523703571\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  3\nTrian : (2224, 1424) (2224, 1)\nValid : (247, 1424) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.5970217262561013\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  4\nTrian : (2224, 1424) (2224, 1)\nValid : (247, 1424) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.6061920271425325\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  5\nTrian : (2224, 1424) (2224, 1)\nValid : (247, 1424) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.6507042158875954\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  6\nTrian : (2224, 1424) (2224, 1)\nValid : (247, 1424) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.6348464006900923\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  7\nTrian : (2224, 1424) (2224, 1)\nValid : (247, 1424) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.5803648632989167\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  8\nTrian : (2224, 1424) (2224, 1)\nValid : (247, 1424) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.6580029879468511\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  9\nTrian : (2224, 1424) (2224, 1)\nValid : (247, 1424) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.6358166546375855\nOOF RMSE Score :  0.6119937279666409\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  0\nTrian : (2223, 1424) (2223, 1)\nValid : (248, 1424) (248, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.6917348330575926\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  1\nTrian : (2224, 1424) (2224, 1)\nValid : (247, 1424) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.590100073399747\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  2\nTrian : (2224, 1424) (2224, 1)\nValid : (247, 1424) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.5794871435611116\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  3\nTrian : (2224, 1424) (2224, 1)\nValid : (247, 1424) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.5917059151941226\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  4\nTrian : (2224, 1424) (2224, 1)\nValid : (247, 1424) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.5822061978639933\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  5\nTrian : (2224, 1424) (2224, 1)\nValid : (247, 1424) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.6062563032477168\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  6\nTrian : (2224, 1424) (2224, 1)\nValid : (247, 1424) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.6833400850131319\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  7\nTrian : (2224, 1424) (2224, 1)\nValid : (247, 1424) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.5593451820524419\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  8\nTrian : (2224, 1424) (2224, 1)\nValid : (247, 1424) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.6453800662550749\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  9\nTrian : (2224, 1424) (2224, 1)\nValid : (247, 1424) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.6106538908629491\nOOF RMSE Score :  0.615531055314198\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  0\nTrian : (2223, 1424) (2223, 1)\nValid : (248, 1424) (248, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.6199128997745086\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  1\nTrian : (2224, 1424) (2224, 1)\nValid : (247, 1424) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.5902726549536391\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  2\nTrian : (2224, 1424) (2224, 1)\nValid : (247, 1424) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.6302650168767181\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  3\nTrian : (2224, 1424) (2224, 1)\nValid : (247, 1424) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.6003156392103203\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  4\nTrian : (2224, 1424) (2224, 1)\nValid : (247, 1424) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.6506646829608405\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  5\nTrian : (2224, 1424) (2224, 1)\nValid : (247, 1424) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.5564869623229763\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  6\nTrian : (2224, 1424) (2224, 1)\nValid : (247, 1424) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.6394323775684017\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  7\nTrian : (2224, 1424) (2224, 1)\nValid : (247, 1424) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.6175197927677062\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  8\nTrian : (2224, 1424) (2224, 1)\nValid : (247, 1424) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.6685268352386214\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  9\nTrian : (2224, 1424) (2224, 1)\nValid : (247, 1424) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.5557550481042863\nOOF RMSE Score :  0.6139539858705308\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  0\nTrian : (2223, 1424) (2223, 1)\nValid : (248, 1424) (248, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.6048660538049478\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  1\nTrian : (2224, 1424) (2224, 1)\nValid : (247, 1424) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.5831348537659451\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  2\nTrian : (2224, 1424) (2224, 1)\nValid : (247, 1424) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.6184122139514129\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  3\nTrian : (2224, 1424) (2224, 1)\nValid : (247, 1424) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.6055503586494762\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  4\nTrian : (2224, 1424) (2224, 1)\nValid : (247, 1424) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.5865712213772877\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  5\nTrian : (2224, 1424) (2224, 1)\nValid : (247, 1424) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.6517701609246751\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  6\nTrian : (2224, 1424) (2224, 1)\nValid : (247, 1424) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.6042821183646064\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  7\nTrian : (2224, 1424) (2224, 1)\nValid : (247, 1424) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.6226809674085122\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  8\nTrian : (2224, 1424) (2224, 1)\nValid : (247, 1424) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.6503069555235028\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  9\nTrian : (2224, 1424) (2224, 1)\nValid : (247, 1424) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.6162900070789705\nOOF RMSE Score :  0.614772828090254\n","output_type":"stream"}]},{"cell_type":"code","source":"np.mean(scores)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T12:36:47.005942Z","iopub.execute_input":"2024-01-09T12:36:47.006553Z","iopub.status.idle":"2024-01-09T12:36:47.012476Z","shell.execute_reply.started":"2024-01-09T12:36:47.006523Z","shell.execute_reply":"2024-01-09T12:36:47.011546Z"},"trusted":true},"execution_count":86,"outputs":[{"execution_count":86,"output_type":"execute_result","data":{"text/plain":"0.6135433792391919"},"metadata":{}}]},{"cell_type":"code","source":"test_feats['score'] = np.mean(test_predict_list, axis=0)\npubliclgbm_pred = test_feats[['id', 'score']]\npubliclgbm_pred","metadata":{"execution":{"iopub.status.busy":"2024-01-09T12:36:47.013638Z","iopub.execute_input":"2024-01-09T12:36:47.013915Z","iopub.status.idle":"2024-01-09T12:36:47.028098Z","shell.execute_reply.started":"2024-01-09T12:36:47.013884Z","shell.execute_reply":"2024-01-09T12:36:47.027267Z"},"trusted":true},"execution_count":87,"outputs":[{"execution_count":87,"output_type":"execute_result","data":{"text/plain":"         id     score\n0  0000aaaa  1.680139\n1  2222bbbb  1.536805\n2  4444cccc  1.635930","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000aaaa</td>\n      <td>1.680139</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2222bbbb</td>\n      <td>1.536805</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4444cccc</td>\n      <td>1.635930</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# **Ensemble**","metadata":{}},{"cell_type":"code","source":"import random\nseed=2023\nnp.random.seed(seed)\nrandom.seed(seed)\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom xgboost import XGBRegressor\n\n\nfrom sklearn.model_selection import KFold,StratifiedKFold\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.impute import SimpleImputer","metadata":{"execution":{"iopub.status.busy":"2024-01-09T12:36:47.029309Z","iopub.execute_input":"2024-01-09T12:36:47.029623Z","iopub.status.idle":"2024-01-09T12:36:47.035243Z","shell.execute_reply.started":"2024-01-09T12:36:47.029591Z","shell.execute_reply":"2024-01-09T12:36:47.034460Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"def make_model():\n    \n    catboost_params= {\n        \"iterations\": 5000,\n        \"early_stopping_rounds\": 50,\n        \"depth\": 6,\n        \"loss_function\": \"RMSE\",\n        \"random_seed\": 42,\n        \"silent\": True\n    }\n    \n    lgbm_params= {\n        'n_estimators': 1024,\n        'learning_rate': 0.005,\n        'metric': 'rmse',\n        'random_state': 42,\n        'force_col_wise': True,\n        'verbosity': 0\n    }\n    \n    xgboost_params ={\n        \"max_depth\": 4,\n        \"learning_rate\": 0.1,\n        \"objective\": \"reg:squarederror\",\n        \"n_estimators\": 1000,  # 修正此处的参数名\n        \"eval_metric\": \"rmse\",\n        \"seed\": 42\n    }\n    \n    model1 = LGBMRegressor(**lgbm_params)\n    \n    model2 = CatBoostRegressor(**catboost_params)\n    \n    model3  = XGBRegressor(**xgboost_params)\n    \n    models = []\n    models.append((model1, 'lgbm'))\n    models.append((model2, 'catboost'))\n    models.append((model3, 'xgboost'))\n    \n    return models","metadata":{"execution":{"iopub.status.busy":"2024-01-09T12:36:47.036440Z","iopub.execute_input":"2024-01-09T12:36:47.037288Z","iopub.status.idle":"2024-01-09T12:36:47.045998Z","shell.execute_reply.started":"2024-01-09T12:36:47.037254Z","shell.execute_reply":"2024-01-09T12:36:47.045191Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"num_folds = 10\n\nmodel_with_scaled_features = ['xgboost']\n\nblending_weights = {\n    'lgbm': 0.6,\n    'catboost': 0.3,\n    'xgboost': 0.1,\n}","metadata":{"execution":{"iopub.status.busy":"2024-01-09T12:36:47.047067Z","iopub.execute_input":"2024-01-09T12:36:47.048242Z","iopub.status.idle":"2024-01-09T12:36:47.060325Z","shell.execute_reply.started":"2024-01-09T12:36:47.048216Z","shell.execute_reply":"2024-01-09T12:36:47.059331Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"code","source":"train_feats.columns = train_feats.columns.astype(str)\ntest_feats.columns = test_feats.columns.astype(str)\ntrain_cols = [str(col) for col in train_cols]","metadata":{"execution":{"iopub.status.busy":"2024-01-09T12:36:47.061470Z","iopub.execute_input":"2024-01-09T12:36:47.061734Z","iopub.status.idle":"2024-01-09T12:36:47.072965Z","shell.execute_reply.started":"2024-01-09T12:36:47.061710Z","shell.execute_reply":"2024-01-09T12:36:47.071997Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"train_feats","metadata":{"execution":{"iopub.status.busy":"2024-01-09T12:36:47.074033Z","iopub.execute_input":"2024-01-09T12:36:47.074314Z","iopub.status.idle":"2024-01-09T12:36:47.107958Z","shell.execute_reply.started":"2024-01-09T12:36:47.074289Z","shell.execute_reply":"2024-01-09T12:36:47.107089Z"},"trusted":true},"execution_count":92,"outputs":[{"execution_count":92,"output_type":"execute_result","data":{"text/plain":"      paragraph_count  paragraph_len_mean  paragraph_len_std  \\\n0                   3          508.000000         134.208793   \n1                   6          278.166667          98.554384   \n2                   6          429.500000         101.087586   \n3                   3          384.000000          56.471232   \n4                   5          283.400000         232.336609   \n...               ...                 ...                ...   \n2466                4          407.750000          91.790976   \n2467                6          387.166667         178.941797   \n2468                3          918.333333         939.787387   \n2469                5          509.600000         122.681702   \n2470                6          247.166667         137.836739   \n\n      paragraph_len_min  paragraph_len_max  paragraph_len_first  \\\n0                   390                654                  390   \n1                   176                462                  240   \n2                   296                568                  491   \n3                   347                449                  347   \n4                    23                627                  351   \n...                 ...                ...                  ...   \n2466                301                514                  372   \n2467                144                648                  144   \n2468                327               2002                  426   \n2469                380                672                  672   \n2470                 59                412                   59   \n\n      paragraph_len_last  paragraph_len_sem  paragraph_len_q1  \\\n0                    480          77.485483            435.00   \n1                    284          40.234659            228.75   \n2                    296          41.268834            356.75   \n3                    356          32.603681            351.50   \n4                     23         103.904090            124.00   \n...                  ...                ...               ...   \n2466                 301          45.895488            354.25   \n2467                 228          73.052683            274.00   \n2468                2002         542.586501            376.50   \n2469                 380          54.864925            394.00   \n2470                 224          56.271613            171.50   \n\n      paragraph_len_median  ...       759       760       761       762  \\\n0                    480.0  ...  0.543616  0.748779 -1.423322 -0.553759   \n1                    261.0  ...  0.620695  0.726740 -1.445171 -0.435152   \n2                    444.5  ...  0.520756  0.750219 -1.417216 -0.586341   \n3                    356.0  ...  0.532827  0.740490 -1.436533 -0.572211   \n4                    292.0  ...  0.675400  0.604619 -1.481418 -0.379651   \n...                    ...  ...       ...       ...       ...       ...   \n2466                 408.0  ...  0.649013  0.675741 -1.480808 -0.407907   \n2467                 424.0  ...  0.691299  0.637945 -1.519715 -0.354740   \n2468                 426.0  ...  0.478948  0.766021 -1.420998 -0.577128   \n2469                 540.0  ...  0.535551  0.731311 -1.442836 -0.572768   \n2470                 229.5  ...  0.460001  0.733167 -1.417120 -0.589561   \n\n           763       764       765       766       767  score_class  \n0    -0.001282 -0.295785 -0.774561  0.301522  0.085557            6  \n1    -0.038977 -0.492697 -0.800179  0.328693  0.057892            6  \n2    -0.024095 -0.249560 -0.753874  0.308836  0.124730           11  \n3     0.016308 -0.249350 -0.752671  0.312077  0.053063            3  \n4    -0.113376 -0.490850 -0.772265  0.343489  0.085378            7  \n...        ...       ...       ...       ...       ...          ...  \n2466 -0.113343 -0.412374 -0.763036  0.364216  0.059319            6  \n2467 -0.129195 -0.512009 -0.793757  0.338959  0.067130            7  \n2468  0.080904 -0.258818 -0.827178  0.186180  0.118300            2  \n2469 -0.023611 -0.239196 -0.740059  0.334475  0.083982            9  \n2470  0.033755 -0.091817 -0.771426  0.250261  0.130504            7  \n\n[2471 rows x 1427 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>paragraph_count</th>\n      <th>paragraph_len_mean</th>\n      <th>paragraph_len_std</th>\n      <th>paragraph_len_min</th>\n      <th>paragraph_len_max</th>\n      <th>paragraph_len_first</th>\n      <th>paragraph_len_last</th>\n      <th>paragraph_len_sem</th>\n      <th>paragraph_len_q1</th>\n      <th>paragraph_len_median</th>\n      <th>...</th>\n      <th>759</th>\n      <th>760</th>\n      <th>761</th>\n      <th>762</th>\n      <th>763</th>\n      <th>764</th>\n      <th>765</th>\n      <th>766</th>\n      <th>767</th>\n      <th>score_class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>508.000000</td>\n      <td>134.208793</td>\n      <td>390</td>\n      <td>654</td>\n      <td>390</td>\n      <td>480</td>\n      <td>77.485483</td>\n      <td>435.00</td>\n      <td>480.0</td>\n      <td>...</td>\n      <td>0.543616</td>\n      <td>0.748779</td>\n      <td>-1.423322</td>\n      <td>-0.553759</td>\n      <td>-0.001282</td>\n      <td>-0.295785</td>\n      <td>-0.774561</td>\n      <td>0.301522</td>\n      <td>0.085557</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6</td>\n      <td>278.166667</td>\n      <td>98.554384</td>\n      <td>176</td>\n      <td>462</td>\n      <td>240</td>\n      <td>284</td>\n      <td>40.234659</td>\n      <td>228.75</td>\n      <td>261.0</td>\n      <td>...</td>\n      <td>0.620695</td>\n      <td>0.726740</td>\n      <td>-1.445171</td>\n      <td>-0.435152</td>\n      <td>-0.038977</td>\n      <td>-0.492697</td>\n      <td>-0.800179</td>\n      <td>0.328693</td>\n      <td>0.057892</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>6</td>\n      <td>429.500000</td>\n      <td>101.087586</td>\n      <td>296</td>\n      <td>568</td>\n      <td>491</td>\n      <td>296</td>\n      <td>41.268834</td>\n      <td>356.75</td>\n      <td>444.5</td>\n      <td>...</td>\n      <td>0.520756</td>\n      <td>0.750219</td>\n      <td>-1.417216</td>\n      <td>-0.586341</td>\n      <td>-0.024095</td>\n      <td>-0.249560</td>\n      <td>-0.753874</td>\n      <td>0.308836</td>\n      <td>0.124730</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>384.000000</td>\n      <td>56.471232</td>\n      <td>347</td>\n      <td>449</td>\n      <td>347</td>\n      <td>356</td>\n      <td>32.603681</td>\n      <td>351.50</td>\n      <td>356.0</td>\n      <td>...</td>\n      <td>0.532827</td>\n      <td>0.740490</td>\n      <td>-1.436533</td>\n      <td>-0.572211</td>\n      <td>0.016308</td>\n      <td>-0.249350</td>\n      <td>-0.752671</td>\n      <td>0.312077</td>\n      <td>0.053063</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>283.400000</td>\n      <td>232.336609</td>\n      <td>23</td>\n      <td>627</td>\n      <td>351</td>\n      <td>23</td>\n      <td>103.904090</td>\n      <td>124.00</td>\n      <td>292.0</td>\n      <td>...</td>\n      <td>0.675400</td>\n      <td>0.604619</td>\n      <td>-1.481418</td>\n      <td>-0.379651</td>\n      <td>-0.113376</td>\n      <td>-0.490850</td>\n      <td>-0.772265</td>\n      <td>0.343489</td>\n      <td>0.085378</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2466</th>\n      <td>4</td>\n      <td>407.750000</td>\n      <td>91.790976</td>\n      <td>301</td>\n      <td>514</td>\n      <td>372</td>\n      <td>301</td>\n      <td>45.895488</td>\n      <td>354.25</td>\n      <td>408.0</td>\n      <td>...</td>\n      <td>0.649013</td>\n      <td>0.675741</td>\n      <td>-1.480808</td>\n      <td>-0.407907</td>\n      <td>-0.113343</td>\n      <td>-0.412374</td>\n      <td>-0.763036</td>\n      <td>0.364216</td>\n      <td>0.059319</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>2467</th>\n      <td>6</td>\n      <td>387.166667</td>\n      <td>178.941797</td>\n      <td>144</td>\n      <td>648</td>\n      <td>144</td>\n      <td>228</td>\n      <td>73.052683</td>\n      <td>274.00</td>\n      <td>424.0</td>\n      <td>...</td>\n      <td>0.691299</td>\n      <td>0.637945</td>\n      <td>-1.519715</td>\n      <td>-0.354740</td>\n      <td>-0.129195</td>\n      <td>-0.512009</td>\n      <td>-0.793757</td>\n      <td>0.338959</td>\n      <td>0.067130</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>2468</th>\n      <td>3</td>\n      <td>918.333333</td>\n      <td>939.787387</td>\n      <td>327</td>\n      <td>2002</td>\n      <td>426</td>\n      <td>2002</td>\n      <td>542.586501</td>\n      <td>376.50</td>\n      <td>426.0</td>\n      <td>...</td>\n      <td>0.478948</td>\n      <td>0.766021</td>\n      <td>-1.420998</td>\n      <td>-0.577128</td>\n      <td>0.080904</td>\n      <td>-0.258818</td>\n      <td>-0.827178</td>\n      <td>0.186180</td>\n      <td>0.118300</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2469</th>\n      <td>5</td>\n      <td>509.600000</td>\n      <td>122.681702</td>\n      <td>380</td>\n      <td>672</td>\n      <td>672</td>\n      <td>380</td>\n      <td>54.864925</td>\n      <td>394.00</td>\n      <td>540.0</td>\n      <td>...</td>\n      <td>0.535551</td>\n      <td>0.731311</td>\n      <td>-1.442836</td>\n      <td>-0.572768</td>\n      <td>-0.023611</td>\n      <td>-0.239196</td>\n      <td>-0.740059</td>\n      <td>0.334475</td>\n      <td>0.083982</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>2470</th>\n      <td>6</td>\n      <td>247.166667</td>\n      <td>137.836739</td>\n      <td>59</td>\n      <td>412</td>\n      <td>59</td>\n      <td>224</td>\n      <td>56.271613</td>\n      <td>171.50</td>\n      <td>229.5</td>\n      <td>...</td>\n      <td>0.460001</td>\n      <td>0.733167</td>\n      <td>-1.417120</td>\n      <td>-0.589561</td>\n      <td>0.033755</td>\n      <td>-0.091817</td>\n      <td>-0.771426</td>\n      <td>0.250261</td>\n      <td>0.130504</td>\n      <td>7</td>\n    </tr>\n  </tbody>\n</table>\n<p>2471 rows × 1427 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train_feats.replace([np.inf, -np.inf], np.nan, inplace=True)\n\ndef RMSE(y_true,y_pred):\n    return np.sqrt(np.mean((y_true-y_pred)**2))\n\nmodels_and_errors_dict = {}\n\nfor model, model_type in make_model():\n    \n    oof_pred=np.zeros(train_feats.shape[0])        \n\n    kf = KFold(n_splits=num_folds, shuffle=True, random_state=seed + num_folds)\n\n    for fold, (train_idx, valid_idx) in enumerate(kf.split(train_feats)):\n\n        print(f'--- Fold #{fold} ---')       \n        X_test = test_feats[train_cols]\n        # Split data into train and test sets\n        X_train, y_train = train_feats.iloc[train_idx][train_cols], train_feats.iloc[train_idx][target_col]\n        X_test, y_test = train_feats.iloc[valid_idx][train_cols], train_feats.iloc[valid_idx][target_col]\n\n        X_train_copy, X_test_copy = X_train.copy(), X_test.copy()\n\n        print(f'Training a {model_type} model on fold {fold}')\n\n\n        if model_type in model_with_scaled_features:\n\n            imputer = SimpleImputer(strategy='mean')\n            X_train_imputed = imputer.fit_transform(X_train.copy())\n            X_test_imputed = imputer.transform(X_test.copy())\n\n            scaler = MinMaxScaler(feature_range=(-1, 1))\n            X_train_scaled = scaler.fit_transform(X_train_imputed)\n            X_test_scaled = scaler.transform(X_test_imputed)\n\n            X_train_copy = X_train_scaled\n            X_test_copy = X_test_scaled\n\n        if model_type == 'lgb':\n            \n            early_stopping_callback = LGBMRegressor.early_stopping(200, first_metric_only=True, verbose=False)\n            verbose_callback = LGBMRegressor.log_evaluation(100)\n\n            model.fit(X_train_copy, y_train, eval_set=[(X_test_copy, y_test)],  \n                      callbacks=[early_stopping_callback, verbose_callback],)\n        else:\n            model.fit(X_train_copy, y_train)\n\n\n        y_hat = model.predict(X_test_copy)\n\n        oof_pred[valid_idx]=y_hat\n\n        rmse = RMSE(y_test.values.reshape(-1), y_hat)\n        print(f'RMSE: {rmse} on fold {fold}')\n\n\n        if model_type not in models_and_errors_dict:\n            models_and_errors_dict[model_type] = []\n\n        if model_type in model_with_scaled_features:\n            models_and_errors_dict[model_type].append((model, rmse, imputer, scaler,oof_pred))\n        else:\n            models_and_errors_dict[model_type].append((model, rmse, None, None,oof_pred))  ","metadata":{"execution":{"iopub.status.busy":"2024-01-09T12:36:47.109341Z","iopub.execute_input":"2024-01-09T12:36:47.109651Z","iopub.status.idle":"2024-01-09T14:58:38.133753Z","shell.execute_reply.started":"2024-01-09T12:36:47.109623Z","shell.execute_reply":"2024-01-09T14:58:38.132663Z"},"trusted":true},"execution_count":93,"outputs":[{"name":"stdout","text":"--- Fold #0 ---\nTraining a lgbm model on fold 0\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\nRMSE: 0.5915866246197972 on fold 0\n--- Fold #1 ---\nTraining a lgbm model on fold 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\nRMSE: 0.6229203543715441 on fold 1\n--- Fold #2 ---\nTraining a lgbm model on fold 2\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\nRMSE: 0.5799143965420785 on fold 2\n--- Fold #3 ---\nTraining a lgbm model on fold 3\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\nRMSE: 0.6370054652138515 on fold 3\n--- Fold #4 ---\nTraining a lgbm model on fold 4\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\nRMSE: 0.6581942026875308 on fold 4\n--- Fold #5 ---\nTraining a lgbm model on fold 5\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\nRMSE: 0.6663500629815551 on fold 5\n--- Fold #6 ---\nTraining a lgbm model on fold 6\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\nRMSE: 0.6167316633325122 on fold 6\n--- Fold #7 ---\nTraining a lgbm model on fold 7\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\nRMSE: 0.6054299344112196 on fold 7\n--- Fold #8 ---\nTraining a lgbm model on fold 8\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\nRMSE: 0.5923641344954647 on fold 8\n--- Fold #9 ---\nTraining a lgbm model on fold 9\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\nRMSE: 0.6699150882878421 on fold 9\n--- Fold #0 ---\nTraining a catboost model on fold 0\nRMSE: 0.5855517341761023 on fold 0\n--- Fold #1 ---\nTraining a catboost model on fold 1\nRMSE: 0.6070085501747627 on fold 1\n--- Fold #2 ---\nTraining a catboost model on fold 2\nRMSE: 0.5753115632523214 on fold 2\n--- Fold #3 ---\nTraining a catboost model on fold 3\nRMSE: 0.6175967181566 on fold 3\n--- Fold #4 ---\nTraining a catboost model on fold 4\nRMSE: 0.6498335358587458 on fold 4\n--- Fold #5 ---\nTraining a catboost model on fold 5\nRMSE: 0.6580954282772558 on fold 5\n--- Fold #6 ---\nTraining a catboost model on fold 6\nRMSE: 0.6285694470784462 on fold 6\n--- Fold #7 ---\nTraining a catboost model on fold 7\nRMSE: 0.6014761806468116 on fold 7\n--- Fold #8 ---\nTraining a catboost model on fold 8\nRMSE: 0.57030374284486 on fold 8\n--- Fold #9 ---\nTraining a catboost model on fold 9\nRMSE: 0.6520925841037742 on fold 9\n--- Fold #0 ---\nTraining a xgboost model on fold 0\nRMSE: 0.5987595416344031 on fold 0\n--- Fold #1 ---\nTraining a xgboost model on fold 1\nRMSE: 0.6121550703361341 on fold 1\n--- Fold #2 ---\nTraining a xgboost model on fold 2\nRMSE: 0.5943724371720139 on fold 2\n--- Fold #3 ---\nTraining a xgboost model on fold 3\nRMSE: 0.6480844104033779 on fold 3\n--- Fold #4 ---\nTraining a xgboost model on fold 4\nRMSE: 0.6593346301274601 on fold 4\n--- Fold #5 ---\nTraining a xgboost model on fold 5\nRMSE: 0.6756422533728538 on fold 5\n--- Fold #6 ---\nTraining a xgboost model on fold 6\nRMSE: 0.6338252961958017 on fold 6\n--- Fold #7 ---\nTraining a xgboost model on fold 7\nRMSE: 0.6253309982388959 on fold 7\n--- Fold #8 ---\nTraining a xgboost model on fold 8\nRMSE: 0.5964751413407655 on fold 8\n--- Fold #9 ---\nTraining a xgboost model on fold 9\nRMSE: 0.6834638031764316 on fold 9\n","output_type":"stream"}]},{"cell_type":"code","source":"target=train_feats[\"score\"]\nlgb_oof_pred=models_and_errors_dict['lgbm'][9][4]\ncat_oof_pred=models_and_errors_dict['catboost'][9][4]\nxgboost_oof_pred=models_and_errors_dict['xgboost'][9][4]\nmargin=1000\ntarget=target.values\ncurrent_RMSE=RMSE(target,(lgb_oof_pred+cat_oof_pred+xgboost_oof_pred)/3)\nbest_i=0\nbest_j=0\nfor i in range(0,margin):\n    for j in range(0,margin-i):\n        #k=1000-i-j\n        blend_oof_pred=(i*lgb_oof_pred+j*cat_oof_pred+(margin-i-j)*xgboost_oof_pred)/margin\n        if RMSE(target,blend_oof_pred)<current_RMSE:\n            print(f\"current_RMSE:{current_RMSE}\")\n            current_RMSE=RMSE(target,blend_oof_pred)\n            best_i=i\n            best_j=j\n\nblending_weights['lgbm']=best_i/margin\nblending_weights['catboost']=best_j/margin\nblending_weights['xgboost']=(margin-best_i-best_j)/margin\nprint(f\"blending_weights:{blending_weights}\")","metadata":{"execution":{"iopub.status.busy":"2024-01-09T14:58:38.135192Z","iopub.execute_input":"2024-01-09T14:58:38.135525Z","iopub.status.idle":"2024-01-09T14:59:04.469135Z","shell.execute_reply.started":"2024-01-09T14:58:38.135494Z","shell.execute_reply":"2024-01-09T14:59:04.468082Z"},"trusted":true},"execution_count":94,"outputs":[{"name":"stdout","text":"current_RMSE:0.6162508675470676\ncurrent_RMSE:0.6162393822443927\ncurrent_RMSE:0.6162261591171604\ncurrent_RMSE:0.6162129900631403\ncurrent_RMSE:0.6161998750857993\ncurrent_RMSE:0.6161868141885904\ncurrent_RMSE:0.6161738073749525\ncurrent_RMSE:0.6161608546483106\ncurrent_RMSE:0.6161479560120758\ncurrent_RMSE:0.6161351114696452\ncurrent_RMSE:0.6161223210244019\ncurrent_RMSE:0.6161095846797151\ncurrent_RMSE:0.61609690243894\ncurrent_RMSE:0.6160842743054179\ncurrent_RMSE:0.6160717002824759\ncurrent_RMSE:0.6160591803734273\ncurrent_RMSE:0.6160467145815716\ncurrent_RMSE:0.6160343029101937\ncurrent_RMSE:0.6160219453625652\ncurrent_RMSE:0.6160096419419431\ncurrent_RMSE:0.6159973926515708\ncurrent_RMSE:0.6159851974946776\ncurrent_RMSE:0.6159730564744788\ncurrent_RMSE:0.6159609695941752\ncurrent_RMSE:0.6159489368569545\ncurrent_RMSE:0.6159369582659896\ncurrent_RMSE:0.6159250338244396\ncurrent_RMSE:0.6159131635354498\ncurrent_RMSE:0.6159013474021511\ncurrent_RMSE:0.6158895854276605\ncurrent_RMSE:0.615877877615081\ncurrent_RMSE:0.6158662239675015\ncurrent_RMSE:0.6158546244879968\ncurrent_RMSE:0.6158430791796278\ncurrent_RMSE:0.6158315880454412\ncurrent_RMSE:0.6158201510884694\ncurrent_RMSE:0.6158087683117315\ncurrent_RMSE:0.6157974397182314\ncurrent_RMSE:0.61578616531096\ncurrent_RMSE:0.6157749450928933\ncurrent_RMSE:0.6157637790669939\ncurrent_RMSE:0.6157526672362095\ncurrent_RMSE:0.6157416096034746\ncurrent_RMSE:0.6157306061717087\ncurrent_RMSE:0.6157196569438181\ncurrent_RMSE:0.6157087619226943\ncurrent_RMSE:0.615697921111215\ncurrent_RMSE:0.6156871345122438\ncurrent_RMSE:0.6156764021286298\ncurrent_RMSE:0.6156657239632086\ncurrent_RMSE:0.6156551000188012\ncurrent_RMSE:0.6156445302982145\ncurrent_RMSE:0.6156340148042416\ncurrent_RMSE:0.6156235535396613\ncurrent_RMSE:0.6156131465072379\ncurrent_RMSE:0.6156027937097219\ncurrent_RMSE:0.6155924951498498\ncurrent_RMSE:0.6155822508303437\ncurrent_RMSE:0.6155720607539116\ncurrent_RMSE:0.6155619249232472\ncurrent_RMSE:0.6155518433410303\ncurrent_RMSE:0.6155418160099264\ncurrent_RMSE:0.6155318429325868\ncurrent_RMSE:0.6155219241116485\ncurrent_RMSE:0.6155120595497349\ncurrent_RMSE:0.6155022492494545\ncurrent_RMSE:0.6154924932134018\ncurrent_RMSE:0.6154827914441576\ncurrent_RMSE:0.615473143944288\ncurrent_RMSE:0.615463550716345\ncurrent_RMSE:0.6154540117628663\ncurrent_RMSE:0.6154445270863756\ncurrent_RMSE:0.6154350966893826\ncurrent_RMSE:0.6154257205743824\ncurrent_RMSE:0.6154163987438559\ncurrent_RMSE:0.61540713120027\ncurrent_RMSE:0.6153979179460773\ncurrent_RMSE:0.6153887589837163\ncurrent_RMSE:0.6153796543156109\ncurrent_RMSE:0.6153706039441711\ncurrent_RMSE:0.6153616078717927\ncurrent_RMSE:0.6153526661008569\ncurrent_RMSE:0.6153437786337314\ncurrent_RMSE:0.6153349454727686\ncurrent_RMSE:0.6153261666203076\ncurrent_RMSE:0.6153174420786728\ncurrent_RMSE:0.6153087718501744\ncurrent_RMSE:0.6153001559371085\ncurrent_RMSE:0.6152915943417567\ncurrent_RMSE:0.6152830870663866\ncurrent_RMSE:0.6152746341132512\ncurrent_RMSE:0.6152662354845898\ncurrent_RMSE:0.6152578911826267\ncurrent_RMSE:0.6152496012095726\ncurrent_RMSE:0.6152413655676235\ncurrent_RMSE:0.6152331842589613\ncurrent_RMSE:0.6152250572857536\ncurrent_RMSE:0.6152169846501536\ncurrent_RMSE:0.6152089663543004\ncurrent_RMSE:0.6152010024003188\ncurrent_RMSE:0.6151930927903192\ncurrent_RMSE:0.6151852375263976\ncurrent_RMSE:0.6151774366106361\ncurrent_RMSE:0.615169690045102\ncurrent_RMSE:0.6151619978318488\ncurrent_RMSE:0.6151543599729152\ncurrent_RMSE:0.615146776470326\ncurrent_RMSE:0.6151392473260915\ncurrent_RMSE:0.6151317725422077\ncurrent_RMSE:0.6151243521206564\ncurrent_RMSE:0.6151169860634048\ncurrent_RMSE:0.6151096743724063\ncurrent_RMSE:0.6151024170495992\ncurrent_RMSE:0.6150952140969083\ncurrent_RMSE:0.6150880655162434\ncurrent_RMSE:0.6150809713095005\ncurrent_RMSE:0.6150739314785609\ncurrent_RMSE:0.6150669460252918\ncurrent_RMSE:0.6150600149515458\ncurrent_RMSE:0.6150531382591615\ncurrent_RMSE:0.6150463159499628\ncurrent_RMSE:0.6150395480257596\ncurrent_RMSE:0.6150328344883472\ncurrent_RMSE:0.6150261753395067\ncurrent_RMSE:0.6150195705810045\ncurrent_RMSE:0.6150130202145935\ncurrent_RMSE:0.6150065242420111\ncurrent_RMSE:0.6150000826649811\ncurrent_RMSE:0.6149936954852129\ncurrent_RMSE:0.6149873627044012\ncurrent_RMSE:0.6149810843242267\ncurrent_RMSE:0.6149748603463554\ncurrent_RMSE:0.6149686907724392\ncurrent_RMSE:0.6149625756041155\ncurrent_RMSE:0.6149565148430072\ncurrent_RMSE:0.6149505084907231\ncurrent_RMSE:0.6149445565488574\ncurrent_RMSE:0.6149386590189901\ncurrent_RMSE:0.6149328159026867\ncurrent_RMSE:0.6149270272014982\ncurrent_RMSE:0.6149212929169614\ncurrent_RMSE:0.6149156130505988\ncurrent_RMSE:0.6149099876039184\ncurrent_RMSE:0.6149044165784134\ncurrent_RMSE:0.6148988999755634\ncurrent_RMSE:0.6148934377968329\ncurrent_RMSE:0.6148880300436724\ncurrent_RMSE:0.6148826767175178\ncurrent_RMSE:0.6148773778197909\ncurrent_RMSE:0.6148721333518986\ncurrent_RMSE:0.614866943315234\ncurrent_RMSE:0.6148618077111749\ncurrent_RMSE:0.6148567265410859\ncurrent_RMSE:0.614851699806316\ncurrent_RMSE:0.6148467275082008\ncurrent_RMSE:0.6148418096480606\ncurrent_RMSE:0.6148369462272019\ncurrent_RMSE:0.6148321372469165\ncurrent_RMSE:0.6148273827084819\ncurrent_RMSE:0.6148226826131611\ncurrent_RMSE:0.6148180369622026\ncurrent_RMSE:0.6148134457568407\ncurrent_RMSE:0.6148089089982952\ncurrent_RMSE:0.6148044266877714\ncurrent_RMSE:0.61479999882646\ncurrent_RMSE:0.6147956254155377\ncurrent_RMSE:0.6147913064561663\ncurrent_RMSE:0.6147870419494936\ncurrent_RMSE:0.6147828318966526\ncurrent_RMSE:0.6147786762987622\ncurrent_RMSE:0.6147745751569264\ncurrent_RMSE:0.6147705284722351\ncurrent_RMSE:0.6147665362457638\ncurrent_RMSE:0.6147625984785736\ncurrent_RMSE:0.6147587151717107\ncurrent_RMSE:0.6147548863262071\ncurrent_RMSE:0.6147511119430807\ncurrent_RMSE:0.6147473920233345\ncurrent_RMSE:0.6147437265679572\ncurrent_RMSE:0.6147401155779231\ncurrent_RMSE:0.6147365590541919\ncurrent_RMSE:0.614733056997709\ncurrent_RMSE:0.6147296094094052\ncurrent_RMSE:0.6147262162901972\ncurrent_RMSE:0.6147228776409867\ncurrent_RMSE:0.6147195934626611\ncurrent_RMSE:0.6147163637560937\ncurrent_RMSE:0.6147131885221432\ncurrent_RMSE:0.6147100677616534\ncurrent_RMSE:0.6147070014754542\ncurrent_RMSE:0.6147039896643606\ncurrent_RMSE:0.6147010323291734\ncurrent_RMSE:0.6146981294706788\ncurrent_RMSE:0.6146952810896488\ncurrent_RMSE:0.6146924871868404\ncurrent_RMSE:0.6146897477629968\ncurrent_RMSE:0.6146870628188461\ncurrent_RMSE:0.6146844323551023\ncurrent_RMSE:0.6146818563724649\ncurrent_RMSE:0.6146793348716186\ncurrent_RMSE:0.6146768678532342\ncurrent_RMSE:0.6146744553179675\ncurrent_RMSE:0.6146720972664601\ncurrent_RMSE:0.614669793699339\ncurrent_RMSE:0.6146675446172168\ncurrent_RMSE:0.6146653500206918\ncurrent_RMSE:0.6146632099103472\ncurrent_RMSE:0.6146611242867523\ncurrent_RMSE:0.6146590931504619\ncurrent_RMSE:0.6146571165020159\ncurrent_RMSE:0.61465519434194\ncurrent_RMSE:0.6146533266707457\ncurrent_RMSE:0.6146515134889293\ncurrent_RMSE:0.6146497547969735\ncurrent_RMSE:0.6146480505953456\ncurrent_RMSE:0.614646400884499\ncurrent_RMSE:0.6146448056648723\ncurrent_RMSE:0.6146432649368901\ncurrent_RMSE:0.614641778700962\ncurrent_RMSE:0.6146403469574834\ncurrent_RMSE:0.614638969706835\ncurrent_RMSE:0.614637646949383\ncurrent_RMSE:0.6146363786854795\ncurrent_RMSE:0.6146351649154617\ncurrent_RMSE:0.6146340056396523\ncurrent_RMSE:0.6146329008583599\ncurrent_RMSE:0.6146318505718782\ncurrent_RMSE:0.6146308547804866\ncurrent_RMSE:0.61462991348445\ncurrent_RMSE:0.6146290266840188\ncurrent_RMSE:0.6146281943794286\ncurrent_RMSE:0.6146274165709013\ncurrent_RMSE:0.6146266932586434\ncurrent_RMSE:0.6146260244428474\ncurrent_RMSE:0.6146254101236912\ncurrent_RMSE:0.6146248503013383\ncurrent_RMSE:0.6146243449759374\ncurrent_RMSE:0.6146238941476232\ncurrent_RMSE:0.6146234978165155\ncurrent_RMSE:0.6146231559827195\ncurrent_RMSE:0.6146228686463265\ncurrent_RMSE:0.6146226358074128\ncurrent_RMSE:0.6146224574660403\ncurrent_RMSE:0.6146223336222563\ncurrent_RMSE:0.614622264276094\ncurrent_RMSE:0.6146222494275717\ncurrent_RMSE:0.6146221898723574\ncurrent_RMSE:0.6146213396443334\ncurrent_RMSE:0.6146205439129322\ncurrent_RMSE:0.6146198026783656\ncurrent_RMSE:0.6146191159408307\ncurrent_RMSE:0.6146184837005104\ncurrent_RMSE:0.6146179059575726\ncurrent_RMSE:0.6146173827121713\ncurrent_RMSE:0.6146169139644453\ncurrent_RMSE:0.6146164997145195\ncurrent_RMSE:0.6146161399625041\ncurrent_RMSE:0.6146158347084948\ncurrent_RMSE:0.6146155839525727\ncurrent_RMSE:0.6146153876948046\ncurrent_RMSE:0.6146152459352426\ncurrent_RMSE:0.6146151586739247\ncurrent_RMSE:0.6146151259108737\ncurrent_RMSE:0.6146145239941961\ncurrent_RMSE:0.6146137103395727\ncurrent_RMSE:0.6146129511823443\ncurrent_RMSE:0.6146122465227132\ncurrent_RMSE:0.6146115963608664\ncurrent_RMSE:0.6146110006969773\ncurrent_RMSE:0.6146104595312042\ncurrent_RMSE:0.6146099728636909\ncurrent_RMSE:0.6146095406945671\ncurrent_RMSE:0.6146091630239475\ncurrent_RMSE:0.6146088398519328\ncurrent_RMSE:0.614608571178609\ncurrent_RMSE:0.6146083570040474\ncurrent_RMSE:0.614608197328305\ncurrent_RMSE:0.6146080921514242\ncurrent_RMSE:0.6146080414734332\ncurrent_RMSE:0.6146077474303172\ncurrent_RMSE:0.6146069158521261\ncurrent_RMSE:0.6146061387718865\ncurrent_RMSE:0.6146054161898047\ncurrent_RMSE:0.6146047481060731\ncurrent_RMSE:0.6146041345208696\ncurrent_RMSE:0.614603575434357\ncurrent_RMSE:0.6146030708466844\ncurrent_RMSE:0.6146026207579858\ncurrent_RMSE:0.614602225168381\ncurrent_RMSE:0.6146018840779753\ncurrent_RMSE:0.6146015974868595\ncurrent_RMSE:0.6146013653951097\ncurrent_RMSE:0.6146011878027875\ncurrent_RMSE:0.6146010647099405\ncurrent_RMSE:0.6146009961166012\ncurrent_RMSE:0.6146009820227879\ncurrent_RMSE:0.6146001604518893\ncurrent_RMSE:0.6145993654482919\ncurrent_RMSE:0.6145986249434088\ncurrent_RMSE:0.6145979389374371\ncurrent_RMSE:0.6145973074305592\ncurrent_RMSE:0.614596730422943\ncurrent_RMSE:0.6145962079147422\ncurrent_RMSE:0.6145957399060957\ncurrent_RMSE:0.614595326397128\ncurrent_RMSE:0.614594967387949\ncurrent_RMSE:0.6145946628786545\ncurrent_RMSE:0.6145944128693253\ncurrent_RMSE:0.6145942173600277\ncurrent_RMSE:0.6145940763508142\ncurrent_RMSE:0.6145939898417219\ncurrent_RMSE:0.6145939578327739\ncurrent_RMSE:0.6145934441401506\ncurrent_RMSE:0.6145926312128528\ncurrent_RMSE:0.6145918727848212\ncurrent_RMSE:0.6145911688562575\ncurrent_RMSE:0.6145905194273489\ncurrent_RMSE:0.6145899244982683\ncurrent_RMSE:0.6145893840691741\ncurrent_RMSE:0.6145888981402098\ncurrent_RMSE:0.6145884667115047\ncurrent_RMSE:0.6145880897831737\ncurrent_RMSE:0.6145877673553171\ncurrent_RMSE:0.6145874994280205\ncurrent_RMSE:0.6145872860013554\ncurrent_RMSE:0.6145871270753784\ncurrent_RMSE:0.6145870226501319\ncurrent_RMSE:0.6145869727256436\ncurrent_RMSE:0.6145867669181919\ncurrent_RMSE:0.6145859360668543\ncurrent_RMSE:0.61458515971533\ncurrent_RMSE:0.6145844378638259\ncurrent_RMSE:0.6145837705125338\ncurrent_RMSE:0.6145831576616314\ncurrent_RMSE:0.6145825993112817\ncurrent_RMSE:0.6145820954616331\ncurrent_RMSE:0.6145816461128198\ncurrent_RMSE:0.6145812512649614\ncurrent_RMSE:0.6145809109181628\ncurrent_RMSE:0.6145806250725147\ncurrent_RMSE:0.6145803937280928\ncurrent_RMSE:0.614580216884959\ncurrent_RMSE:0.6145800945431602\ncurrent_RMSE:0.6145800267027292\ncurrent_RMSE:0.6145800133636836\ncurrent_RMSE:0.6145792800115736\ncurrent_RMSE:0.6145784857362163\ncurrent_RMSE:0.6145777459614268\ncurrent_RMSE:0.6145770606874016\ncurrent_RMSE:0.6145764299143234\ncurrent_RMSE:0.6145758536423597\ncurrent_RMSE:0.6145753318716639\ncurrent_RMSE:0.6145748646023749\ncurrent_RMSE:0.6145744518346169\ncurrent_RMSE:0.6145740935684998\ncurrent_RMSE:0.6145737898041188\ncurrent_RMSE:0.614573540541555\ncurrent_RMSE:0.6145733457808744\ncurrent_RMSE:0.6145732055221289\ncurrent_RMSE:0.614573119765356\ncurrent_RMSE:0.6145730885105783\ncurrent_RMSE:0.614572663048281\ncurrent_RMSE:0.6145718508487535\ncurrent_RMSE:0.6145710931503369\ncurrent_RMSE:0.6145703899532327\ncurrent_RMSE:0.6145697412576279\ncurrent_RMSE:0.6145691470636951\ncurrent_RMSE:0.6145686073715925\ncurrent_RMSE:0.6145681221814636\ncurrent_RMSE:0.6145676914934376\ncurrent_RMSE:0.6145673153076289\ncurrent_RMSE:0.6145669936241378\ncurrent_RMSE:0.6145667264430495\ncurrent_RMSE:0.6145665137644356\ncurrent_RMSE:0.6145663555883524\ncurrent_RMSE:0.6145662519148419\ncurrent_RMSE:0.6145662027439319\ncurrent_RMSE:0.6145660851782391\ncurrent_RMSE:0.6145652550542078\ncurrent_RMSE:0.6145644794318259\ncurrent_RMSE:0.6145637583112997\ncurrent_RMSE:0.6145630916928214\ncurrent_RMSE:0.6145624795765678\ncurrent_RMSE:0.6145619219627022\ncurrent_RMSE:0.6145614188513728\ncurrent_RMSE:0.6145609702427134\ncurrent_RMSE:0.6145605761368435\ncurrent_RMSE:0.6145602365338678\ncurrent_RMSE:0.6145599514338768\ncurrent_RMSE:0.6145597208369462\ncurrent_RMSE:0.6145595447431375\ncurrent_RMSE:0.6145594231524975\ncurrent_RMSE:0.6145593560650584\ncurrent_RMSE:0.6145593434808382\ncurrent_RMSE:0.6145586983538378\ncurrent_RMSE:0.6145579048071559\ncurrent_RMSE:0.6145571657628687\ncurrent_RMSE:0.6145564812211728\ncurrent_RMSE:0.6145558511822503\ncurrent_RMSE:0.6145552756462688\ncurrent_RMSE:0.6145547546133815\ncurrent_RMSE:0.6145542880837271\ncurrent_RMSE:0.6145538760574295\ncurrent_RMSE:0.6145535185345986\ncurrent_RMSE:0.6145532155153293\ncurrent_RMSE:0.6145529669997023\ncurrent_RMSE:0.6145527729877839\ncurrent_RMSE:0.6145526334796254\ncurrent_RMSE:0.6145525484752641\ncurrent_RMSE:0.6145525179747225\ncurrent_RMSE:0.614552180748895\ncurrent_RMSE:0.6145513692775816\ncurrent_RMSE:0.6145506123091974\ncurrent_RMSE:0.6145499098439435\ncurrent_RMSE:0.6145492618820072\ncurrent_RMSE:0.6145486684235605\ncurrent_RMSE:0.6145481294687615\ncurrent_RMSE:0.6145476450177536\ncurrent_RMSE:0.6145472150706656\ncurrent_RMSE:0.614546839627612\ncurrent_RMSE:0.6145465186886926\ncurrent_RMSE:0.614546252253993\ncurrent_RMSE:0.6145460403235837\ncurrent_RMSE:0.6145458828975214\ncurrent_RMSE:0.614545779975848\ncurrent_RMSE:0.6145457315585907\ncurrent_RMSE:0.614545702240623\ncurrent_RMSE:0.6145448728443502\ncurrent_RMSE:0.6145440979515365\ncurrent_RMSE:0.614543377562388\ncurrent_RMSE:0.6145427116770962\ncurrent_RMSE:0.6145421002958383\ncurrent_RMSE:0.6145415434187771\ncurrent_RMSE:0.6145410410460608\ncurrent_RMSE:0.6145405931778228\ncurrent_RMSE:0.6145401998141827\ncurrent_RMSE:0.6145398609552447\ncurrent_RMSE:0.6145395766010993\ncurrent_RMSE:0.614539346751822\ncurrent_RMSE:0.6145391714074738\ncurrent_RMSE:0.6145390505681018\ncurrent_RMSE:0.6145389842337379\ncurrent_RMSE:0.6145389724043996\ncurrent_RMSE:0.6145384155087018\ncurrent_RMSE:0.6145376226911295\ncurrent_RMSE:0.6145368843777526\ncurrent_RMSE:0.6145362005687675\ncurrent_RMSE:0.6145355712643559\ncurrent_RMSE:0.6145349964646855\ncurrent_RMSE:0.6145344761699091\ncurrent_RMSE:0.6145340103801652\ncurrent_RMSE:0.6145335990955777\ncurrent_RMSE:0.6145332423162562\ncurrent_RMSE:0.6145329400422954\ncurrent_RMSE:0.6145326922737758\ncurrent_RMSE:0.6145324990107635\ncurrent_RMSE:0.6145323602533097\ncurrent_RMSE:0.6145322760014514\ncurrent_RMSE:0.6145322462552111\ncurrent_RMSE:0.6145319972718687\ncurrent_RMSE:0.6145311865292123\ncurrent_RMSE:0.6145304302912769\ncurrent_RMSE:0.6145297285582635\ncurrent_RMSE:0.6145290813303591\ncurrent_RMSE:0.6145284886077358\ncurrent_RMSE:0.6145279503905513\ncurrent_RMSE:0.6145274666789488\ncurrent_RMSE:0.6145270374730569\ncurrent_RMSE:0.6145266627729902\ncurrent_RMSE:0.6145263425788479\ncurrent_RMSE:0.6145260768907155\ncurrent_RMSE:0.6145258657086636\ncurrent_RMSE:0.6145257090327485\ncurrent_RMSE:0.6145256068630118\ncurrent_RMSE:0.6145255591994806\ncurrent_RMSE:0.6145247894670132\ncurrent_RMSE:0.6145240153041925\ncurrent_RMSE:0.61452329564682\ncurrent_RMSE:0.6145226304950869\ncurrent_RMSE:0.6145220198491704\ncurrent_RMSE:0.614521463709233\ncurrent_RMSE:0.6145209620754226\ncurrent_RMSE:0.6145205149478727\ncurrent_RMSE:0.6145201223267023\ncurrent_RMSE:0.6145197842120159\ncurrent_RMSE:0.6145195006039035\ncurrent_RMSE:0.6145192715024403\ncurrent_RMSE:0.6145190969076876\ncurrent_RMSE:0.6145189768196917\ncurrent_RMSE:0.6145189112384846\ncurrent_RMSE:0.6145189001640836\ncurrent_RMSE:0.6145184315057535\ncurrent_RMSE:0.6145176394177242\ncurrent_RMSE:0.6145169018356644\ncurrent_RMSE:0.6145162187597705\ncurrent_RMSE:0.6145155901902241\ncurrent_RMSE:0.6145150161271924\ncurrent_RMSE:0.6145144965708284\ncurrent_RMSE:0.61451403152127\ncurrent_RMSE:0.6145136209786413\ncurrent_RMSE:0.6145132649430513\ncurrent_RMSE:0.6145129634145948\ncurrent_RMSE:0.6145127163933521\ncurrent_RMSE:0.614512523879389\ncurrent_RMSE:0.6145123858727565\ncurrent_RMSE:0.6145123023734915\ncurrent_RMSE:0.6145122733816162\ncurrent_RMSE:0.6145121126466465\ncurrent_RMSE:0.6145113026330888\ncurrent_RMSE:0.6145105471260176\ncurrent_RMSE:0.6145098461256338\ncurrent_RMSE:0.614509199632124\ncurrent_RMSE:0.6145086076456602\ncurrent_RMSE:0.6145080701664\ncurrent_RMSE:0.6145075871944864\ncurrent_RMSE:0.6145071587300478\ncurrent_RMSE:0.6145067847731983\ncurrent_RMSE:0.6145064653240375\ncurrent_RMSE:0.6145062003826504\ncurrent_RMSE:0.6145059899491073\ncurrent_RMSE:0.6145058340234644\ncurrent_RMSE:0.614505732605763\ncurrent_RMSE:0.6145056856960304\ncurrent_RMSE:0.6145050049514962\ncurrent_RMSE:0.6145042315190924\ncurrent_RMSE:0.6145035125938932\ncurrent_RMSE:0.6145028481760901\ncurrent_RMSE:0.6145022382658596\ncurrent_RMSE:0.6145016828633643\ncurrent_RMSE:0.6145011819687518\ncurrent_RMSE:0.6145007355821553\ncurrent_RMSE:0.614500343703694\ncurrent_RMSE:0.6145000063334718\ncurrent_RMSE:0.6144997234715786\ncurrent_RMSE:0.6144994951180895\ncurrent_RMSE:0.6144993212730657\ncurrent_RMSE:0.6144992019365532\ncurrent_RMSE:0.6144991371085835\ncurrent_RMSE:0.6144991267891744\ncurrent_RMSE:0.6144987463741485\ncurrent_RMSE:0.6144979550160946\ncurrent_RMSE:0.614497218165758\ncurrent_RMSE:0.6144965358233349\ncurrent_RMSE:0.6144959079890067\ncurrent_RMSE:0.6144953346629405\ncurrent_RMSE:0.6144948158452891\ncurrent_RMSE:0.6144943515361904\ncurrent_RMSE:0.6144939417357678\ncurrent_RMSE:0.6144935864441305\ncurrent_RMSE:0.6144932856613733\ncurrent_RMSE:0.6144930393875758\ncurrent_RMSE:0.6144928476228038\ncurrent_RMSE:0.6144927103671083\ncurrent_RMSE:0.6144926276205257\ncurrent_RMSE:0.6144925993830782\ncurrent_RMSE:0.6144925269022404\ncurrent_RMSE:0.6144917176182223\ncurrent_RMSE:0.6144909628424297\ncurrent_RMSE:0.6144902625750633\ncurrent_RMSE:0.6144896168163098\ncurrent_RMSE:0.6144890255663408\ncurrent_RMSE:0.6144884888253137\ncurrent_RMSE:0.6144880065933712\ncurrent_RMSE:0.6144875788706419\ncurrent_RMSE:0.6144872056572395\ncurrent_RMSE:0.6144868869532633\ncurrent_RMSE:0.6144866227587982\ncurrent_RMSE:0.6144864130739143\ncurrent_RMSE:0.6144862578986676\ncurrent_RMSE:0.6144861572330994\ncurrent_RMSE:0.6144861110772364\ncurrent_RMSE:0.6144855193266664\ncurrent_RMSE:0.6144847466251024\ncurrent_RMSE:0.6144840284324732\ncurrent_RMSE:0.6144833647489699\ncurrent_RMSE:0.6144827555747693\ncurrent_RMSE:0.6144822009100331\ncurrent_RMSE:0.6144817007549094\ncurrent_RMSE:0.614481255109531\ncurrent_RMSE:0.6144808639740167\ncurrent_RMSE:0.6144805273484703\ncurrent_RMSE:0.6144802452329817\ncurrent_RMSE:0.6144800176276257\ncurrent_RMSE:0.6144798445324631\ncurrent_RMSE:0.6144797259475399\ncurrent_RMSE:0.6144796618728876\ncurrent_RMSE:0.6144796523085232\ncurrent_RMSE:0.6144793601426103\ncurrent_RMSE:0.6144785695149632\ncurrent_RMSE:0.6144778333967547\ncurrent_RMSE:0.6144771517881809\ncurrent_RMSE:0.6144765246894232\ncurrent_RMSE:0.6144759521006483\ncurrent_RMSE:0.6144754340220088\ncurrent_RMSE:0.6144749704536424\ncurrent_RMSE:0.6144745613956727\ncurrent_RMSE:0.6144742068482084\ncurrent_RMSE:0.6144739068113438\ncurrent_RMSE:0.6144736612851589\ncurrent_RMSE:0.6144734702697189\ncurrent_RMSE:0.6144733337650747\ncurrent_RMSE:0.6144732517712628\ncurrent_RMSE:0.6144732242883048\ncurrent_RMSE:0.6144724315131912\ncurrent_RMSE:0.6144716774690906\ncurrent_RMSE:0.6144709779351287\ncurrent_RMSE:0.6144703329114921\ncurrent_RMSE:0.614469742398352\ncurrent_RMSE:0.6144692063958658\ncurrent_RMSE:0.614468724904176\ncurrent_RMSE:0.614468297923411\ncurrent_RMSE:0.6144679254536841\ncurrent_RMSE:0.6144676074950948\ncurrent_RMSE:0.6144673440477274\ncurrent_RMSE:0.6144671351116522\ncurrent_RMSE:0.6144669806869246\ncurrent_RMSE:0.6144668807735859\ncurrent_RMSE:0.6144668353716627\ncurrent_RMSE:0.6144663326209587\ncurrent_RMSE:0.6144655606506563\ncurrent_RMSE:0.6144648431909926\ncurrent_RMSE:0.6144641802421583\ncurrent_RMSE:0.61446357180433\ncurrent_RMSE:0.6144630178776694\ncurrent_RMSE:0.6144625184623244\ncurrent_RMSE:0.6144620735584274\ncurrent_RMSE:0.6144616831660971\ncurrent_RMSE:0.6144613472854372\ncurrent_RMSE:0.6144610659165373\ncurrent_RMSE:0.6144608390594721\ncurrent_RMSE:0.6144606667143022\ncurrent_RMSE:0.6144605488810733\ncurrent_RMSE:0.6144604855598168\ncurrent_RMSE:0.6144604767505495\ncurrent_RMSE:0.6144602728394299\ncurrent_RMSE:0.6144594829426199\ncurrent_RMSE:0.6144587475569435\ncurrent_RMSE:0.6144580666825965\ncurrent_RMSE:0.6144574403197604\ncurrent_RMSE:0.6144568684686015\ncurrent_RMSE:0.6144563511292722\ncurrent_RMSE:0.6144558883019103\ncurrent_RMSE:0.6144554799866387\ncurrent_RMSE:0.6144551261835663\ncurrent_RMSE:0.6144548268927873\ncurrent_RMSE:0.6144545821143811\ncurrent_RMSE:0.6144543918484131\ncurrent_RMSE:0.6144542560949338\ncurrent_RMSE:0.6144541748539794\ncurrent_RMSE:0.6144541481255713\ncurrent_RMSE:0.6144534443461417\ncurrent_RMSE:0.6144526910341456\ncurrent_RMSE:0.6144519922339743\ncurrent_RMSE:0.6144513479458139\ncurrent_RMSE:0.6144507581698361\ncurrent_RMSE:0.6144502229061974\ncurrent_RMSE:0.6144497421550408\ncurrent_RMSE:0.6144493159164939\ncurrent_RMSE:0.6144489441906702\ncurrent_RMSE:0.6144486269776687\ncurrent_RMSE:0.6144483642775738\ncurrent_RMSE:0.6144481560904553\ncurrent_RMSE:0.6144480024163689\ncurrent_RMSE:0.6144479032553553\ncurrent_RMSE:0.6144478586074408\ncurrent_RMSE:0.614447444862375\ncurrent_RMSE:0.6144466736237553\ncurrent_RMSE:0.6144459568974514\ncurrent_RMSE:0.6144452946836543\ncurrent_RMSE:0.6144446869825398\ncurrent_RMSE:0.6144441337942702\ncurrent_RMSE:0.6144436351189925\ncurrent_RMSE:0.6144431909568394\ncurrent_RMSE:0.614442801307929\ncurrent_RMSE:0.6144424661723653\ncurrent_RMSE:0.6144421855502372\ncurrent_RMSE:0.6144419594416197\ncurrent_RMSE:0.6144417878465727\ncurrent_RMSE:0.614441670765142\ncurrent_RMSE:0.6144416081973587\ncurrent_RMSE:0.6144416001432396\ncurrent_RMSE:0.6144414844924655\ncurrent_RMSE:0.6144406953269218\ncurrent_RMSE:0.6144399606741805\ncurrent_RMSE:0.614439280534437\ncurrent_RMSE:0.6144386549078724\ncurrent_RMSE:0.6144380837946534\ncurrent_RMSE:0.6144375671949316\ncurrent_RMSE:0.6144371051088449\ncurrent_RMSE:0.614436697536516\ncurrent_RMSE:0.6144363444780535\ncurrent_RMSE:0.6144360459335515\ncurrent_RMSE:0.6144358019030893\ncurrent_RMSE:0.6144356123867318\ncurrent_RMSE:0.6144354773845297\ncurrent_RMSE:0.6144353968965187\ncurrent_RMSE:0.6144353709227204\ncurrent_RMSE:0.6144347561447869\ncurrent_RMSE:0.6144340035653066\ncurrent_RMSE:0.6144333054993109\ncurrent_RMSE:0.6144326619469856\ncurrent_RMSE:0.614432072908502\ncurrent_RMSE:0.6144315383840169\ncurrent_RMSE:0.6144310583736725\ncurrent_RMSE:0.6144306328775966\ncurrent_RMSE:0.6144302618959026\ncurrent_RMSE:0.614429945428689\ncurrent_RMSE:0.6144296834760401\ncurrent_RMSE:0.6144294760380258\ncurrent_RMSE:0.6144293231147011\ncurrent_RMSE:0.6144292247061068\ncurrent_RMSE:0.6144291808122692\ncurrent_RMSE:0.6144288560784847\ncurrent_RMSE:0.6144280855719675\ncurrent_RMSE:0.614427369579417\ncurrent_RMSE:0.6144267081010238\ncurrent_RMSE:0.6144261011369643\ncurrent_RMSE:0.6144255486873997\ncurrent_RMSE:0.6144250507524771\ncurrent_RMSE:0.6144246073323292\ncurrent_RMSE:0.6144242184270738\ncurrent_RMSE:0.6144238840368146\ncurrent_RMSE:0.6144236041616405\ncurrent_RMSE:0.6144233788016261\ncurrent_RMSE:0.6144232079568314\ncurrent_RMSE:0.6144230916273017\ncurrent_RMSE:0.6144230298130681\ncurrent_RMSE:0.6144230225141469\ncurrent_RMSE:0.6144229951291422\ncurrent_RMSE:0.6144222066952933\ncurrent_RMSE:0.614421472775889\ncurrent_RMSE:0.6144207933711244\ncurrent_RMSE:0.6144201684811806\ncurrent_RMSE:0.6144195981062239\ncurrent_RMSE:0.614419082246406\ncurrent_RMSE:0.6144186209018642\ncurrent_RMSE:0.6144182140727215\ncurrent_RMSE:0.614417861759086\ncurrent_RMSE:0.6144175639610515\ncurrent_RMSE:0.6144173206786974\ncurrent_RMSE:0.6144171319120882\ncurrent_RMSE:0.6144169976612746\ncurrent_RMSE:0.6144169179262918\ncurrent_RMSE:0.6144168927071614\ncurrent_RMSE:0.614416366936407\ncurrent_RMSE:0.614415615089853\ncurrent_RMSE:0.614414917758417\ncurrent_RMSE:0.6144142749422842\ncurrent_RMSE:0.614413686641626\ncurrent_RMSE:0.614413152856599\ncurrent_RMSE:0.6144126735873453\ncurrent_RMSE:0.6144122488339924\ncurrent_RMSE:0.6144118785966532\ncurrent_RMSE:0.6144115628754266\ncurrent_RMSE:0.6144113016703965\ncurrent_RMSE:0.6144110949816323\ncurrent_RMSE:0.6144109428091892\ncurrent_RMSE:0.6144108451531075\ncurrent_RMSE:0.6144108020134134\ncurrent_RMSE:0.614410566296424\ncurrent_RMSE:0.6144097965224281\ncurrent_RMSE:0.6144090812640235\ncurrent_RMSE:0.6144084205214005\ncurrent_RMSE:0.6144078142947352\ncurrent_RMSE:0.6144072625841889\ncurrent_RMSE:0.6144067653899081\ncurrent_RMSE:0.6144063227120257\ncurrent_RMSE:0.6144059345506592\ncurrent_RMSE:0.614405600905912\ncurrent_RMSE:0.614405321777873\ncurrent_RMSE:0.6144050971666163\ncurrent_RMSE:0.614404927072202\ncurrent_RMSE:0.614404811494675\ncurrent_RMSE:0.6144047504340663\ncurrent_RMSE:0.6144047438903922\ncurrent_RMSE:0.6144040170747255\ncurrent_RMSE:0.6144032838890591\ncurrent_RMSE:0.614402605219648\ncurrent_RMSE:0.614401981066673\ncurrent_RMSE:0.6144014114303002\ncurrent_RMSE:0.6144008963106813\ncurrent_RMSE:0.6144004357079534\ncurrent_RMSE:0.614400029622239\ncurrent_RMSE:0.6143996780536465\ncurrent_RMSE:0.614399381002269\ncurrent_RMSE:0.614399138468186\ncurrent_RMSE:0.6143989504514619\ncurrent_RMSE:0.6143988169521467\ncurrent_RMSE:0.6143987379702759\ncurrent_RMSE:0.6143987135058707\ncurrent_RMSE:0.6143982767478491\ncurrent_RMSE:0.6143975256346309\ncurrent_RMSE:0.6143968290381372\ncurrent_RMSE:0.6143961869585537\ncurrent_RMSE:0.6143955993960512\ncurrent_RMSE:0.6143950663507859\ncurrent_RMSE:0.6143945878229\ncurrent_RMSE:0.6143941638125208\ncurrent_RMSE:0.6143937943197612\ncurrent_RMSE:0.6143934793447193\ncurrent_RMSE:0.6143932188874793\ncurrent_RMSE:0.6143930129481104\ncurrent_RMSE:0.6143928615266674\ncurrent_RMSE:0.6143927646231906\ncurrent_RMSE:0.6143927222377058\ncurrent_RMSE:0.6143925755428956\ncurrent_RMSE:0.6143918065018389\ncurrent_RMSE:0.6143910919779716\ncurrent_RMSE:0.6143904319714839\ncurrent_RMSE:0.6143898264825515\ncurrent_RMSE:0.6143892755113354\ncurrent_RMSE:0.6143887790579824\ncurrent_RMSE:0.6143883371226248\ncurrent_RMSE:0.6143879497053799\ncurrent_RMSE:0.6143876168063512\ncurrent_RMSE:0.6143873384256271\ncurrent_RMSE:0.6143871145632818\ncurrent_RMSE:0.6143869452193748\ncurrent_RMSE:0.6143868303939513\ncurrent_RMSE:0.6143867700870417\ncurrent_RMSE:0.6143867642986621\ncurrent_RMSE:0.6143861264917758\ncurrent_RMSE:0.6143853940402473\ncurrent_RMSE:0.6143847161065633\ncurrent_RMSE:0.6143840926909041\ncurrent_RMSE:0.6143835237934361\ncurrent_RMSE:0.6143830094143103\ncurrent_RMSE:0.614382549553664\ncurrent_RMSE:0.6143821442116193\ncurrent_RMSE:0.6143817933882845\ncurrent_RMSE:0.6143814970837524\ncurrent_RMSE:0.6143812552981025\ncurrent_RMSE:0.6143810680313988\ncurrent_RMSE:0.6143809352836913\ncurrent_RMSE:0.6143808570550152\ncurrent_RMSE:0.6143808333453915\ncurrent_RMSE:0.6143804856055266\ncurrent_RMSE:0.6143797352260525\ncurrent_RMSE:0.6143790393648834\ncurrent_RMSE:0.6143783980222045\ncurrent_RMSE:0.6143778111981866\ncurrent_RMSE:0.6143772788929859\ncurrent_RMSE:0.6143768011067441\ncurrent_RMSE:0.6143763778395883\ncurrent_RMSE:0.6143760090916315\ncurrent_RMSE:0.6143756948629714\ncurrent_RMSE:0.614375435153692\ncurrent_RMSE:0.6143752299638623\ncurrent_RMSE:0.614375079293537\ncurrent_RMSE:0.6143749831427561\ncurrent_RMSE:0.6143749415115453\ncurrent_RMSE:0.6143748838441688\ncurrent_RMSE:0.6143741155364685\ncurrent_RMSE:0.614373401747529\ncurrent_RMSE:0.6143727424775405\ncurrent_RMSE:0.6143721377266784\ncurrent_RMSE:0.6143715874951039\ncurrent_RMSE:0.6143710917829632\ncurrent_RMSE:0.6143706505903885\ncurrent_RMSE:0.6143702639174972\ncurrent_RMSE:0.6143699317643923\ncurrent_RMSE:0.614369654131162\ncurrent_RMSE:0.6143694310178803\ncurrent_RMSE:0.6143692624246069\ncurrent_RMSE:0.6143691483513862\ncurrent_RMSE:0.6143690887982488\ncurrent_RMSE:0.6143690837652107\ncurrent_RMSE:0.6143685349725687\ncurrent_RMSE:0.6143678032555769\ncurrent_RMSE:0.6143671260579924\ncurrent_RMSE:0.6143665033799953\ncurrent_RMSE:0.6143659352217514\ncurrent_RMSE:0.6143654215834121\ncurrent_RMSE:0.614364962465114\ncurrent_RMSE:0.6143645578669793\ncurrent_RMSE:0.6143642077891158\ncurrent_RMSE:0.6143639122316168\ncurrent_RMSE:0.6143636711945608\ncurrent_RMSE:0.614363484678012\ncurrent_RMSE:0.6143633526820202\ncurrent_RMSE:0.6143632752066204\ncurrent_RMSE:0.6143632522518332\ncurrent_RMSE:0.6143629935354195\ncurrent_RMSE:0.614362243890097\ncurrent_RMSE:0.6143615487646333\ncurrent_RMSE:0.6143609081592137\ncurrent_RMSE:0.6143603220740086\ncurrent_RMSE:0.614359790509174\ncurrent_RMSE:0.6143593134648514\ncurrent_RMSE:0.6143588909411679\ncurrent_RMSE:0.6143585229382359\ncurrent_RMSE:0.6143582094561535\ncurrent_RMSE:0.614357950495004\ncurrent_RMSE:0.6143577460548565\ncurrent_RMSE:0.6143575961357652\ncurrent_RMSE:0.6143575007377704\ncurrent_RMSE:0.6143574598608971\ncurrent_RMSE:0.6143567236521512\ncurrent_RMSE:0.6143560105985291\ncurrent_RMSE:0.6143553520654027\ncurrent_RMSE:0.6143547480529477\ncurrent_RMSE:0.6143541985613247\ncurrent_RMSE:0.6143537035906801\ncurrent_RMSE:0.6143532631411456\ncurrent_RMSE:0.6143528772128385\ncurrent_RMSE:0.6143525458058615\ncurrent_RMSE:0.6143522689203029\ncurrent_RMSE:0.6143520465562364\ncurrent_RMSE:0.6143518787137213\ncurrent_RMSE:0.614351765392802\ncurrent_RMSE:0.614351706593509\ncurrent_RMSE:0.6143517023158578\ncurrent_RMSE:0.6143512425427945\ncurrent_RMSE:0.6143505115607375\ncurrent_RMSE:0.6143498350996239\ncurrent_RMSE:0.6143492131596338\ncurrent_RMSE:0.6143486457409327\ncurrent_RMSE:0.6143481328436718\ncurrent_RMSE:0.6143476744679877\ncurrent_RMSE:0.6143472706140022\ncurrent_RMSE:0.614346921281823\ncurrent_RMSE:0.614346626471543\ncurrent_RMSE:0.6143463861832409\ncurrent_RMSE:0.6143462004169804\ncurrent_RMSE:0.6143460691728112\ncurrent_RMSE:0.614345992450768\ncurrent_RMSE:0.6143459702508715\ncurrent_RMSE:0.614345800563074\ncurrent_RMSE:0.6143450516523095\ncurrent_RMSE:0.6143443572629312\ncurrent_RMSE:0.6143437173951244\ncurrent_RMSE:0.6143431320490591\ncurrent_RMSE:0.6143426012248911\ncurrent_RMSE:0.614342124922762\ncurrent_RMSE:0.6143417031427983\ncurrent_RMSE:0.6143413358851125\ncurrent_RMSE:0.6143410231498024\ncurrent_RMSE:0.6143407649369512\ncurrent_RMSE:0.6143405612466277\ncurrent_RMSE:0.6143404120788861\ncurrent_RMSE:0.6143403174337659\ncurrent_RMSE:0.6143402773112927\ncurrent_RMSE:0.6143396308742882\ncurrent_RMSE:0.6143389185563717\ncurrent_RMSE:0.6143382607604696\ncurrent_RMSE:0.6143376574867571\ncurrent_RMSE:0.6143371087353948\ncurrent_RMSE:0.6143366145065288\ncurrent_RMSE:0.6143361748002906\ncurrent_RMSE:0.6143357896167975\ncurrent_RMSE:0.6143354589561517\ncurrent_RMSE:0.6143351828184416\ncurrent_RMSE:0.6143349612037405\ncurrent_RMSE:0.6143347941121075\ncurrent_RMSE:0.6143346815435872\ncurrent_RMSE:0.6143346234982092\ncurrent_RMSE:0.6143346199759893\ncurrent_RMSE:0.6143342492277096\ncurrent_RMSE:0.6143335189809843\ncurrent_RMSE:0.6143328432567122\ncurrent_RMSE:0.6143322220550731\ncurrent_RMSE:0.6143316553762325\ncurrent_RMSE:0.6143311432203411\ncurrent_RMSE:0.6143306855875355\ncurrent_RMSE:0.6143302824779373\ncurrent_RMSE:0.614329933891654\ncurrent_RMSE:0.6143296398287784\ncurrent_RMSE:0.6143294002893889\ncurrent_RMSE:0.614329215273549\ncurrent_RMSE:0.6143290847813082\ncurrent_RMSE:0.6143290088127011\ncurrent_RMSE:0.6143289873677481\ncurrent_RMSE:0.6143289067136022\ncurrent_RMSE:0.6143281585378011\ncurrent_RMSE:0.6143274648848873\ncurrent_RMSE:0.6143268257550456\ncurrent_RMSE:0.614326241148446\ncurrent_RMSE:0.6143257110652444\ncurrent_RMSE:0.6143252355055817\ncurrent_RMSE:0.6143248144695846\ncurrent_RMSE:0.6143244479573652\ncurrent_RMSE:0.6143241359690211\ncurrent_RMSE:0.6143238785046354\ncurrent_RMSE:0.6143236755642767\ncurrent_RMSE:0.6143235271479989\ncurrent_RMSE:0.6143234332558415\ncurrent_RMSE:0.6143233938878295\ncurrent_RMSE:0.6143228372278462\ncurrent_RMSE:0.6143221256460228\ncurrent_RMSE:0.6143214685877059\ncurrent_RMSE:0.6143208660530705\ncurrent_RMSE:0.6143203180422767\ncurrent_RMSE:0.6143198245554709\ncurrent_RMSE:0.6143193855927842\ncurrent_RMSE:0.6143190011543335\ncurrent_RMSE:0.6143186712402213\ncurrent_RMSE:0.6143183958505353\ncurrent_RMSE:0.6143181749853488\ncurrent_RMSE:0.6143180086447209\ncurrent_RMSE:0.6143178968286954\ncurrent_RMSE:0.6143178395373026\ncurrent_RMSE:0.6143178367705574\ncurrent_RMSE:0.6143175550521367\ncurrent_RMSE:0.6143168255411391\ncurrent_RMSE:0.6143161505540778\ncurrent_RMSE:0.6143155300911326\ncurrent_RMSE:0.6143149641524688\ncurrent_RMSE:0.614314452738237\ncurrent_RMSE:0.6143139958485735\ncurrent_RMSE:0.6143135934835998\ncurrent_RMSE:0.6143132456434232\ncurrent_RMSE:0.6143129523281361\ncurrent_RMSE:0.6143127135378168\ncurrent_RMSE:0.6143125292725289\ncurrent_RMSE:0.6143123995323212\ncurrent_RMSE:0.6143123243172285\ncurrent_RMSE:0.6143123036272707\ncurrent_RMSE:0.6143115645712488\ncurrent_RMSE:0.6143108716551775\ncurrent_RMSE:0.6143102332636524\ncurrent_RMSE:0.6143096493968437\ncurrent_RMSE:0.6143091200549067\ncurrent_RMSE:0.6143086452379826\ncurrent_RMSE:0.6143082249461976\ncurrent_RMSE:0.6143078591796637\ncurrent_RMSE:0.6143075479384783\ncurrent_RMSE:0.6143072912227242\ncurrent_RMSE:0.6143070890324699\ncurrent_RMSE:0.6143069413677691\ncurrent_RMSE:0.6143068482286612\ncurrent_RMSE:0.6143068096151709\ncurrent_RMSE:0.6143063427373578\ncurrent_RMSE:0.6143056318920139\ncurrent_RMSE:0.6143049755716422\ncurrent_RMSE:0.6143043737764171\ncurrent_RMSE:0.6143038265064992\ncurrent_RMSE:0.6143033337620339\ncurrent_RMSE:0.6143028955431528\ncurrent_RMSE:0.6143025118499722\ncurrent_RMSE:0.6143021826825944\ncurrent_RMSE:0.6143019080411073\ncurrent_RMSE:0.6143016879255836\ncurrent_RMSE:0.6143015223360823\ncurrent_RMSE:0.6143014112726473\ncurrent_RMSE:0.614301354735308\ncurrent_RMSE:0.6143013527240798\ncurrent_RMSE:0.6143011600404638\ncurrent_RMSE:0.6143004312655886\ncurrent_RMSE:0.6142997570161066\ncurrent_RMSE:0.6142991372921974\ncurrent_RMSE:0.614298572094026\ncurrent_RMSE:0.6142980614217428\ncurrent_RMSE:0.6142976052754839\ncurrent_RMSE:0.6142972036553707\ncurrent_RMSE:0.6142968565615102\ncurrent_RMSE:0.6142965639939948\ncurrent_RMSE:0.6142963259529024\ncurrent_RMSE:0.6142961424382963\ncurrent_RMSE:0.6142960134502256\ncurrent_RMSE:0.6142959389887245\ncurrent_RMSE:0.6142959190538128\ncurrent_RMSE:0.6142952697768954\ncurrent_RMSE:0.6142945775980434\ncurrent_RMSE:0.6142939399451854\ncurrent_RMSE:0.6142933568184914\ncurrent_RMSE:0.6142928282181168\ncurrent_RMSE:0.6142923541442021\ncurrent_RMSE:0.6142919345968737\ncurrent_RMSE:0.6142915695762433\ncurrent_RMSE:0.6142912590824081\ncurrent_RMSE:0.6142910031154509\ncurrent_RMSE:0.6142908016754396\ncurrent_RMSE:0.614290654762428\ncurrent_RMSE:0.6142905623764552\ncurrent_RMSE:0.6142905245175458\ncurrent_RMSE:0.6142901474269211\ncurrent_RMSE:0.6142894373184422\ncurrent_RMSE:0.6142887817363745\ncurrent_RMSE:0.6142881806808924\ncurrent_RMSE:0.614287634152156\ncurrent_RMSE:0.614287142150311\ncurrent_RMSE:0.6142867046754883\ncurrent_RMSE:0.6142863217278044\ncurrent_RMSE:0.6142859933073612\ncurrent_RMSE:0.6142857194142464\ncurrent_RMSE:0.6142855000485327\ncurrent_RMSE:0.6142853352102785\ncurrent_RMSE:0.6142852248995279\ncurrent_RMSE:0.61428516911631\ncurrent_RMSE:0.61428516786064\ncurrent_RMSE:0.6142850642166446\ncurrent_RMSE:0.6142843361782857\ncurrent_RMSE:0.6142836626667505\ncurrent_RMSE:0.6142830436822182\ncurrent_RMSE:0.6142824792248535\ncurrent_RMSE:0.6142819692948068\ncurrent_RMSE:0.6142815138922141\ncurrent_RMSE:0.6142811130171963\ncurrent_RMSE:0.6142807666698605\ncurrent_RMSE:0.6142804748502987\ncurrent_RMSE:0.6142802375585888\ncurrent_RMSE:0.6142800547947939\ncurrent_RMSE:0.6142799265589626\ncurrent_RMSE:0.6142798528511291\ncurrent_RMSE:0.614279833671313\ncurrent_RMSE:0.6142792741785491\ncurrent_RMSE:0.6142785827372921\ncurrent_RMSE:0.6142779458234506\ncurrent_RMSE:0.6142773634371944\ncurrent_RMSE:0.6142768355786784\ncurrent_RMSE:0.6142763622480432\ncurrent_RMSE:0.6142759434454149\ncurrent_RMSE:0.614275579170905\ncurrent_RMSE:0.6142752694246106\ncurrent_RMSE:0.614275014206614\ncurrent_RMSE:0.6142748135169834\ncurrent_RMSE:0.6142746673557721\ncurrent_RMSE:0.6142745757230189\ncurrent_RMSE:0.6142745386187485\ncurrent_RMSE:0.6142742513201997\ncurrent_RMSE:0.6142735419489702\ncurrent_RMSE:0.6142728871055644\ncurrent_RMSE:0.6142722867901567\ncurrent_RMSE:0.6142717410029068\ncurrent_RMSE:0.6142712497439604\ncurrent_RMSE:0.6142708130134481\ncurrent_RMSE:0.6142704308114864\ncurrent_RMSE:0.6142701031381768\ncurrent_RMSE:0.6142698299936067\ncurrent_RMSE:0.6142696113778489\ncurrent_RMSE:0.6142694472909614\ncurrent_RMSE:0.6142693377329883\ncurrent_RMSE:0.6142692827039584\ncurrent_RMSE:0.6142692822038865\ncurrent_RMSE:0.6142692676041981\ncurrent_RMSE:0.6142685403027485\ncurrent_RMSE:0.6142678675295263\ncurrent_RMSE:0.6142672492847108\ncurrent_RMSE:0.6142666855684662\ncurrent_RMSE:0.614266176380943\ncurrent_RMSE:0.6142657217222768\ncurrent_RMSE:0.6142653215925886\ncurrent_RMSE:0.6142649759919848\ncurrent_RMSE:0.6142646849205576\ncurrent_RMSE:0.6142644483783847\ncurrent_RMSE:0.6142642663655288\ncurrent_RMSE:0.6142641388820385\ncurrent_RMSE:0.6142640659279476\ncurrent_RMSE:0.6142640475032758\ncurrent_RMSE:0.6142635777995832\ncurrent_RMSE:0.6142628870962962\ncurrent_RMSE:0.6142622509218195\ncurrent_RMSE:0.6142616692763229\ncurrent_RMSE:0.614261142159961\ncurrent_RMSE:0.6142606695728742\ncurrent_RMSE:0.6142602515151884\ncurrent_RMSE:0.6142598879870149\ncurrent_RMSE:0.6142595789884506\ncurrent_RMSE:0.6142593245195779\ncurrent_RMSE:0.6142591245804643\ncurrent_RMSE:0.6142589791711631\ncurrent_RMSE:0.6142588882917132\ncurrent_RMSE:0.6142588519421386\ncurrent_RMSE:0.6142586544404227\ncurrent_RMSE:0.6142579458068258\ncurrent_RMSE:0.6142572917024387\ncurrent_RMSE:0.6142566921274357\ncurrent_RMSE:0.6142561470819764\ncurrent_RMSE:0.614255656566206\ncurrent_RMSE:0.6142552205802551\ncurrent_RMSE:0.6142548391242397\ncurrent_RMSE:0.6142545121982617\ncurrent_RMSE:0.6142542398024078\ncurrent_RMSE:0.6142540219367508\ncurrent_RMSE:0.6142538586013486\ncurrent_RMSE:0.6142537497962448\ncurrent_RMSE:0.6142536955214684\ncurrent_RMSE:0.6142530436620601\ncurrent_RMSE:0.6142523716275163\ncurrent_RMSE:0.6142517541227563\ncurrent_RMSE:0.6142511911479444\ncurrent_RMSE:0.6142506827032306\ncurrent_RMSE:0.6142502287887502\ncurrent_RMSE:0.6142498294046244\ncurrent_RMSE:0.6142494845509591\ncurrent_RMSE:0.6142491942278465\ncurrent_RMSE:0.6142489584353639\ncurrent_RMSE:0.6142487771735738\ncurrent_RMSE:0.6142486504425249\ncurrent_RMSE:0.6142485782422507\ncurrent_RMSE:0.6142485605727704\ncurrent_RMSE:0.6142481806629362\ncurrent_RMSE:0.614247490697993\ncurrent_RMSE:0.6142468552632286\ncurrent_RMSE:0.6142462743588124\ncurrent_RMSE:0.614245747984899\ncurrent_RMSE:0.6142452761416284\ncurrent_RMSE:0.6142448588291265\ncurrent_RMSE:0.6142444960475044\ncurrent_RMSE:0.6142441877968586\ncurrent_RMSE:0.6142439340772715\ncurrent_RMSE:0.6142437348888102\ncurrent_RMSE:0.6142435902315283\ncurrent_RMSE:0.6142435001054638\ncurrent_RMSE:0.6142434645106412\ncurrent_RMSE:0.6142433568103838\ncurrent_RMSE:0.6142426489148018\ncurrent_RMSE:0.6142419955497894\ncurrent_RMSE:0.6142413967155205\ncurrent_RMSE:0.6142408524121544\ncurrent_RMSE:0.6142403626398363\ncurrent_RMSE:0.6142399273986966\ncurrent_RMSE:0.6142395466888512\ncurrent_RMSE:0.6142392205104014\ncurrent_RMSE:0.6142389488634341\ncurrent_RMSE:0.6142387317480218\ncurrent_RMSE:0.6142385691642223\ncurrent_RMSE:0.6142384611120787\ncurrent_RMSE:0.61423840759162\ncurrent_RMSE:0.6142378462788687\ncurrent_RMSE:0.6142371749833677\ncurrent_RMSE:0.6142365582190011\ncurrent_RMSE:0.614235995985933\ncurrent_RMSE:0.6142354882843135\ncurrent_RMSE:0.6142350351142774\ncurrent_RMSE:0.6142346364759456\ncurrent_RMSE:0.6142342923694243\ncurrent_RMSE:0.6142340027948051\ncurrent_RMSE:0.614233767752165\ncurrent_RMSE:0.6142335872415667\ncurrent_RMSE:0.6142334612630586\ncurrent_RMSE:0.6142333898166737\ncurrent_RMSE:0.6142333729024313\ncurrent_RMSE:0.6142330827911118\ncurrent_RMSE:0.6142323935648852\ncurrent_RMSE:0.6142317588701794\ncurrent_RMSE:0.6142311787071634\ncurrent_RMSE:0.6142306530759918\ncurrent_RMSE:0.6142301819768042\ncurrent_RMSE:0.6142297654097266\ncurrent_RMSE:0.6142294033748696\ncurrent_RMSE:0.6142290958723297\ncurrent_RMSE:0.6142288429021889\ncurrent_RMSE:0.6142286444645144\ncurrent_RMSE:0.6142285005593594\ncurrent_RMSE:0.6142284111867617\ncurrent_RMSE:0.6142283763467455\ncurrent_RMSE:0.6142283584524418\ncurrent_RMSE:0.6142276512952561\ncurrent_RMSE:0.6142269986699732\ncurrent_RMSE:0.6142264005767667\ncurrent_RMSE:0.6142258570157957\ncurrent_RMSE:0.6142253679872052\ncurrent_RMSE:0.6142249334911254\ncurrent_RMSE:0.6142245535276721\ncurrent_RMSE:0.6142242280969465\ncurrent_RMSE:0.6142239571990351\ncurrent_RMSE:0.6142237408340103\ncurrent_RMSE:0.6142235790019295\ncurrent_RMSE:0.614223471702836\ncurrent_RMSE:0.6142234189367581\ncurrent_RMSE:0.6142229481753874\ncurrent_RMSE:0.6142222776192926\ncurrent_RMSE:0.614221661595656\ncurrent_RMSE:0.6142211001046424\ncurrent_RMSE:0.6142205931464007\ncurrent_RMSE:0.6142201407210661\ncurrent_RMSE:0.6142197428287592\ncurrent_RMSE:0.6142193994695859\ncurrent_RMSE:0.6142191106436379\ncurrent_RMSE:0.6142188763509918\ncurrent_RMSE:0.6142186965917101\ncurrent_RMSE:0.6142185713658408\ncurrent_RMSE:0.614218500673417\ncurrent_RMSE:0.6142184845144578\ncurrent_RMSE:0.6142182842061782\ncurrent_RMSE:0.6142175957190401\ncurrent_RMSE:0.6142169617647383\ncurrent_RMSE:0.6142163823434412\ncurrent_RMSE:0.6142158574553036\ncurrent_RMSE:0.614215387100465\ncurrent_RMSE:0.6142149712790508\ncurrent_RMSE:0.6142146099911716\ncurrent_RMSE:0.6142143032369238\ncurrent_RMSE:0.6142140510163892\ncurrent_RMSE:0.6142138533296346\ncurrent_RMSE:0.6142137101767131\ncurrent_RMSE:0.6142136215576626\ncurrent_RMSE:0.6142135874725067\ncurrent_RMSE:0.6142129529701115\ncurrent_RMSE:0.6142123010849116\ncurrent_RMSE:0.6142117037330946\ncurrent_RMSE:0.6142111609148196\ncurrent_RMSE:0.614210672630231\ncurrent_RMSE:0.614210238879459\ncurrent_RMSE:0.614209859662619\ncurrent_RMSE:0.6142095349798122\ncurrent_RMSE:0.6142092648311249\ncurrent_RMSE:0.6142090492166292\ncurrent_RMSE:0.6142088881363825\ncurrent_RMSE:0.6142087815904276\ncurrent_RMSE:0.614208729578793\ncurrent_RMSE:0.6142083493733941\ncurrent_RMSE:0.6142076795570675\ncurrent_RMSE:0.6142070642744971\ncurrent_RMSE:0.6142065035258467\ncurrent_RMSE:0.6142059973112658\ncurrent_RMSE:0.6142055456308889\ncurrent_RMSE:0.6142051484848366\ncurrent_RMSE:0.6142048058732148\ncurrent_RMSE:0.6142045177961145\ncurrent_RMSE:0.6142042842536125\ncurrent_RMSE:0.614204105245771\ncurrent_RMSE:0.6142039807726377\ncurrent_RMSE:0.6142039108342457\ncurrent_RMSE:0.6142038954306138\ncurrent_RMSE:0.6142037849297686\ncurrent_RMSE:0.6142030971820898\ncurrent_RMSE:0.614202463968536\ncurrent_RMSE:0.6142018852892757\ncurrent_RMSE:0.6142013611444633\ncurrent_RMSE:0.6142008915342382\ncurrent_RMSE:0.6142004764587256\ncurrent_RMSE:0.614200115918036\ncurrent_RMSE:0.6141998099122655\ncurrent_RMSE:0.6141995584414957\ncurrent_RMSE:0.6141993615057932\ncurrent_RMSE:0.6141992191052109\ncurrent_RMSE:0.6141991312397865\ncurrent_RMSE:0.6141990979095435\ncurrent_RMSE:0.6141985539608547\ncurrent_RMSE:0.6141979028160908\ncurrent_RMSE:0.6141973062059897\ncurrent_RMSE:0.6141967641307101\ncurrent_RMSE:0.6141962765903967\ncurrent_RMSE:0.6141958435851791\ncurrent_RMSE:0.6141954651151726\ncurrent_RMSE:0.6141951411804782\ncurrent_RMSE:0.6141948717811824\ncurrent_RMSE:0.6141946569173563\ncurrent_RMSE:0.6141944965890577\ncurrent_RMSE:0.6141943907963291\ncurrent_RMSE:0.6141943395391987\ncurrent_RMSE:0.614194049894231\ncurrent_RMSE:0.6141933808180341\ncurrent_RMSE:0.6141927662768644\ncurrent_RMSE:0.6141922062708857\ncurrent_RMSE:0.614191700800247\ncurrent_RMSE:0.6141912498650831\ncurrent_RMSE:0.6141908534655142\ncurrent_RMSE:0.6141905116016457\ncurrent_RMSE:0.6141902242735687\ncurrent_RMSE:0.6141899914813599\ncurrent_RMSE:0.6141898132250813\ncurrent_RMSE:0.61418968950478\ncurrent_RMSE:0.6141896203204895\ncurrent_RMSE:0.6141896056722279\ncurrent_RMSE:0.6141895849830803\ncurrent_RMSE:0.6141888979752305\ncurrent_RMSE:0.614188265502768\ncurrent_RMSE:0.6141876875658612\ncurrent_RMSE:0.6141871641646641\ncurrent_RMSE:0.6141866952993162\ncurrent_RMSE:0.6141862809699424\ncurrent_RMSE:0.614185921176653\ncurrent_RMSE:0.6141856159195439\ncurrent_RMSE:0.6141853651986963\ncurrent_RMSE:0.6141851690141771\ncurrent_RMSE:0.6141850273660385\ncurrent_RMSE:0.6141849402543184\ncurrent_RMSE:0.6141849076790397\ncurrent_RMSE:0.6141844542885375\ncurrent_RMSE:0.6141838038845613\ncurrent_RMSE:0.6141832080165013\ncurrent_RMSE:0.6141826666845159\ncurrent_RMSE:0.6141821798887496\ncurrent_RMSE:0.6141817476293321\ncurrent_RMSE:0.6141813699063783\ncurrent_RMSE:0.614181046719989\ncurrent_RMSE:0.6141807780702503\ncurrent_RMSE:0.6141805639572336\ncurrent_RMSE:0.6141804043809961\ncurrent_RMSE:0.6141802993415802\ncurrent_RMSE:0.6141802488390141\ncurrent_RMSE:0.6141800497588049\ncurrent_RMSE:0.6141793814230977\ncurrent_RMSE:0.6141787676236624\ncurrent_RMSE:0.6141782083606625\ncurrent_RMSE:0.6141777036342468\ncurrent_RMSE:0.61417725344455\ncurrent_RMSE:0.614176857791692\ncurrent_RMSE:0.614176516675778\ncurrent_RMSE:0.6141762300968989\ncurrent_RMSE:0.6141759980551313\ncurrent_RMSE:0.6141758205505369\ncurrent_RMSE:0.6141756975831629\ncurrent_RMSE:0.614175629153042\ncurrent_RMSE:0.6141756152601926\ncurrent_RMSE:0.614174998119223\ncurrent_RMSE:0.614174366388194\ncurrent_RMSE:0.6141737891939564\ncurrent_RMSE:0.6141732665366639\ncurrent_RMSE:0.6141727984164558\ncurrent_RMSE:0.6141723848334568\ncurrent_RMSE:0.6141720257877772\ncurrent_RMSE:0.6141717212795124\ncurrent_RMSE:0.6141714713087437\ncurrent_RMSE:0.6141712758755377\ncurrent_RMSE:0.6141711349799462\ncurrent_RMSE:0.6141710486220071\ncurrent_RMSE:0.6141710168017432\ncurrent_RMSE:0.6141706539737758\ncurrent_RMSE:0.6141700043109379\ncurrent_RMSE:0.614169409185243\ncurrent_RMSE:0.6141688685968496\ncurrent_RMSE:0.6141683825459017\ncurrent_RMSE:0.6141679510325286\ncurrent_RMSE:0.6141675740568455\ncurrent_RMSE:0.6141672516189528\ncurrent_RMSE:0.6141669837189364\ncurrent_RMSE:0.6141667703568675\ncurrent_RMSE:0.614166611532803\ncurrent_RMSE:0.6141665072467853\ncurrent_RMSE:0.6141664574988422\ncurrent_RMSE:0.6141663489875867\ncurrent_RMSE:0.6141656813927284\ncurrent_RMSE:0.6141650683353601\ncurrent_RMSE:0.6141645098156451\ncurrent_RMSE:0.6141640058337321\ncurrent_RMSE:0.6141635563897553\ncurrent_RMSE:0.6141631614838347\ncurrent_RMSE:0.6141628211160752\ncurrent_RMSE:0.6141625352865676\ncurrent_RMSE:0.6141623039953882\ncurrent_RMSE:0.6141621272425983\ncurrent_RMSE:0.6141620050282454\ncurrent_RMSE:0.6141619373523616\ncurrent_RMSE:0.6141619242149653\ncurrent_RMSE:0.6141613976343925\ncurrent_RMSE:0.6141607666451381\ncurrent_RMSE:0.6141601901938841\ncurrent_RMSE:0.6141596682807843\ncurrent_RMSE:0.6141592009059775\ncurrent_RMSE:0.6141587880695885\ncurrent_RMSE:0.614158429771727\ncurrent_RMSE:0.6141581260124885\ncurrent_RMSE:0.6141578767919541\ncurrent_RMSE:0.6141576821101901\ncurrent_RMSE:0.6141575419672483\ncurrent_RMSE:0.6141574563631661\ncurrent_RMSE:0.6141574252979662\ncurrent_RMSE:0.6141571530367496\ncurrent_RMSE:0.6141565041153997\ncurrent_RMSE:0.614155909732393\ncurrent_RMSE:0.614155369887888\ncurrent_RMSE:0.6141548845820284\ncurrent_RMSE:0.6141544538149435\ncurrent_RMSE:0.614154077586748\ncurrent_RMSE:0.6141537558975423\ncurrent_RMSE:0.614153488747412\ncurrent_RMSE:0.6141532761364282\ncurrent_RMSE:0.6141531180646478\ncurrent_RMSE:0.6141530145321126\ncurrent_RMSE:0.6141529655388502\ncurrent_RMSE:0.6141529476006115\ncurrent_RMSE:0.6141522807469603\ncurrent_RMSE:0.6141516684319905\ncurrent_RMSE:0.6141511106558655\ncurrent_RMSE:0.6141506074187335\ncurrent_RMSE:0.6141501587207289\ncurrent_RMSE:0.614149764561971\ncurrent_RMSE:0.614149424942565\ncurrent_RMSE:0.6141491398626013\ncurrent_RMSE:0.6141489093221558\ncurrent_RMSE:0.61414873332129\ncurrent_RMSE:0.6141486118600508\ncurrent_RMSE:0.6141485449384704\ncurrent_RMSE:0.6141485325565668\ncurrent_RMSE:0.6141480965406277\ncurrent_RMSE:0.614147466293488\ncurrent_RMSE:0.6141468905855314\ncurrent_RMSE:0.6141463694169111\ncurrent_RMSE:0.6141459027877663\ncurrent_RMSE:0.614145490698221\ncurrent_RMSE:0.614145133148385\ncurrent_RMSE:0.6141448301383539\ncurrent_RMSE:0.6141445816682081\ncurrent_RMSE:0.6141443877380138\ncurrent_RMSE:0.6141442483478229\ncurrent_RMSE:0.6141441634976723\ncurrent_RMSE:0.6141441331875848\ncurrent_RMSE:0.6141439514972027\ncurrent_RMSE:0.6141433033176894\ncurrent_RMSE:0.614142709677693\ncurrent_RMSE:0.614142170577372\ncurrent_RMSE:0.6141416860168697\ncurrent_RMSE:0.6141412559963153\ncurrent_RMSE:0.6141408805158235\ncurrent_RMSE:0.6141405595754941\ncurrent_RMSE:0.6141402931754129\ncurrent_RMSE:0.6141400813156505\ncurrent_RMSE:0.6141399239962636\ncurrent_RMSE:0.6141398212172942\ncurrent_RMSE:0.6141397729787693\ncurrent_RMSE:0.614139179505391\ncurrent_RMSE:0.6141385679331504\ncurrent_RMSE:0.6141380109009192\ncurrent_RMSE:0.614137508408846\ncurrent_RMSE:0.6141370604570643\ncurrent_RMSE:0.6141366670456938\ncurrent_RMSE:0.6141363281748391\ncurrent_RMSE:0.6141360438445904\ncurrent_RMSE:0.6141358140550238\ncurrent_RMSE:0.6141356388062001\ncurrent_RMSE:0.6141355180981664\ncurrent_RMSE:0.6141354519309545\ncurrent_RMSE:0.6141354403045823\ncurrent_RMSE:0.6141350948573817\ncurrent_RMSE:0.6141344653526957\ncurrent_RMSE:0.6141338903883489\ncurrent_RMSE:0.6141333699644942\ncurrent_RMSE:0.6141329040812706\ncurrent_RMSE:0.6141324927388019\ncurrent_RMSE:0.6141321359371981\ncurrent_RMSE:0.6141318336765539\ncurrent_RMSE:0.6141315859569499\ncurrent_RMSE:0.6141313927784523\ncurrent_RMSE:0.6141312541411124\ncurrent_RMSE:0.6141311700449671\ncurrent_RMSE:0.6141311404900389\ncurrent_RMSE:0.6141310493744432\ncurrent_RMSE:0.6141304019371139\ncurrent_RMSE:0.6141298090404489\ncurrent_RMSE:0.6141292706846062\ncurrent_RMSE:0.6141287868697292\ncurrent_RMSE:0.6141283575959468\ncurrent_RMSE:0.6141279828633733\ncurrent_RMSE:0.6141276626721086\ncurrent_RMSE:0.6141273970222381\ncurrent_RMSE:0.6141271859138323\ncurrent_RMSE:0.6141270293469477\ncurrent_RMSE:0.6141269273216261\ncurrent_RMSE:0.6141268798378945\ncurrent_RMSE:0.6141263776871824\ncurrent_RMSE:0.6141257668580004\ncurrent_RMSE:0.6141252105699663\ncurrent_RMSE:0.6141247088232279\ncurrent_RMSE:0.6141242616179192\ncurrent_RMSE:0.6141238689541592\ncurrent_RMSE:0.6141235308320525\ncurrent_RMSE:0.6141232472516893\ncurrent_RMSE:0.6141230182131451\ncurrent_RMSE:0.6141228437164808\ncurrent_RMSE:0.6141227237617432\ncurrent_RMSE:0.614122658348964\ncurrent_RMSE:0.6141226474781606\ncurrent_RMSE:0.614122392603671\ncurrent_RMSE:0.6141217638417766\ncurrent_RMSE:0.614121189621351\ncurrent_RMSE:0.614120669942547\ncurrent_RMSE:0.6141202048055029\ncurrent_RMSE:0.6141197942103429\ncurrent_RMSE:0.6141194381571763\ncurrent_RMSE:0.6141191366460977\ncurrent_RMSE:0.614118889677188\ncurrent_RMSE:0.6141186972505125\ncurrent_RMSE:0.6141185593661226\ncurrent_RMSE:0.6141184760240552\ncurrent_RMSE:0.6141184472243323\ncurrent_RMSE:0.6141184466873423\ncurrent_RMSE:0.6141177999925436\ncurrent_RMSE:0.6141172078395299\ncurrent_RMSE:0.6141166702284591\ncurrent_RMSE:0.6141161871594741\ncurrent_RMSE:0.614115758632704\ncurrent_RMSE:0.6141153846482627\ncurrent_RMSE:0.6141150652062498\ncurrent_RMSE:0.6141148003067507\ncurrent_RMSE:0.6141145899498358\ncurrent_RMSE:0.6141144341355611\ncurrent_RMSE:0.6141143328639682\ncurrent_RMSE:0.6141142861350841\ncurrent_RMSE:0.6141138753110597\ncurrent_RMSE:0.6141132652252649\ncurrent_RMSE:0.6141127096817295\ncurrent_RMSE:0.6141122086806016\ncurrent_RMSE:0.6141117622220146\ncurrent_RMSE:0.6141113703060874\ncurrent_RMSE:0.6141110329329245\ncurrent_RMSE:0.6141107501026158\ncurrent_RMSE:0.6141105218152367\ncurrent_RMSE:0.614110348070848\ncurrent_RMSE:0.6141102288694958\ncurrent_RMSE:0.6141101642112121\ncurrent_RMSE:0.6141101540960141\ncurrent_RMSE:0.6141099897980757\ncurrent_RMSE:0.6141093617793101\ncurrent_RMSE:0.6141087883031161\ncurrent_RMSE:0.6141082693696465\ncurrent_RMSE:0.6141078049790394\ncurrent_RMSE:0.6141073951314185\ncurrent_RMSE:0.6141070398268933\ncurrent_RMSE:0.6141067390655582\ncurrent_RMSE:0.6141064928474936\ncurrent_RMSE:0.6141063011727649\ncurrent_RMSE:0.6141061640414232\ncurrent_RMSE:0.614106081453505\ncurrent_RMSE:0.6141060534090325\ncurrent_RMSE:0.6141054975024126\ncurrent_RMSE:0.6141049060933692\ncurrent_RMSE:0.6141043692273624\ncurrent_RMSE:0.6141038869045354\ncurrent_RMSE:0.6141034591250167\ncurrent_RMSE:0.6141030858889203\ncurrent_RMSE:0.6141027671963455\ncurrent_RMSE:0.6141025030473775\ncurrent_RMSE:0.6141022934420862\ncurrent_RMSE:0.614102138380528\ncurrent_RMSE:0.6141020378627439\ncurrent_RMSE:0.6141019918887607\ncurrent_RMSE:0.6141016723953118\ncurrent_RMSE:0.6141010630532315\ncurrent_RMSE:0.6141005082544959\ncurrent_RMSE:0.6141000079992527\ncurrent_RMSE:0.6140995622876351\ncurrent_RMSE:0.6140991711197619\ncurrent_RMSE:0.6140988344957375\ncurrent_RMSE:0.6140985524156514\ncurrent_RMSE:0.6140983248795788\ncurrent_RMSE:0.6140981518875805\ncurrent_RMSE:0.6140980334397024\ncurrent_RMSE:0.6140979695359762\ncurrent_RMSE:0.6140979601764187\ncurrent_RMSE:0.6140978864587395\ncurrent_RMSE:0.6140972591834386\ncurrent_RMSE:0.6140966864517856\ncurrent_RMSE:0.6140961682639331\ncurrent_RMSE:0.6140957046200192\ncurrent_RMSE:0.6140952955201674\ncurrent_RMSE:0.6140949409644868\ncurrent_RMSE:0.6140946409530718\ncurrent_RMSE:0.6140943954860022\ncurrent_RMSE:0.6140942045633436\ncurrent_RMSE:0.614094068185147\ncurrent_RMSE:0.6140939863514484\ncurrent_RMSE:0.6140939590622698\ncurrent_RMSE:0.6140934944847183\ncurrent_RMSE:0.6140929038199628\ncurrent_RMSE:0.6140923676993116\ncurrent_RMSE:0.6140918861229073\ncurrent_RMSE:0.6140914590908783\ncurrent_RMSE:0.6140910866033382\ncurrent_RMSE:0.6140907686603866\ncurrent_RMSE:0.614090505262108\ncurrent_RMSE:0.6140902964085726\ncurrent_RMSE:0.6140901420998361\ncurrent_RMSE:0.6140900423359397\ncurrent_RMSE:0.6140899971169097\ncurrent_RMSE:0.6140897689577909\ncurrent_RMSE:0.6140891603597516\ncurrent_RMSE:0.6140886063061155\ncurrent_RMSE:0.6140881067970301\ncurrent_RMSE:0.6140876618326286\ncurrent_RMSE:0.6140872714130297\ncurrent_RMSE:0.6140869355383373\ncurrent_RMSE:0.6140866542086408\ncurrent_RMSE:0.6140864274240153\ncurrent_RMSE:0.6140862551845213\ncurrent_RMSE:0.6140861374902045\ncurrent_RMSE:0.6140860743410964\ncurrent_RMSE:0.6140860657372139\ncurrent_RMSE:0.6140854560718682\ncurrent_RMSE:0.6140848840850645\ncurrent_RMSE:0.6140843666431108\ncurrent_RMSE:0.6140839037461453\ncurrent_RMSE:0.6140834953942911\ncurrent_RMSE:0.614083141587657\ncurrent_RMSE:0.6140828423263375\ncurrent_RMSE:0.6140825976104121\ncurrent_RMSE:0.6140824074399461\ncurrent_RMSE:0.6140822718149903\ncurrent_RMSE:0.6140821907355806\ncurrent_RMSE:0.6140821642017388\ncurrent_RMSE:0.6140817909570213\ncurrent_RMSE:0.6140812010368708\ncurrent_RMSE:0.6140806656618653\ncurrent_RMSE:0.6140801848321473\ncurrent_RMSE:0.614079758547845\ncurrent_RMSE:0.614079386809072\ncurrent_RMSE:0.6140790696159275\ncurrent_RMSE:0.6140788069684959\ncurrent_RMSE:0.6140785988668471\ncurrent_RMSE:0.6140784453110366\ncurrent_RMSE:0.6140783463011055\ncurrent_RMSE:0.61407830183708\ncurrent_RMSE:0.6140781650159127\ncurrent_RMSE:0.6140775571622396\ncurrent_RMSE:0.6140770038540017\ncurrent_RMSE:0.6140765050913464\ncurrent_RMSE:0.6140760608744066\ncurrent_RMSE:0.6140756712033009\ncurrent_RMSE:0.6140753360781329\ncurrent_RMSE:0.614075055498992\ncurrent_RMSE:0.6140748294659529\ncurrent_RMSE:0.614074657979076\ncurrent_RMSE:0.6140745410384069\ncurrent_RMSE:0.6140744786439766\ncurrent_RMSE:0.6140744707958019\ncurrent_RMSE:0.6140739524618678\ncurrent_RMSE:0.6140733812202207\ncurrent_RMSE:0.6140728645244468\ncurrent_RMSE:0.6140724023746836\ncurrent_RMSE:0.6140719947710546\ncurrent_RMSE:0.6140716417136681\ncurrent_RMSE:0.6140713432026185\ncurrent_RMSE:0.6140710992379851\ncurrent_RMSE:0.614070909819833\ncurrent_RMSE:0.6140707749482127\ncurrent_RMSE:0.6140706946231601\ncurrent_RMSE:0.6140706688446966\ncurrent_RMSE:0.6140703869364457\ncurrent_RMSE:0.6140697977612157\ncurrent_RMSE:0.614069263132145\ncurrent_RMSE:0.6140687830493761\ncurrent_RMSE:0.6140683575130367\ncurrent_RMSE:0.6140679865232402\ncurrent_RMSE:0.6140676700800857\ncurrent_RMSE:0.6140674081836573\ncurrent_RMSE:0.614067200834025\ncurrent_RMSE:0.6140670480312439\ncurrent_RMSE:0.6140669497753547\ncurrent_RMSE:0.6140669060663837\ncurrent_RMSE:0.6140668605866554\ncurrent_RMSE:0.6140662534776727\ncurrent_RMSE:0.6140657009151308\ncurrent_RMSE:0.6140652028991768\ncurrent_RMSE:0.6140647594299433\ncurrent_RMSE:0.6140643705075487\ncurrent_RMSE:0.6140640361320967\ncurrent_RMSE:0.6140637563036762\ncurrent_RMSE:0.6140635310223618\ncurrent_RMSE:0.6140633602882136\ncurrent_RMSE:0.6140632441012771\ncurrent_RMSE:0.6140631824615833\ncurrent_RMSE:0.6140631753691486\ncurrent_RMSE:0.61406274837027\ncurrent_RMSE:0.6140621778740856\ncurrent_RMSE:0.6140616619247707\ncurrent_RMSE:0.6140612005224632\ncurrent_RMSE:0.6140607936672857\ncurrent_RMSE:0.6140604413593469\ncurrent_RMSE:0.6140601435987404\ncurrent_RMSE:0.6140599003855458\ncurrent_RMSE:0.6140597117198278\ncurrent_RMSE:0.6140595776016367\ncurrent_RMSE:0.6140594980310081\ncurrent_RMSE:0.6140594730079635\ncurrent_RMSE:0.6140592824396782\ncurrent_RMSE:0.6140586940096837\ncurrent_RMSE:0.614058160126836\ncurrent_RMSE:0.6140576807912774\ncurrent_RMSE:0.6140572560031358\ncurrent_RMSE:0.6140568857625244\ncurrent_RMSE:0.6140565700695416\ncurrent_RMSE:0.6140563089242718\ncurrent_RMSE:0.6140561023267846\ncurrent_RMSE:0.6140559502771349\ncurrent_RMSE:0.6140558527753632\ncurrent_RMSE:0.6140558098214958\ncurrent_RMSE:0.6140552493225919\ncurrent_RMSE:0.6140546975060425\ncurrent_RMSE:0.6140542002370597\ncurrent_RMSE:0.6140537575157761\ncurrent_RMSE:0.6140533693423096\ncurrent_RMSE:0.6140530357167637\ncurrent_RMSE:0.6140527566392273\ncurrent_RMSE:0.614052532109775\ncurrent_RMSE:0.6140523621284661\ncurrent_RMSE:0.6140522466953464\ncurrent_RMSE:0.6140521858104464\ncurrent_RMSE:0.6140521794737825\ncurrent_RMSE:0.6140518438134697\ncurrent_RMSE:0.6140512740630532\ncurrent_RMSE:0.6140507588604761\ncurrent_RMSE:0.6140502982058761\ncurrent_RMSE:0.6140498920993757\ncurrent_RMSE:0.6140495405410832\ncurrent_RMSE:0.6140492435310921\ncurrent_RMSE:0.614049001069482\ncurrent_RMSE:0.6140488131563171\ncurrent_RMSE:0.6140486797916477\ncurrent_RMSE:0.6140486009755093\ncurrent_RMSE:0.6140485767079228\ncurrent_RMSE:0.6140484774829689\ncurrent_RMSE:0.6140478897985232\ncurrent_RMSE:0.6140473566621856\ncurrent_RMSE:0.614046878074098\ncurrent_RMSE:0.6140464540343882\ncurrent_RMSE:0.6140460845431689\ncurrent_RMSE:0.6140457696005388\ncurrent_RMSE:0.6140455092065817\ncurrent_RMSE:0.6140453033613672\ncurrent_RMSE:0.6140451520649499\ncurrent_RMSE:0.6140450553173703\ncurrent_RMSE:0.6140450131186541\ncurrent_RMSE:0.6140445447131003\ncurrent_RMSE:0.614043993642839\ncurrent_RMSE:0.6140434971210965\ncurrent_RMSE:0.6140430551480051\ncurrent_RMSE:0.6140426677236825\ncurrent_RMSE:0.6140423348482321\ncurrent_RMSE:0.6140420565217426\ncurrent_RMSE:0.6140418327442881\ncurrent_RMSE:0.6140416635159283\ncurrent_RMSE:0.6140415488367083\ncurrent_RMSE:0.6140414887066585\ncurrent_RMSE:0.6140414831257952\ncurrent_RMSE:0.6140412388074251\ncurrent_RMSE:0.6140406698030805\ncurrent_RMSE:0.6140401553475187\ncurrent_RMSE:0.614039695440877\ncurrent_RMSE:0.614039290083278\ncurrent_RMSE:0.6140389392748296\ncurrent_RMSE:0.6140386430156253\ncurrent_RMSE:0.6140384013057442\ncurrent_RMSE:0.6140382141452505\ncurrent_RMSE:0.6140380815341941\ncurrent_RMSE:0.6140380034726106\ncurrent_RMSE:0.6140379799605207\ncurrent_RMSE:0.6140379720821301\ncurrent_RMSE:0.6140373851435459\ncurrent_RMSE:0.6140368527540043\ncurrent_RMSE:0.6140363749136472\ncurrent_RMSE:0.6140359516226019\ncurrent_RMSE:0.6140355828809811\ncurrent_RMSE:0.6140352686888834\ncurrent_RMSE:0.6140350090463921\ncurrent_RMSE:0.6140348039535768\ncurrent_RMSE:0.6140346534104919\ncurrent_RMSE:0.6140345574171776\ncurrent_RMSE:0.6140345159736595\ncurrent_RMSE:0.6140341396648643\ncurrent_RMSE:0.6140335893411856\ncurrent_RMSE:0.6140330935669511\ncurrent_RMSE:0.6140326523422932\ncurrent_RMSE:0.6140322656673294\ncurrent_RMSE:0.6140319335421627\ncurrent_RMSE:0.6140316559668815\ncurrent_RMSE:0.61403143294156\ncurrent_RMSE:0.6140312644662576\ncurrent_RMSE:0.6140311505410191\ncurrent_RMSE:0.614031091165875\ncurrent_RMSE:0.6140310863408409\ncurrent_RMSE:0.6140309333676568\ncurrent_RMSE:0.6140303651096871\ncurrent_RMSE:0.614029851401417\ncurrent_RMSE:0.6140293922429836\ncurrent_RMSE:0.6140289876345092\ncurrent_RMSE:0.6140286375761016\ncurrent_RMSE:0.6140283420678541\ncurrent_RMSE:0.6140281011098454\ncurrent_RMSE:0.6140279147021399\ncurrent_RMSE:0.6140277828447871\ncurrent_RMSE:0.6140277055378223\ncurrent_RMSE:0.614027682781266\ncurrent_RMSE:0.6140271800601261\ncurrent_RMSE:0.6140266484176655\ncurrent_RMSE:0.6140261713252971\ncurrent_RMSE:0.6140257487831481\ncurrent_RMSE:0.6140253807913311\ncurrent_RMSE:0.6140250673499441\ncurrent_RMSE:0.6140248084590708\ncurrent_RMSE:0.6140246041187801\ncurrent_RMSE:0.6140244543291264\ncurrent_RMSE:0.6140243590901499\ncurrent_RMSE:0.6140243184018755\ncurrent_RMSE:0.6140240341931125\ncurrent_RMSE:0.6140234846163097\ncurrent_RMSE:0.6140229895898504\ncurrent_RMSE:0.6140225491138662\ncurrent_RMSE:0.6140221631884748\ncurrent_RMSE:0.6140218318137788\ncurrent_RMSE:0.6140215549898665\ncurrent_RMSE:0.6140213327168119\ncurrent_RMSE:0.6140211649946743\ncurrent_RMSE:0.6140210518234981\ncurrent_RMSE:0.6140209932033135\ncurrent_RMSE:0.6140209891341364\ncurrent_RMSE:0.6140209275092483\ncurrent_RMSE:0.6140203599979555\ncurrent_RMSE:0.6140198470372523\ncurrent_RMSE:0.614019388627276\ncurrent_RMSE:0.6140189847681484\ncurrent_RMSE:0.6140186354599771\ncurrent_RMSE:0.6140183407028554\ncurrent_RMSE:0.6140181004968617\ncurrent_RMSE:0.6140179148420601\ncurrent_RMSE:0.6140177837385001\ncurrent_RMSE:0.6140177071862167\ncurrent_RMSE:0.6140176851852301\ncurrent_RMSE:0.6140172745632007\ncurrent_RMSE:0.6140167436681049\ncurrent_RMSE:0.6140162673239825\ncurrent_RMSE:0.6140158455309604\ncurrent_RMSE:0.6140154782891513\ncurrent_RMSE:0.6140151655986528\ncurrent_RMSE:0.6140149074595482\ncurrent_RMSE:0.6140147038719064\ncurrent_RMSE:0.6140145548357817\ncurrent_RMSE:0.6140144603512139\ncurrent_RMSE:0.614014420418228\ncurrent_RMSE:0.6140142283126361\ncurrent_RMSE:0.6140136794830019\ncurrent_RMSE:0.6140131852045834\ncurrent_RMSE:0.6140127454775122\ncurrent_RMSE:0.6140123603019056\ncurrent_RMSE:0.6140120296778664\ncurrent_RMSE:0.6140117536054825\ncurrent_RMSE:0.6140115320848277\ncurrent_RMSE:0.614011365115961\ncurrent_RMSE:0.6140112526989268\ncurrent_RMSE:0.614011194833755\ncurrent_RMSE:0.6140111915204612\ncurrent_RMSE:0.6140106544825301\ncurrent_RMSE:0.6140101422696684\ncurrent_RMSE:0.6140096846083968\ncurrent_RMSE:0.614009281498837\ncurrent_RMSE:0.6140089329410967\ncurrent_RMSE:0.6140086389352687\ncurrent_RMSE:0.6140083994814313\ncurrent_RMSE:0.6140082145796485\ncurrent_RMSE:0.6140080842299693\ncurrent_RMSE:0.6140080084324288\ncurrent_RMSE:0.6140079871870469\ncurrent_RMSE:0.6140076686672686\ncurrent_RMSE:0.6140071385198206\ncurrent_RMSE:0.6140066629242004\ncurrent_RMSE:0.614006241880535\ncurrent_RMSE:0.6140058753889367\ncurrent_RMSE:0.6140055634495029\ncurrent_RMSE:0.614005306062317\ncurrent_RMSE:0.6140051032274474\ncurrent_RMSE:0.6140049549449483\ncurrent_RMSE:0.6140048612148593\ncurrent_RMSE:0.6140048220372052\ncurrent_RMSE:0.6140047220377892\ncurrent_RMSE:0.6140041739556146\ncurrent_RMSE:0.6140036804255016\ncurrent_RMSE:0.6140032414475814\ncurrent_RMSE:0.6140028570219714\ncurrent_RMSE:0.614002527148774\ncurrent_RMSE:0.6140022518280768\ncurrent_RMSE:0.6140020310599535\ncurrent_RMSE:0.6140018648444628\ncurrent_RMSE:0.6140017531816492\ncurrent_RMSE:0.6140016960715421\ncurrent_RMSE:0.6140016935141571\ncurrent_RMSE:0.6140012485776184\ncurrent_RMSE:0.6140007371128713\ncurrent_RMSE:0.614000280200551\ncurrent_RMSE:0.6139998778407793\ncurrent_RMSE:0.6139995300336631\ncurrent_RMSE:0.6139992367792957\ncurrent_RMSE:0.6139989980777549\ncurrent_RMSE:0.6139988139291044\ncurrent_RMSE:0.6139986843333932\ncurrent_RMSE:0.613998609290656\ncurrent_RMSE:0.6139985888009127\ncurrent_RMSE:0.6139983623863918\ncurrent_RMSE:0.6139978329868732\ncurrent_RMSE:0.6139973581400103\ncurrent_RMSE:0.6139969378459301\ncurrent_RMSE:0.6139965721047445\ncurrent_RMSE:0.6139962609165509\ncurrent_RMSE:0.6139960042814322\ncurrent_RMSE:0.6139958021994572\ncurrent_RMSE:0.6139956546706793\ncurrent_RMSE:0.613995561695138\ncurrent_RMSE:0.6139955232728581\ncurrent_RMSE:0.6139955153824876\ncurrent_RMSE:0.613994968048063\ncurrent_RMSE:0.6139944752665191\ncurrent_RMSE:0.6139940370379872\ncurrent_RMSE:0.613993653362584\ncurrent_RMSE:0.6139933242404121\ncurrent_RMSE:0.6139930496715589\ncurrent_RMSE:0.6139928296560978\ncurrent_RMSE:0.6139926641940873\ncurrent_RMSE:0.6139925532855716\ncurrent_RMSE:0.6139924969305801\ncurrent_RMSE:0.613992495129128\ncurrent_RMSE:0.6139921422969896\ncurrent_RMSE:0.6139916315806293\ncurrent_RMSE:0.6139911754175058\ncurrent_RMSE:0.6139907738077409\ncurrent_RMSE:0.6139904267514416\ncurrent_RMSE:0.6139901342487004\ncurrent_RMSE:0.6139898962995953\ncurrent_RMSE:0.6139897129041897\ncurrent_RMSE:0.6139895840625326\ncurrent_RMSE:0.613989509774658\ncurrent_RMSE:0.6139894900405861\ncurrent_RMSE:0.6139893557341941\ncurrent_RMSE:0.6139888270828854\ncurrent_RMSE:0.613988352985034\ncurrent_RMSE:0.6139879334407663\ncurrent_RMSE:0.6139875684501943\ncurrent_RMSE:0.6139872580134151\ncurrent_RMSE:0.6139870021305116\ncurrent_RMSE:0.6139868008015519\ncurrent_RMSE:0.6139866540265897\ncurrent_RMSE:0.613986561805664\ncurrent_RMSE:0.6139865241387996\ncurrent_RMSE:0.6139860617738243\ncurrent_RMSE:0.613985569741112\ncurrent_RMSE:0.6139851322622042\ncurrent_RMSE:0.6139847493372175\ncurrent_RMSE:0.6139844209662537\ncurrent_RMSE:0.6139841471494006\ncurrent_RMSE:0.6139839278867312\ncurrent_RMSE:0.6139837631783038\ncurrent_RMSE:0.6139836530241625\ncurrent_RMSE:0.6139835974243364\ncurrent_RMSE:0.6139835963788405\ncurrent_RMSE:0.6139833356539756\ncurrent_RMSE:0.6139828256862729\ncurrent_RMSE:0.6139823702725908\ncurrent_RMSE:0.6139819694130507\ncurrent_RMSE:0.6139816231077594\ncurrent_RMSE:0.6139813313568092\ncurrent_RMSE:0.6139810941602779\ncurrent_RMSE:0.6139809115182286\ncurrent_RMSE:0.6139807834307103\ncurrent_RMSE:0.6139807098977568\ncurrent_RMSE:0.6139806909193879\ncurrent_RMSE:0.6139806487238613\ncurrent_RMSE:0.6139801208210421\ncurrent_RMSE:0.6139796474724551\ncurrent_RMSE:0.6139792286782265\ncurrent_RMSE:0.6139788644384679\ncurrent_RMSE:0.6139785547532763\ncurrent_RMSE:0.6139782996227344\ncurrent_RMSE:0.6139780990469101\ncurrent_RMSE:0.6139779530258569\ncurrent_RMSE:0.6139778615596136\ncurrent_RMSE:0.6139778246482049\ncurrent_RMSE:0.6139774551459376\ncurrent_RMSE:0.6139769638623186\ncurrent_RMSE:0.6139765271332698\ncurrent_RMSE:0.6139761449589076\ncurrent_RMSE:0.6139758173393338\ncurrent_RMSE:0.613975544274636\ncurrent_RMSE:0.6139753257648868\ncurrent_RMSE:0.6139751618101443\ncurrent_RMSE:0.6139750524104526\ncurrent_RMSE:0.6139749975658405\ncurrent_RMSE:0.6139749972763228\ncurrent_RMSE:0.6139748286614698\ncurrent_RMSE:0.613974319442695\ncurrent_RMSE:0.6139738647786976\ncurrent_RMSE:0.613973464669599\ncurrent_RMSE:0.6139731191155058\ncurrent_RMSE:0.6139728281165102\ncurrent_RMSE:0.6139725916726896\ncurrent_RMSE:0.6139724097841072\ncurrent_RMSE:0.6139722824508114\ncurrent_RMSE:0.6139722096728361\ncurrent_RMSE:0.6139721914502007\ncurrent_RMSE:0.6139717142140906\ncurrent_RMSE:0.6139712416150199\ncurrent_RMSE:0.6139708235710556\ncurrent_RMSE:0.613970460082309\ncurrent_RMSE:0.6139701511488772\ncurrent_RMSE:0.6139698967708423\ncurrent_RMSE:0.6139696969482723\ncurrent_RMSE:0.6139695516812204\ncurrent_RMSE:0.6139694609697253\ncurrent_RMSE:0.6139694248138111\ncurrent_RMSE:0.6139691481770044\ncurrent_RMSE:0.613968657642739\ncurrent_RMSE:0.6139682216637831\ncurrent_RMSE:0.6139678402402526\ncurrent_RMSE:0.6139675133722495\ncurrent_RMSE:0.6139672410598608\ncurrent_RMSE:0.6139670233031591\ncurrent_RMSE:0.6139668601022026\ncurrent_RMSE:0.6139667514570345\ncurrent_RMSE:0.6139666973676841\ncurrent_RMSE:0.6139666213319279\ncurrent_RMSE:0.6139661128623498\ncurrent_RMSE:0.6139656589482796\ncurrent_RMSE:0.6139652595898383\ncurrent_RMSE:0.6139649147871323\ncurrent_RMSE:0.6139646245402537\ncurrent_RMSE:0.6139643888492798\ncurrent_RMSE:0.6139642077142734\ncurrent_RMSE:0.6139640811352828\ncurrent_RMSE:0.6139640091123417\ncurrent_RMSE:0.6139639916454693\ncurrent_RMSE:0.6139636072743395\ncurrent_RMSE:0.613963135425036\ncurrent_RMSE:0.6139627181315602\ncurrent_RMSE:0.6139623553940233\ncurrent_RMSE:0.6139620472125222\ncurrent_RMSE:0.6139617935871388\ncurrent_RMSE:0.6139615945179407\ncurrent_RMSE:0.6139614500049813\ncurrent_RMSE:0.6139613600482988\ncurrent_RMSE:0.6139613246479173\ncurrent_RMSE:0.6139611408791881\ncurrent_RMSE:0.6139606510945356\ncurrent_RMSE:0.6139602158659051\ncurrent_RMSE:0.6139598351934127\ncurrent_RMSE:0.6139595090771596\ncurrent_RMSE:0.6139592375172329\ncurrent_RMSE:0.6139590205137052\ncurrent_RMSE:0.6139588580666341\ncurrent_RMSE:0.6139587501760628\ncurrent_RMSE:0.6139586968420203\ncurrent_RMSE:0.6139582059572541\ncurrent_RMSE:0.6139577527933522\ncurrent_RMSE:0.6139573541857829\ncurrent_RMSE:0.6139570101346522\ncurrent_RMSE:0.613956720640052\ncurrent_RMSE:0.6139564857020595\ncurrent_RMSE:0.6139563053207373\ncurrent_RMSE:0.6139561794961335\ncurrent_RMSE:0.6139561082282816\ncurrent_RMSE:0.6139560915172005\ncurrent_RMSE:0.6139558000136599\ncurrent_RMSE:0.6139553289143732\ncurrent_RMSE:0.613954912371609\ncurrent_RMSE:0.6139545503854784\ncurrent_RMSE:0.6139542429560778\ncurrent_RMSE:0.6139539900834892\ncurrent_RMSE:0.61395379176778\ncurrent_RMSE:0.613953648009003\ncurrent_RMSE:0.6139535588071966\ncurrent_RMSE:0.6139535241623846\ncurrent_RMSE:0.6139534332642136\ncurrent_RMSE:0.6139529442294324\ncurrent_RMSE:0.613952509751359\ncurrent_RMSE:0.6139521298301094\ncurrent_RMSE:0.6139518044657849\ncurrent_RMSE:0.6139515336584722\ncurrent_RMSE:0.6139513174082435\ncurrent_RMSE:0.6139511557151563\ncurrent_RMSE:0.6139510485792539\ncurrent_RMSE:0.6139509960005648\ncurrent_RMSE:0.6139505987389862\ncurrent_RMSE:0.6139501463254928\ncurrent_RMSE:0.6139497484690088\ncurrent_RMSE:0.6139494051696404\ncurrent_RMSE:0.613949116427479\ncurrent_RMSE:0.6139488822426016\ncurrent_RMSE:0.6139487026150707\ncurrent_RMSE:0.613948577544934\ncurrent_RMSE:0.6139485070322253\ncurrent_RMSE:0.6139484910769627\ncurrent_RMSE:0.6139482924434841\ncurrent_RMSE:0.6139478220944631\ncurrent_RMSE:0.6139474063026327\ncurrent_RMSE:0.6139470450681036\ncurrent_RMSE:0.6139467383909725\ncurrent_RMSE:0.6139464862713208\ncurrent_RMSE:0.6139462887092159\ncurrent_RMSE:0.6139461457047104\ncurrent_RMSE:0.6139460572578425\ncurrent_RMSE:0.6139460233686357\ncurrent_RMSE:0.6139455370587149\ncurrent_RMSE:0.6139451033314293\ncurrent_RMSE:0.6139447241616267\ncurrent_RMSE:0.6139443995494079\ncurrent_RMSE:0.6139441294948599\ncurrent_RMSE:0.6139439139980543\ncurrent_RMSE:0.6139437530590486\ncurrent_RMSE:0.6139436466778859\ncurrent_RMSE:0.6139435948545945\ncurrent_RMSE:0.6139432912186861\ncurrent_RMSE:0.6139428395558402\ncurrent_RMSE:0.6139424424506541\ncurrent_RMSE:0.6139420999032337\ncurrent_RMSE:0.6139418119136703\ncurrent_RMSE:0.6139415784820407\ncurrent_RMSE:0.613941399608407\ncurrent_RMSE:0.6139412752928171\ncurrent_RMSE:0.613941205535304\ncurrent_RMSE:0.6139411903358863\ncurrent_RMSE:0.6139410845748066\ncurrent_RMSE:0.6139406149762986\ncurrent_RMSE:0.6139401999356229\ncurrent_RMSE:0.6139398394528899\ncurrent_RMSE:0.6139395335281959\ncurrent_RMSE:0.6139392821616223\ncurrent_RMSE:0.6139390853532363\ncurrent_RMSE:0.6139389431030902\ncurrent_RMSE:0.613938855411222\ncurrent_RMSE:0.6139388222776551\ncurrent_RMSE:0.6139384295932309\ncurrent_RMSE:0.6139379966169625\ncurrent_RMSE:0.6139376181988095\ncurrent_RMSE:0.613937294338873\ncurrent_RMSE:0.613937025037239\ncurrent_RMSE:0.6139368102939796\ncurrent_RMSE:0.6139366501091519\ncurrent_RMSE:0.6139365444827986\ncurrent_RMSE:0.6139364934149478\ncurrent_RMSE:0.6139362834070553\ncurrent_RMSE:0.613935832495095\ncurrent_RMSE:0.6139354361414182\ncurrent_RMSE:0.6139350943461306\ncurrent_RMSE:0.6139348071093234\ncurrent_RMSE:0.6139345744310729\ncurrent_RMSE:0.6139343963114414\ncurrent_RMSE:0.6139342727504764\ncurrent_RMSE:0.6139342037482107\ncurrent_RMSE:0.6139341893046628\ncurrent_RMSE:0.6139341764181832\ncurrent_RMSE:0.6139337075704349\ncurrent_RMSE:0.6139332932811337\ncurrent_RMSE:0.61393293355039\ncurrent_RMSE:0.6139326283782998\ncurrent_RMSE:0.6139323777649442\ncurrent_RMSE:0.6139321817103904\ncurrent_RMSE:0.6139320402146905\ncurrent_RMSE:0.6139319532778822\ncurrent_RMSE:0.6139319208999887\ncurrent_RMSE:0.6139316218433891\ncurrent_RMSE:0.6139311896183663\ncurrent_RMSE:0.613930811952065\ncurrent_RMSE:0.6139304888445857\ncurrent_RMSE:0.6139302202960145\ncurrent_RMSE:0.6139300063064231\ncurrent_RMSE:0.6139298468758686\ncurrent_RMSE:0.6139297420043933\ncurrent_RMSE:0.6139296916920254\ncurrent_RMSE:0.6139295753143573\ncurrent_RMSE:0.6139291251535194\ncurrent_RMSE:0.6139287295515622\ncurrent_RMSE:0.613928388508591\ncurrent_RMSE:0.6139281020246968\ncurrent_RMSE:0.6139278700999562\ncurrent_RMSE:0.6139276927344307\ncurrent_RMSE:0.6139275699281677\ncurrent_RMSE:0.6139275016812\ncurrent_RMSE:0.6139274879935457\ncurrent_RMSE:0.6139270998869881\ncurrent_RMSE:0.6139266863492803\ncurrent_RMSE:0.613926327370718\ncurrent_RMSE:0.6139260229513971\ncurrent_RMSE:0.6139257730913986\ncurrent_RMSE:0.6139255777907894\ncurrent_RMSE:0.6139254370496213\ncurrent_RMSE:0.6139253508679319\ncurrent_RMSE:0.6139253192457442\ncurrent_RMSE:0.6139251138191604\ncurrent_RMSE:0.6139246823456106\ncurrent_RMSE:0.6139243054313615\ncurrent_RMSE:0.6139239830765133\ncurrent_RMSE:0.6139237152811523\ncurrent_RMSE:0.6139235020453498\ncurrent_RMSE:0.6139233433691627\ncurrent_RMSE:0.613923239252633\ncurrent_RMSE:0.6139231896957889\ncurrent_RMSE:0.6139231669504166\ncurrent_RMSE:0.613922717540937\ncurrent_RMSE:0.6139223226909085\ncurrent_RMSE:0.6139219824004364\ncurrent_RMSE:0.6139216966696114\ncurrent_RMSE:0.6139214654985097\ncurrent_RMSE:0.6139212888871929\ncurrent_RMSE:0.6139211668357081\ncurrent_RMSE:0.6139210993440877\ncurrent_RMSE:0.6139210864123501\ncurrent_RMSE:0.6139207919356361\ncurrent_RMSE:0.6139203791497393\ncurrent_RMSE:0.6139200209235496\ncurrent_RMSE:0.6139197172571624\ncurrent_RMSE:0.6139194681506589\ncurrent_RMSE:0.6139192736041054\ncurrent_RMSE:0.6139191336175537\ncurrent_RMSE:0.6139190481910412\ncurrent_RMSE:0.6139190173245906\ncurrent_RMSE:0.6139189055300764\ncurrent_RMSE:0.6139184748082258\ncurrent_RMSE:0.6139180986462284\ncurrent_RMSE:0.6139177770441847\ncurrent_RMSE:0.6139175100021802\ncurrent_RMSE:0.6139172975202863\ncurrent_RMSE:0.6139171395985596\ncurrent_RMSE:0.6139170362370421\ncurrent_RMSE:0.6139169874357615\ncurrent_RMSE:0.613916609666733\ncurrent_RMSE:0.6139162155688414\ncurrent_RMSE:0.6139158760310497\ncurrent_RMSE:0.6139155910534487\ncurrent_RMSE:0.6139153606361142\ncurrent_RMSE:0.6139151847791076\ncurrent_RMSE:0.6139150634824759\ncurrent_RMSE:0.6139149967462514\ncurrent_RMSE:0.6139149845704518\ncurrent_RMSE:0.613914783725618\ncurrent_RMSE:0.613914371691749\ncurrent_RMSE:0.6139140142181218\ncurrent_RMSE:0.6139137113048319\ncurrent_RMSE:0.6139134629519599\ncurrent_RMSE:0.6139132691595722\ncurrent_RMSE:0.6139131299277204\ncurrent_RMSE:0.6139130452564416\ncurrent_RMSE:0.6139130151457585\ncurrent_RMSE:0.6139129969852308\ncurrent_RMSE:0.6139125670153045\ncurrent_RMSE:0.6139121916057574\ncurrent_RMSE:0.6139118707566898\ncurrent_RMSE:0.6139116044681872\ncurrent_RMSE:0.6139113927403205\ncurrent_RMSE:0.6139112355731461\ncurrent_RMSE:0.6139111329667061\ncurrent_RMSE:0.6139110849210277\ncurrent_RMSE:0.6139108015398537\ncurrent_RMSE:0.6139104081943059\ncurrent_RMSE:0.6139100694093751\ncurrent_RMSE:0.6139097851851518\ncurrent_RMSE:0.6139095555217116\ncurrent_RMSE:0.6139093804191159\ncurrent_RMSE:0.6139092598774111\ncurrent_RMSE:0.6139091938966296\ncurrent_RMSE:0.6139091824767888\ncurrent_RMSE:0.6139090752657346\ncurrent_RMSE:0.6139086639841087\ncurrent_RMSE:0.613908307263233\ncurrent_RMSE:0.6139080051032024\ncurrent_RMSE:0.6139077575040978\ncurrent_RMSE:0.6139075644659849\ncurrent_RMSE:0.6139074259889155\ncurrent_RMSE:0.6139073420729261\ncurrent_RMSE:0.6139073127180393\ncurrent_RMSE:0.6139069589755001\ncurrent_RMSE:0.6139065843186008\ncurrent_RMSE:0.6139062642226801\ncurrent_RMSE:0.6139059986878234\ncurrent_RMSE:0.6139057877141013\ncurrent_RMSE:0.6139056313015704\ncurrent_RMSE:0.6139055294502721\ncurrent_RMSE:0.6139054821602337\ncurrent_RMSE:0.6139052931688066\ncurrent_RMSE:0.6139049005758087\ncurrent_RMSE:0.6139045625439181\ncurrent_RMSE:0.6139042790732252\ncurrent_RMSE:0.6139040501638054\ncurrent_RMSE:0.6139038758157197\ncurrent_RMSE:0.6139037560290147\ncurrent_RMSE:0.6139036908037223\ncurrent_RMSE:0.61390368013986\ncurrent_RMSE:0.6139036665643475\ncurrent_RMSE:0.6139032560351793\ncurrent_RMSE:0.6139029000672427\ncurrent_RMSE:0.6139025986606329\ncurrent_RMSE:0.61390235181543\ncurrent_RMSE:0.6139021595316998\ncurrent_RMSE:0.6139020218094938\ncurrent_RMSE:0.6139019386488487\ncurrent_RMSE:0.6139019100497864\ncurrent_RMSE:0.6139016506970274\ncurrent_RMSE:0.6139012767929722\ncurrent_RMSE:0.6139009574503681\ncurrent_RMSE:0.6139006926693004\ncurrent_RMSE:0.6139004824498395\ncurrent_RMSE:0.6139003267920417\ncurrent_RMSE:0.6139002256959483\ncurrent_RMSE:0.6139001791615862\ncurrent_RMSE:0.6139000845616609\ncurrent_RMSE:0.6138996927214176\ncurrent_RMSE:0.6138993554427454\ncurrent_RMSE:0.6138990727257345\ncurrent_RMSE:0.61389884457046\ncurrent_RMSE:0.6138986709769828\ncurrent_RMSE:0.6138985519453491\ncurrent_RMSE:0.613898487475591\ncurrent_RMSE:0.6138984775677252\ncurrent_RMSE:0.6138981478528824\ncurrent_RMSE:0.6138977926380719\ncurrent_RMSE:0.6138974919850427\ncurrent_RMSE:0.6138972458938751\ncurrent_RMSE:0.6138970543646345\ncurrent_RMSE:0.6138969173973722\ncurrent_RMSE:0.6138968349921247\ncurrent_RMSE:0.6138968071489138\ncurrent_RMSE:0.6138966421876625\ncurrent_RMSE:0.6138962690366466\ncurrent_RMSE:0.6138959504475278\ncurrent_RMSE:0.6138956864203908\ncurrent_RMSE:0.6138954769553066\ncurrent_RMSE:0.6138953220523304\ncurrent_RMSE:0.6138952217115039\ncurrent_RMSE:0.6138951759328538\ncurrent_RMSE:0.6138951757260466\ncurrent_RMSE:0.6138947846387617\ncurrent_RMSE:0.613894448113485\ncurrent_RMSE:0.6138941661503065\ncurrent_RMSE:0.613893938749301\ncurrent_RMSE:0.6138937659105296\ncurrent_RMSE:0.6138936476340379\ncurrent_RMSE:0.6138935839198577\ncurrent_RMSE:0.613893574768006\ncurrent_RMSE:0.6138933394447013\ncurrent_RMSE:0.6138929849832023\ncurrent_RMSE:0.6138926850839128\ncurrent_RMSE:0.6138924397469128\ncurrent_RMSE:0.6138922489722675\ncurrent_RMSE:0.613892112760028\ncurrent_RMSE:0.6138920311102305\ncurrent_RMSE:0.6138920040228969\ncurrent_RMSE:0.6138919334547426\ncurrent_RMSE:0.61389156105696\ncurrent_RMSE:0.6138912432214939\ncurrent_RMSE:0.6138909799484288\ncurrent_RMSE:0.6138907712378352\ncurrent_RMSE:0.6138906170897684\ncurrent_RMSE:0.6138905175042698\ncurrent_RMSE:0.6138904724813657\ncurrent_RMSE:0.6138901763350307\ncurrent_RMSE:0.6138898405633257\ncurrent_RMSE:0.6138895593541289\ncurrent_RMSE:0.6138893327075153\ncurrent_RMSE:0.6138891606235456\ncurrent_RMSE:0.6138890431022653\ncurrent_RMSE:0.6138889801437061\ncurrent_RMSE:0.6138889717478845\ncurrent_RMSE:0.61388883081768\ncurrent_RMSE:0.6138884771096772\ncurrent_RMSE:0.6138881779642852\ncurrent_RMSE:0.6138879333815839\ncurrent_RMSE:0.6138877433616386\ncurrent_RMSE:0.6138876079044999\ncurrent_RMSE:0.6138875270102038\ncurrent_RMSE:0.613887500678772\ncurrent_RMSE:0.6138871528608093\ncurrent_RMSE:0.6138868357791624\ncurrent_RMSE:0.6138865732603089\ncurrent_RMSE:0.6138863653043191\ncurrent_RMSE:0.6138862119112481\ncurrent_RMSE:0.6138861130811373\ncurrent_RMSE:0.6138860688140125\ncurrent_RMSE:0.6138858678169762\ncurrent_RMSE:0.6138855327990176\ncurrent_RMSE:0.6138852523439509\ncurrent_RMSE:0.6138850264518508\ncurrent_RMSE:0.6138848551227777\ncurrent_RMSE:0.6138847383567774\ncurrent_RMSE:0.6138846761538806\ncurrent_RMSE:0.6138846685141043\ncurrent_RMSE:0.6138846219784238\ncurrent_RMSE:0.6138842690241004\ncurrent_RMSE:0.6138839706327628\ncurrent_RMSE:0.6138837268044905\ncurrent_RMSE:0.6138835375393485\ncurrent_RMSE:0.6138834028373874\ncurrent_RMSE:0.613883322698643\ncurrent_RMSE:0.6138832971231366\ncurrent_RMSE:0.613883044454653\ncurrent_RMSE:0.6138827281269904\ncurrent_RMSE:0.6138824663624871\ncurrent_RMSE:0.6138822591612131\ncurrent_RMSE:0.6138821065232235\ncurrent_RMSE:0.6138820084485589\ncurrent_RMSE:0.6138819649372458\ncurrent_RMSE:0.6138818590909101\ncurrent_RMSE:0.6138815248268716\ncurrent_RMSE:0.6138812451260821\ncurrent_RMSE:0.6138810199886162\ncurrent_RMSE:0.6138808494145337\ncurrent_RMSE:0.6138807334038804\ncurrent_RMSE:0.6138806719566868\ncurrent_RMSE:0.6138806650729698\ncurrent_RMSE:0.6138803607326371\ncurrent_RMSE:0.6138800630955096\ncurrent_RMSE:0.6138798200217953\ncurrent_RMSE:0.6138796315115592\ncurrent_RMSE:0.6138794975648514\ncurrent_RMSE:0.6138794181817077\ncurrent_RMSE:0.6138793933621493\ncurrent_RMSE:0.6138792358445098\ncurrent_RMSE:0.6138789202709958\ncurrent_RMSE:0.6138786592609804\ncurrent_RMSE:0.613878452814533\ncurrent_RMSE:0.613878300931709\ncurrent_RMSE:0.6138782036125484\ncurrent_RMSE:0.6138781608570776\ncurrent_RMSE:0.6138781501627053\ncurrent_RMSE:0.6138778166527599\ncurrent_RMSE:0.6138775377063937\ncurrent_RMSE:0.6138773133236812\ncurrent_RMSE:0.6138771435046823\ncurrent_RMSE:0.6138770282494419\ncurrent_RMSE:0.6138769675579911\ncurrent_RMSE:0.6138769614303461\ncurrent_RMSE:0.6138767522410132\ncurrent_RMSE:0.6138764553582504\ncurrent_RMSE:0.613876213039222\ncurrent_RMSE:0.613876025283993\ncurrent_RMSE:0.6138758920926133\ncurrent_RMSE:0.6138758134651183\ncurrent_RMSE:0.6138757894015292\ncurrent_RMSE:0.6138757270359598\ncurrent_RMSE:0.6138754122167576\ncurrent_RMSE:0.6138751519613663\ncurrent_RMSE:0.6138749462698556\ncurrent_RMSE:0.6138747951422802\ncurrent_RMSE:0.6138746985786803\ncurrent_RMSE:0.6138746565790818\ncurrent_RMSE:0.6138744082821153\ncurrent_RMSE:0.6138741300903174\ncurrent_RMSE:0.6138739064624766\ncurrent_RMSE:0.6138737373986527\ncurrent_RMSE:0.6138736228988907\ncurrent_RMSE:0.613873562963221\ncurrent_RMSE:0.6138735575916597\ncurrent_RMSE:0.6138734435545158\ncurrent_RMSE:0.613873147426271\ncurrent_RMSE:0.6138729058620556\ncurrent_RMSE:0.6138727188619337\ncurrent_RMSE:0.6138725864259555\ncurrent_RMSE:0.6138725085541563\ncurrent_RMSE:0.6138724852465567\ncurrent_RMSE:0.6138722039694153\ncurrent_RMSE:0.6138719444687838\ncurrent_RMSE:0.6138717395323183\ncurrent_RMSE:0.6138715891600736\ncurrent_RMSE:0.6138714933520898\ncurrent_RMSE:0.6138714521083924\ncurrent_RMSE:0.6138712997199314\ncurrent_RMSE:0.6138710222828457\ncurrent_RMSE:0.613870799409994\ncurrent_RMSE:0.6138706311014358\ncurrent_RMSE:0.6138705173572159\ncurrent_RMSE:0.6138704581773646\ncurrent_RMSE:0.6138704535618977\ncurrent_RMSE:0.6138704346779923\ncurrent_RMSE:0.613870139304418\ncurrent_RMSE:0.6138698984951412\ncurrent_RMSE:0.6138697122502258\ncurrent_RMSE:0.6138695805697216\ncurrent_RMSE:0.6138695034536639\ncurrent_RMSE:0.6138694809020729\ncurrent_RMSE:0.61386929553367\ncurrent_RMSE:0.6138690367879321\ncurrent_RMSE:0.6138688326066196\ncurrent_RMSE:0.6138686829897866\ncurrent_RMSE:0.613868587937473\ncurrent_RMSE:0.6138685474497044\ncurrent_RMSE:0.613868490970763\ncurrent_RMSE:0.6138682142885322\ncurrent_RMSE:0.6138679921707858\ncurrent_RMSE:0.6138678246175826\ncurrent_RMSE:0.6138677116289677\ncurrent_RMSE:0.6138676532049709\ncurrent_RMSE:0.6138676493456081\ncurrent_RMSE:0.6138674309970991\ncurrent_RMSE:0.6138671909428853\ncurrent_RMSE:0.6138670054532743\ncurrent_RMSE:0.6138668745283156\ncurrent_RMSE:0.6138667981680439\ncurrent_RMSE:0.6138667763724798\ncurrent_RMSE:0.6138666869137825\ncurrent_RMSE:0.6138664289230717\ncurrent_RMSE:0.6138662254970186\ncurrent_RMSE:0.6138660766356773\ncurrent_RMSE:0.6138659823390873\ncurrent_RMSE:0.6138659426072739\ncurrent_RMSE:0.6138657061114913\ncurrent_RMSE:0.613865484748965\ncurrent_RMSE:0.6138653179512054\ncurrent_RMSE:0.6138652057182571\ncurrent_RMSE:0.61386514805015\ncurrent_RMSE:0.6138651449468995\ncurrent_RMSE:0.6138650225082822\ncurrent_RMSE:0.6138647832092552\ncurrent_RMSE:0.6138645984750455\ncurrent_RMSE:0.6138644683057024\ncurrent_RMSE:0.6138643927012605\ncurrent_RMSE:0.61386437166174\ncurrent_RMSE:0.6138641208780238\ncurrent_RMSE:0.6138639182073354\ncurrent_RMSE:0.6138637701015643\ncurrent_RMSE:0.61386367656075\ncurrent_RMSE:0.6138636375849175\ncurrent_RMSE:0.6138634977553978\ncurrent_RMSE:0.6138632771482057\ncurrent_RMSE:0.6138631111059768\ncurrent_RMSE:0.6138629996287559\ncurrent_RMSE:0.6138629427165724\ncurrent_RMSE:0.6138629403694416\ncurrent_RMSE:0.6138629138414965\ncurrent_RMSE:0.6138626752977787\ncurrent_RMSE:0.6138624913190661\ncurrent_RMSE:0.6138623619054077\ncurrent_RMSE:0.6138622870568381\ncurrent_RMSE:0.6138622667733771\ncurrent_RMSE:0.6138621126561702\ncurrent_RMSE:0.6138619107409508\ncurrent_RMSE:0.6138617633908277\ncurrent_RMSE:0.6138616706058401\ncurrent_RMSE:0.6138616323860125\ncurrent_RMSE:0.6138615892234878\ncurrent_RMSE:0.6138613693717424\ncurrent_RMSE:0.6138612040851306\ncurrent_RMSE:0.6138610933636964\ncurrent_RMSE:0.6138610372074694\ncurrent_RMSE:0.6138610356164645\ncurrent_RMSE:0.6138608672115444\ncurrent_RMSE:0.6138606839884235\ncurrent_RMSE:0.613860555330518\ncurrent_RMSE:0.613860481237862\ncurrent_RMSE:0.6138604617104751\ncurrent_RMSE:0.6138604042604533\ncurrent_RMSE:0.6138602031008062\ncurrent_RMSE:0.6138600565064076\ncurrent_RMSE:0.6138599644772964\ncurrent_RMSE:0.6138599270134973\ncurrent_RMSE:0.6138597614223708\ncurrent_RMSE:0.613859596891461\ncurrent_RMSE:0.6138594869258721\ncurrent_RMSE:0.6138594315256333\ncurrent_RMSE:0.6138594306907594\ncurrent_RMSE:0.6138593589532019\ncurrent_RMSE:0.6138591764857663\ncurrent_RMSE:0.6138590485836803\ncurrent_RMSE:0.6138589752469782\ncurrent_RMSE:0.6138589564756793\ncurrent_RMSE:0.6138587952894038\ncurrent_RMSE:0.6138586494508051\ncurrent_RMSE:0.6138585581776193\ncurrent_RMSE:0.6138585214698705\ncurrent_RMSE:0.6138584533024469\ncurrent_RMSE:0.613858289527323\ncurrent_RMSE:0.6138581803176366\ncurrent_RMSE:0.6138581256734167\ncurrent_RMSE:0.6138581255946779\ncurrent_RMSE:0.613857968813303\ncurrent_RMSE:0.6138578416671027\ncurrent_RMSE:0.6138577690863936\ncurrent_RMSE:0.613857751071195\ncurrent_RMSE:0.6138576873088065\ncurrent_RMSE:0.6138575422260821\ncurrent_RMSE:0.6138574517088692\ncurrent_RMSE:0.6138574157571921\ncurrent_RMSE:0.6138572819946324\ncurrent_RMSE:0.6138571735409047\ncurrent_RMSE:0.6138571196527333\ncurrent_RMSE:0.6138570609728036\ncurrent_RMSE:0.6138569345825534\ncurrent_RMSE:0.6138568627578753\ncurrent_RMSE:0.6138568454987886\ncurrent_RMSE:0.6138567348338608\ncurrent_RMSE:0.6138566450726675\ncurrent_RMSE:0.6138566098770817\ncurrent_RMSE:0.6138565742948655\ncurrent_RMSE:0.6138564665971515\ncurrent_RMSE:0.6138564134650569\ncurrent_RMSE:0.6138563273313618\ncurrent_RMSE:0.6138562562627516\ncurrent_RMSE:0.6138562397597871\ncurrent_RMSE:0.6138562272753245\ncurrent_RMSE:0.6138561382701962\ncurrent_RMSE:0.6138561038307206\ncurrent_RMSE:0.6138560594874132\ncurrent_RMSE:0.6138560071114226\ncurrent_RMSE:0.6138559496019114\ncurrent_RMSE:0.613855933855078\ncurrent_RMSE:0.613855931302198\nblending_weights:{'lgbm': 0.214, 'catboost': 0.695, 'xgboost': 0.091}\n","output_type":"stream"}]},{"cell_type":"code","source":"y_hats = dict()\ntest_essays_copy=test_essaysdf.copy()\nbest_features=train_feats.drop(['score','score_class'],axis=1).keys().values\n\nsubmission_df = pd.DataFrame(test_feats['id'])\nsubmission_df['score'] = 3.5\n\n\nX_unseen = test_feats.copy()[best_features]\n\nX_unseen.drop(columns=['id'], inplace=True)\n\nX_unseen.replace([np.inf, -np.inf], np.nan, inplace=True)\n\nfor model_name, model_info in models_and_errors_dict.items():\n    print(f'\\n--- {model_name} ---\\n')\n    \n\n    X_unseen_copy = X_unseen.copy()\n    y_hats[model_name] = []\n\n    for ix, (trained_model, error, imputer, scaler,oof_pred) in enumerate(model_info, start=1):\n        print(f\"Using model {ix} with error {error}\")\n\n        if model_name in model_with_scaled_features:\n            X_unseen_imputed = imputer.transform(X_unseen_copy)\n            X_unseen_scaled = scaler.transform(X_unseen_imputed)\n            y_hats[model_name].append(trained_model.predict(X_unseen_scaled))\n        else:\n            y_hats[model_name].append(trained_model.predict(X_unseen_copy))\n    \n    if y_hats[model_name]:\n        y_hat_avg = np.mean(y_hats[model_name], axis=0)\n        submission_df['score_' + model_name] = y_hat_avg\n    print(\"Done.\")\nprint(\"blending\")\nblended_score=np.zeros((len(test_essays_copy)))\nfor k, v in blending_weights.items():\n    blended_score += submission_df['score_' + k] * v\nprint(f\"blended_score:{blended_score}\")","metadata":{"execution":{"iopub.status.busy":"2024-01-09T14:59:04.470724Z","iopub.execute_input":"2024-01-09T14:59:04.471052Z","iopub.status.idle":"2024-01-09T14:59:05.359735Z","shell.execute_reply.started":"2024-01-09T14:59:04.471008Z","shell.execute_reply":"2024-01-09T14:59:05.358688Z"},"trusted":true},"execution_count":95,"outputs":[{"name":"stdout","text":"\n--- lgbm ---\n\nUsing model 1 with error 0.5915866246197972\nUsing model 2 with error 0.6229203543715441\nUsing model 3 with error 0.5799143965420785\nUsing model 4 with error 0.6370054652138515\nUsing model 5 with error 0.6581942026875308\nUsing model 6 with error 0.6663500629815551\nUsing model 7 with error 0.6167316633325122\nUsing model 8 with error 0.6054299344112196\nUsing model 9 with error 0.5923641344954647\nUsing model 10 with error 0.6699150882878421\nDone.\n\n--- catboost ---\n\nUsing model 1 with error 0.5855517341761023\nUsing model 2 with error 0.6070085501747627\nUsing model 3 with error 0.5753115632523214\nUsing model 4 with error 0.6175967181566\nUsing model 5 with error 0.6498335358587458\nUsing model 6 with error 0.6580954282772558\nUsing model 7 with error 0.6285694470784462\nUsing model 8 with error 0.6014761806468116\nUsing model 9 with error 0.57030374284486\nUsing model 10 with error 0.6520925841037742\nDone.\n\n--- xgboost ---\n\nUsing model 1 with error 0.5987595416344031\nUsing model 2 with error 0.6121550703361341\nUsing model 3 with error 0.5943724371720139\nUsing model 4 with error 0.6480844104033779\nUsing model 5 with error 0.6593346301274601\nUsing model 6 with error 0.6756422533728538\nUsing model 7 with error 0.6338252961958017\nUsing model 8 with error 0.6253309982388959\nUsing model 9 with error 0.5964751413407655\nUsing model 10 with error 0.6834638031764316\nDone.\nblending\nblended_score:0    1.804541\n1    1.618949\n2    1.611147\nName: score_lgbm, dtype: float64\n","output_type":"stream"}]},{"cell_type":"code","source":"blended_score","metadata":{"execution":{"iopub.status.busy":"2024-01-09T14:59:05.361173Z","iopub.execute_input":"2024-01-09T14:59:05.361496Z","iopub.status.idle":"2024-01-09T14:59:05.371303Z","shell.execute_reply.started":"2024-01-09T14:59:05.361465Z","shell.execute_reply":"2024-01-09T14:59:05.370348Z"},"trusted":true},"execution_count":96,"outputs":[{"execution_count":96,"output_type":"execute_result","data":{"text/plain":"0    1.804541\n1    1.618949\n2    1.611147\nName: score_lgbm, dtype: float64"},"metadata":{}}]},{"cell_type":"markdown","source":"# **Submission**","metadata":{}},{"cell_type":"code","source":"submission = pd.DataFrame({'id': test_essays.index, 'score': blended_score.values*0.5+publiclgbm_pred[\"score\"].values*0.5})\nsubmission.to_csv('submission.csv', index=False)\nsubmission","metadata":{"execution":{"iopub.status.busy":"2024-01-09T15:04:12.931406Z","iopub.execute_input":"2024-01-09T15:04:12.931821Z","iopub.status.idle":"2024-01-09T15:04:12.948481Z","shell.execute_reply.started":"2024-01-09T15:04:12.931788Z","shell.execute_reply":"2024-01-09T15:04:12.947608Z"},"trusted":true},"execution_count":97,"outputs":[{"execution_count":97,"output_type":"execute_result","data":{"text/plain":"         id     score\n0  0000aaaa  1.742340\n1  2222bbbb  1.577877\n2  4444cccc  1.623538","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000aaaa</td>\n      <td>1.742340</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2222bbbb</td>\n      <td>1.577877</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4444cccc</td>\n      <td>1.623538</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]}]}